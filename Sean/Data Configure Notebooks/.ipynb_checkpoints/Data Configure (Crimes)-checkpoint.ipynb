{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import calendar\n",
    "\n",
    "NUM_TIME_BINS_PER_DAY = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# LOAD CRIME DATA #\n",
    "###################\n",
    "crime_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Raw Data/Crimes (Chicago).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 6059764\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# CONDITION CRIME DATA #\n",
    "########################\n",
    "# Delete columns that are redundant or unhelpful\n",
    "columns_to_delete = ['Case Number', 'Location Description', 'Block', 'Arrest', 'Domestic', 'FBI Code', 'IUCR', 'Description','X Coordinate', 'Y Coordinate', 'Year', 'Updated On', 'Location']\n",
    "final_dataframe = crime_dataframe.drop(columns=columns_to_delete).copy().dropna()\n",
    "print('Length: %d' % len(final_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 6059764\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# CONVERT POLICE AND COMMNUNITY #\n",
    "#################################\n",
    "final_dataframe['Beat'] = final_dataframe['Beat'].astype(int).astype(str).apply(lambda x: 'BEAT_'+x)\n",
    "final_dataframe['District'] = final_dataframe['District'].astype(int).astype(str).apply(lambda x: 'DISTRICT_'+x)\n",
    "final_dataframe['Ward'] = final_dataframe['Ward'].astype(int).astype(str).apply(lambda x: 'WARD_'+x)\n",
    "final_dataframe['Community Area'] = final_dataframe['Community Area'].astype(int).astype(str).apply(lambda x: 'COMMUNITY_'+x)\n",
    "print('Length: %d' % len(final_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 6059764\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# CONDITION DATE #\n",
    "##################\n",
    "# Convert crime dates to YEAR, MONTH, DAY, HOUR, MINUTE, and weekday columns\n",
    "# Convert those to one-hot and concat with final dataframe\n",
    "final_dataframe['Date'] = pd.to_datetime(final_dataframe['Date'])\n",
    "final_dataframe['YEAR'] = final_dataframe['Date'].dt.year.astype(str).apply(lambda x: 'YEAR_'+x)\n",
    "final_dataframe['MONTH'] = final_dataframe['Date'].dt.month.apply(lambda x: calendar.month_abbr[x])\n",
    "final_dataframe['DAY'] = final_dataframe['Date'].dt.day.astype(str).apply(lambda x: 'DAY_'+x)\n",
    "final_dataframe['WEEKDAY'] = final_dataframe['Date'].dt.weekday.apply(lambda x: calendar.day_name[x])\n",
    "hours = final_dataframe['Date'].dt.hour\n",
    "minutes = final_dataframe['Date'].dt.minute\n",
    "final_dataframe['TIME_OF_DAY'] = ((hours + minutes / 60.) / 24. * NUM_TIME_BINS_PER_DAY).astype(int).astype(str).apply(lambda x: 'TIME_SLOT_'+x)\n",
    "print('Length: %d' % len(final_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 1861170\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "# JOIN SOCIOECONOMIC INDICATORS #\n",
    "#################################\n",
    "socioeconomic_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Raw Data/Socioeconomic Indicators (Chicago).csv')\n",
    "# Remove unhelpful data\n",
    "socioeconomic_dataframe = socioeconomic_dataframe.drop(columns=['COMMUNITY AREA NAME']).dropna()\n",
    "# This data is only for 2008 - 2012, so filter for that interval\n",
    "final_dataframe = final_dataframe[(final_dataframe['Date'] >= '2008-01-01') & (final_dataframe['Date'] <= '2012-12-31')]\n",
    "# Join the two dataframes on Community Area\n",
    "socioeconomic_dataframe = socioeconomic_dataframe.rename(columns={'Community Area Number':'Community Area'})\n",
    "socioeconomic_dataframe['Community Area'] = socioeconomic_dataframe['Community Area'].astype(int).astype(str).apply(lambda x: 'COMMUNITY_'+x)\n",
    "final_dataframe = final_dataframe.merge(socioeconomic_dataframe, on=['Community Area'], how='left').dropna()\n",
    "print('Length: %d' % len(final_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 1851559\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# JOIN TEMPERATURE #\n",
    "####################\n",
    "temperature_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Raw Data/Temperatures (Chicago).csv')\n",
    "# Drop the TAVG column because it has too many NaNs\n",
    "temperature_dataframe = temperature_dataframe.drop(columns=['TAVG'])\n",
    "# Convert the Precipitation, max T, and min T columns to float\n",
    "temperature_dataframe['PRCP'] = pd.to_numeric(temperature_dataframe['PRCP'])\n",
    "temperature_dataframe['TMAX'] = pd.to_numeric(temperature_dataframe['TMAX'])\n",
    "temperature_dataframe['TMIN'] = pd.to_numeric(temperature_dataframe['TMIN'])\n",
    "temperature_dataframe = temperature_dataframe.rename(columns={'PRCP':'PRECIPITATION', 'TMAX':'MAX TEMP', 'TMIN':'MIN TEMP'})\n",
    "# Join with the final dataframe\n",
    "temperature_dataframe['date_join'] = pd.to_datetime(temperature_dataframe['DATE']).dt.date\n",
    "# Drop unnecessary columns before merge\n",
    "temperature_dataframe = temperature_dataframe.drop(columns=['DATE', 'STATION', 'NAME'])\n",
    "final_dataframe['date_join'] = final_dataframe['Date'].dt.date\n",
    "final_dataframe = final_dataframe.merge(temperature_dataframe, on=['date_join'], how='left').drop(columns=['date_join']).dropna()\n",
    "print('Length: %d' % len(final_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 1851559\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# JOIN LIFE EXPECTANCY #\n",
    "########################\n",
    "life_expectancy_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Raw Data/Life Expectancy (Chicago).csv')\n",
    "life_expectancy_dataframe = life_expectancy_dataframe.dropna()\n",
    "life_expectancy_dataframe = life_expectancy_dataframe.drop(columns=['1990 Lower 95% CI', '1990 Upper 95% CI', '2000 Lower 95% CI', '2000 Upper 95% CI', '2010 Lower 95% CI', '2010 Upper 95% CI'])\n",
    "# Build a new dataframe to interpolate/extrapolate life expectancy and reformat for merging\n",
    "temp = pd.DataFrame({'YEAR':[],'Community Area':[],'Life Expectancy':[]})\n",
    "index = 0\n",
    "for year in range(2001,2019):\n",
    "    for community in life_expectancy_dataframe['Community Area Number']:\n",
    "        max_age_2000 = life_expectancy_dataframe.loc[life_expectancy_dataframe['Community Area Number']==community]['2000 Life Expectancy'].values[0]\n",
    "        max_age_2010 = life_expectancy_dataframe.loc[life_expectancy_dataframe['Community Area Number']==community]['2010 Life Expectancy'].values[0]\n",
    "        temp = temp.append({'YEAR':year,\n",
    "                            'Community Area':community,\n",
    "                            'Life Expectancy':((max_age_2010-max_age_2000)/(2010-2000)*(year-2000)+max_age_2000)},\n",
    "                           ignore_index=True)\n",
    "life_expectancy_dataframe = temp\n",
    "# Merge the life expectancy data\n",
    "life_expectancy_dataframe['Community Area'] = life_expectancy_dataframe['Community Area'].astype(int).astype(str).apply(lambda x: 'COMMUNITY_'+x)\n",
    "life_expectancy_dataframe['YEAR'] = life_expectancy_dataframe['YEAR'].astype(int).astype(str).apply(lambda x: 'YEAR_'+x)\n",
    "final_dataframe = final_dataframe.merge(life_expectancy_dataframe, on=['YEAR', 'Community Area'], how='left').dropna()\n",
    "print('Length: %d' % len(final_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the new crime data to a temporary file in my workspace\n",
    "writer = pd.ExcelWriter('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/27_November.xlsx')\n",
    "final_dataframe.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Date', 'Primary Type', 'Beat', 'District', 'Ward',\n",
       "       'Community Area', 'Latitude', 'Longitude', 'YEAR', 'MONTH', 'DAY',\n",
       "       'WEEKDAY', 'TIME_OF_DAY', 'PERCENT OF HOUSING CROWDED',\n",
       "       'PERCENT HOUSEHOLDS BELOW POVERTY', 'PERCENT AGED 16+ UNEMPLOYED',\n",
       "       'PERCENT AGED 25+ WITHOUT HIGH SCHOOL DIPLOMA',\n",
       "       'PERCENT AGED UNDER 18 OR OVER 64', 'PER CAPITA INCOME ',\n",
       "       'HARDSHIP INDEX', 'PRECIPITATION', 'MAX TEMP', 'MIN TEMP',\n",
       "       'Life Expectancy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# JOIN L ENTRIES #\n",
    "##################\n",
    "L_entry_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Raw Data/L Station Entries (Chicago).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['station_id', 'stationname', 'date', 'daytype', 'rides'], dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_entry_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# JOIN SBIF GRANTS #\n",
    "####################\n",
    "SBIF_grant_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Raw Data/SBIF Grants (Chicago).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Company', 'Address', 'TIF District', 'Completion Date', 'Actual Costs',\n",
       "       'Actual Grant', 'Work Items'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SBIF_grant_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sean/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3018: DtypeWarning: Columns (9,16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# JOIN BUSINESS LICENSES #\n",
    "##########################\n",
    "business_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Raw Data/Business Licenses (Chicago).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'LICENSE ID', 'ACCOUNT NUMBER', 'SITE NUMBER', 'LEGAL NAME',\n",
       "       'DOING BUSINESS AS NAME', 'ADDRESS', 'CITY', 'STATE', 'ZIP CODE',\n",
       "       'WARD', 'PRECINCT', 'WARD PRECINCT', 'POLICE DISTRICT', 'LICENSE CODE',\n",
       "       'LICENSE DESCRIPTION', 'BUSINESS ACTIVITY ID', 'BUSINESS ACTIVITY',\n",
       "       'LICENSE NUMBER', 'APPLICATION TYPE', 'APPLICATION CREATED DATE',\n",
       "       'APPLICATION REQUIREMENTS COMPLETE', 'PAYMENT DATE',\n",
       "       'CONDITIONAL APPROVAL', 'LICENSE TERM START DATE',\n",
       "       'LICENSE TERM EXPIRATION DATE', 'LICENSE APPROVED FOR ISSUANCE',\n",
       "       'DATE ISSUED', 'LICENSE STATUS', 'LICENSE STATUS CHANGE DATE', 'SSA',\n",
       "       'LATITUDE', 'LONGITUDE', 'LOCATION'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataframes not yet joined:\n",
    "#  business licenses\n",
    "#  SBIF\n",
    "#  L Entries by Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AHS_2001_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Raw Data/AHS/2001.csv').dropna()\n",
    "# AHS_2003_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Raw Data/AHS/2003.csv').dropna()\n",
    "# AHS_2005_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Raw Data/AHS/2005.csv').dropna()\n",
    "# AHS_2007_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Raw Data/AHS/2007.csv').dropna()\n",
    "# AHS_2009_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Raw Data/AHS/2009.csv').dropna()\n",
    "# AHS_2011_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Raw Data/AHS/2011.csv').dropna()\n",
    "# AHS_2013_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Raw Data/AHS/2013.csv').dropna()\n",
    "# AHS_2015_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Raw Data/AHS/2015.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
