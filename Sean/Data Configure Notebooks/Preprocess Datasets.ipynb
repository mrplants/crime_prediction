{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from shapely.geometry import Point, Polygon\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building CNN Data\n",
    "# Data that varies with Time and Location:\n",
    "# - Crime (OUTPUT - YOLO with crime and location)\n",
    "# - L entries (8 layers - one per line)\n",
    "# - Businesses (5+ layers - types of businesses)\n",
    "# - SBIF grants (1 layer)\n",
    "# - MMRP Permits (21 layers - types of permits)\n",
    "# - Life Expectancy (1 layer)\n",
    "#\n",
    "# Data that varies with Time Only:\n",
    "# - Temperature (3 layers - MIN, MAX, and PRECIPITATION)\n",
    "# - Date (43 layers - 12 for month, 31 for day)\n",
    "# - Time (5 layers - one for each time slot)\n",
    "#\n",
    "# Data that varies with Location Only:\n",
    "# - Buildings (10 layers - stories|units|sqfeet for sound|minor repair|major repair.  Also uninhabitable or not.)\n",
    "# - Waterways (1 layer)\n",
    "# - Major Streets (1 layer)\n",
    "# - Libraries (1 layer)\n",
    "# - Public Parks (1 layer)\n",
    "# - Forests (1 layer)\n",
    "# - Schools (1 layer)\n",
    "\n",
    "# Total layers: 60 + (date layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# HELPER FUNCTIONS #\n",
    "####################\n",
    "\n",
    "# Geocode:  Use Google Maps API to find latitude and longitude from an address\n",
    "# Input: Address (String)\n",
    "# Output: Location (Point)\n",
    "def geocode(address, region=None):\n",
    "    GOOGLE_MAPS_API_URL = 'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "    params = {'address': address,'key':open('google_api.key').read().strip()}\n",
    "\n",
    "    # Do the request and get the response data\n",
    "    geo_request = requests.get(GOOGLE_MAPS_API_URL, params=params)\n",
    "    geo_response = geo_request.json()\n",
    "    \n",
    "    if len(geo_response['results']) == 0: return None\n",
    "    latitude = geo_response['results'][0]['geometry']['location']['lat']\n",
    "    longitude = geo_response['results'][0]['geometry']['location']['lng']\n",
    "    return Point(latitude, longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMRP permit dataset processed.\n"
     ]
    }
   ],
   "source": [
    "# - MMRP Permits (21 layers)\n",
    "\n",
    "MMRP_permits = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Unprocessed/MMRP Permits.csv')\n",
    "permits_dataframe = MMRP_permits.loc[:,['Last Permit Activity Date/Time',\n",
    "                              'Permit Type Description',\n",
    "                              'Location']].rename(columns={'Last Permit Activity Date/Time':'Date',\n",
    "                                                           'Permit Type Description':'Permit Type'}).dropna()\n",
    "writer = pd.ExcelWriter('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/MMRP Permits.xlsx')\n",
    "MMRP_permits.to_excel(writer)\n",
    "writer.save()\n",
    "print('MMRP permit dataset processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset processed.\n"
     ]
    }
   ],
   "source": [
    "# - Buildings (10 layers)\n",
    "\n",
    "buildings = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Unprocessed/Buildings.csv', low_memory=False)\n",
    "# Removed demolished buildings\n",
    "# For 'SOUND', 'NEEDS MAJOR REPAIR', and 'NEEDS MINOR REPAIR' in BLDG_CONDI:\n",
    "# - Layer for NO_STORIES, NO_OF_UNIT, and BLDG_SQ_FO\n",
    "# For 'UNNHABITABLE', 'UNINHABITABLE' in BLDG_CONDI:\n",
    "# - 1 layer\n",
    "\n",
    "buildings_dataframe = buildings.loc[:,['BLDG_CONDI',\n",
    "                                       'NO_STORIES',\n",
    "                                       'NO_OF_UNIT',\n",
    "                                       'BLDG_SQ_FO',\n",
    "                                       'the_geom']].rename(columns={'BLDG_CONDI':'Condition',\n",
    "                                                                    'NO_STORIES':'Stories',\n",
    "                                                                    'NO_OF_UNIT':'Units',\n",
    "                                                                    'BLDG_SQ_FO':'Square Footage',\n",
    "                                                                    'the_geom':'Footprint'}).dropna()\n",
    "writer = pd.ExcelWriter('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/Buildings.xlsx')\n",
    "buildings_dataframe.to_excel(writer)\n",
    "writer.save()\n",
    "print('Building dataset processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community dataset processed.\n"
     ]
    }
   ],
   "source": [
    "# Community Areas (metadata)\n",
    "\n",
    "communities = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Unprocessed/Community Areas.csv')\n",
    "communities_dataframe = communities.loc[:,['the_geom',\n",
    "                                           'COMMUNITY',\n",
    "                                           'AREA_NUM_1']].rename(columns={'the_geom':'Outline',\n",
    "                                                                          'COMMUNITY':'Name',\n",
    "                                                                          'AREA_NUM_1':'Number'}).dropna()\n",
    "writer = pd.ExcelWriter('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/Communities.xlsx')\n",
    "communities_dataframe.to_excel(writer)\n",
    "writer.save()\n",
    "print('Community dataset processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business dataset processed.\n"
     ]
    }
   ],
   "source": [
    "# Businesses (5 layers - types of businesses)\n",
    "\n",
    "# Business Activity Categories:\n",
    "#\n",
    "# Food: 'Retail Sales of Perishable Foods',\n",
    "#       'Wholesale Food Sales',\n",
    "#       'Sale of Food Prepared Onsite With Dining Area',\n",
    "#       'Sale of Food Prepared Onsite Without Dining Area',\n",
    "#       'Retail Sales of General Merchandise and Non-Perishable Food'\n",
    "# Tobacco: 'Retail Sale of Tobacco'\n",
    "# Bar: 'Consumption of Liquor on Premises',\n",
    "#      'Tavern - Consumption of Liquor on Premise',\n",
    "#      'Sale of Liquor Until 4am, Monday - Saturday and 5am on Sunday',\n",
    "#       'Special Event Beer & Wine',\n",
    "#       'Consumption of Liquor on Premises, Not for Profit'\n",
    "# Package Store: 'Retail Sales of Packaged Liquor',\n",
    "#                'Sale of Liquor Outdoors on Private Property'\n",
    "# Gas Station: 'Operation of a Fuel Filling Station'\n",
    "\n",
    "business_licenses = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Unprocessed/Business Licenses (Chicago).csv', low_memory=False)\n",
    "business_dataframe = business_licenses.loc[:,['BUSINESS ACTIVITY',\n",
    "                                              'LICENSE TERM START DATE',\n",
    "                                              'LICENSE TERM EXPIRATION DATE',\n",
    "                                              'LOCATION']].rename(columns={'BUSINESS ACTIVITY':'Activity',\n",
    "                                                                           'LICENSE TERM START DATE':'Start Date',\n",
    "                                                                           'LICENSE TERM EXPIRATION DATE':'End Date',\n",
    "                                                                           'LOCATION':'Location'}).dropna()\n",
    "# Categorize the data\n",
    "activity = business_dataframe['Activity']\n",
    "business_dataframe['Food Service'] = ((activity == 'Retail Sales of Perishable Foods') | (activity == 'Wholesale Food Sales') | (activity == 'Sale of Food Prepared Onsite With Dining Area') | (activity == 'Sale of Food Prepared Onsite Without Dining Area') | (activity == 'Retail Sales of General Merchandise and Non-Perishable Food'))\n",
    "business_dataframe['Tobacco Sale'] = (activity == 'Retail Sale of Tobacco')\n",
    "business_dataframe['Alcohol Consumption'] = ((activity == 'Consumption of Liquor on Premises') | (activity == 'Consumption of Liquor on Premises, Not for Profit') | (activity == 'Special Event Beer & Wine') | (activity == 'Sale of Liquor Until 4am, Monday - Saturday and 5am on Sunday') | (activity == 'Tavern - Consumption of Liquor on Premise'))\n",
    "business_dataframe['Package Store'] = ((activity == 'Retail Sales of Packaged Liquor') | (activity == 'Sale of Liquor Outdoors on Private Property'))\n",
    "business_dataframe['Gas Station'] = (activity == 'Operation of a Fuel Filling Station')\n",
    "# Drop the Activity column\n",
    "business_dataframe = business_dataframe.drop(columns=['Activity'])\n",
    "# Drop the rows which do not perform the activities specified above\n",
    "business_dataframe = business_dataframe.loc[business_dataframe['Food Service']|business_dataframe['Tobacco Sale']|business_dataframe['Gas Station']|business_dataframe['Package Store']|business_dataframe['Alcohol Consumption'],:].reset_index()\n",
    "writer = pd.ExcelWriter('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/Businesses.xlsx')\n",
    "business_dataframe.to_excel(writer)\n",
    "writer.save()\n",
    "print('Business dataset processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBIF Grant dataset processed.\n"
     ]
    }
   ],
   "source": [
    "# SBIF grants (1 layer)\n",
    "\n",
    "SBIF_grants = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Unprocessed/SBIF Grants (Chicago).csv', low_memory=False)\n",
    "\n",
    "# def chicago_geocode(address):\n",
    "#     # Chop up the address if it contains a '-'\n",
    "#     fixed_address = address.split('-')[-1]\n",
    "#     # Look up the address near Chicago\n",
    "#     geo_point = geocode(fixed_address, region='Chicago, IL')\n",
    "#     if geo_point == None:\n",
    "#         print('Could not find address: '+ str(address))\n",
    "#     return geo_point    \n",
    "\n",
    "# SBIF_grant_locations = SBIF_grants['Address'].apply(chicago_geocode)\n",
    "# Could not find address: 228 W Washtenaw\n",
    "# Could not find address: 4201-A W Lawrence\n",
    "# Could not find address: 4201-A W Lawrence\n",
    "\n",
    "SBIF_grants['Location'] = SBIF_grant_locations\n",
    "# Filter important columns and rename\n",
    "SBIF_grant_dataframe = SBIF_grants.loc[:,['Completion Date',\n",
    "                                          'Location',\n",
    "                                          'Actual Grant']].rename(columns={'Completion Date':'Date',\n",
    "                                                                           'Actual Grant':'Amount'}).dropna().reset_index()\n",
    "# Write to file\n",
    "writer = pd.ExcelWriter('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/SBIF Grants.xlsx')\n",
    "SBIF_grant_dataframe.to_excel(writer)\n",
    "writer.save()\n",
    "print('SBIF Grant dataset processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L Entry dataset processed.\n"
     ]
    }
   ],
   "source": [
    "# L entries (8 layers - one per line)\n",
    "\n",
    "L_entries = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Unprocessed/L Station Entries (Chicago).csv', low_memory=False)\n",
    "L_stations = gp.read_file('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Unprocessed/CTA_RailStations/CTA_RailStations.shp').to_crs(epsg=4326)\n",
    "\n",
    "# Specify the L lines being considered.\n",
    "L_lines = ['Green','Red','Brown','Purple','Yellow','Blue','Pink','Orange']\n",
    "# Condition the L_station name data\n",
    "L_stations['STATION_ID'] = L_stations['STATION_ID'].unique()+40000\n",
    "# Merge L_station location with L_entries\n",
    "L_entries_dataframe = L_entries.merge(L_stations.drop(columns=['LONGNAME', 'ADDRESS', 'ADA', 'PKNRD', 'POINT_X', 'POINT_Y', 'GTFS']), how='inner', left_on='station_id', right_on='STATION_ID')\n",
    "# Retrieve and clean Line specification data.\n",
    "for line in L_lines:\n",
    "    L_entries_dataframe[line+str(' Line')] = L_entries_dataframe['LINES'].str.contains(line)\n",
    "# Filter important columns and rename\n",
    "L_entries_dataframe = L_entries_dataframe.drop(columns=['station_id',\n",
    "                                                        'stationname',\n",
    "                                                        'daytype',\n",
    "                                                        'STATION_ID',\n",
    "                                                        'LINES']).rename(columns={'date':'Date',\n",
    "                                                                                  'rides':'Entries',\n",
    "                                                                                  'geometry':'Location'}).dropna()\n",
    "# Write to file\n",
    "writer = pd.ExcelWriter('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/L Entries.xlsx')\n",
    "L_entries_dataframe.to_excel(writer)\n",
    "writer.save()\n",
    "print('L Entry dataset processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life Expectancy dataset processed.\n"
     ]
    }
   ],
   "source": [
    "# Life Expectancy (1 layer)\n",
    "\n",
    "life_expectancy_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Unprocessed/Life Expectancy (Chicago).csv').dropna()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "life_expectancy_dataframe = life_expectancy_dataframe.drop(columns=['1990 Lower 95% CI', '1990 Upper 95% CI', '2000 Lower 95% CI', '2000 Upper 95% CI', '2010 Lower 95% CI', '2010 Upper 95% CI'])\n",
    "# Build a new dataframe to interpolate/extrapolate life expectancy and reformat for merging\n",
    "temp = pd.DataFrame({'Year':[],'Community Area':[],'Life Expectancy':[]})\n",
    "index = 0\n",
    "for year in range(2001,2019):\n",
    "    for community in life_expectancy_dataframe['Community Area Number']:\n",
    "        max_age_2000 = life_expectancy_dataframe.loc[life_expectancy_dataframe['Community Area Number']==community]['2000 Life Expectancy'].values[0]\n",
    "        max_age_2010 = life_expectancy_dataframe.loc[life_expectancy_dataframe['Community Area Number']==community]['2010 Life Expectancy'].values[0]\n",
    "        temp = temp.append({'Year':year,\n",
    "                            'Community Area':community,\n",
    "                            'Life Expectancy':((max_age_2010-max_age_2000)/(2010-2000)*(year-2000)+max_age_2000)},\n",
    "                           ignore_index=True)\n",
    "life_expectancy_dataframe = temp\n",
    "\n",
    "# Write to file\n",
    "writer = pd.ExcelWriter('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/Life Expectancy.xlsx')\n",
    "life_expectancy_dataframe.to_excel(writer)\n",
    "writer.save()\n",
    "print('Life Expectancy dataset processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature dataset processed.\n"
     ]
    }
   ],
   "source": [
    "# Temperature (3 layers - MIN, MAX, and PRECIPITATION)\n",
    "\n",
    "temperature_dataframe = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Unprocessed/Temperatures (Chicago).csv')\n",
    "\n",
    "# Drop the TAVG column because it has too many NaNs\n",
    "temperature_dataframe = temperature_dataframe\n",
    "# Convert the Precipitation, max T, and min T columns to float\n",
    "temperature_dataframe['PRCP'] = pd.to_numeric(temperature_dataframe['PRCP'])\n",
    "temperature_dataframe['TMAX'] = pd.to_numeric(temperature_dataframe['TMAX'])\n",
    "temperature_dataframe['TMIN'] = pd.to_numeric(temperature_dataframe['TMIN'])\n",
    "temperature_dataframe['DATE'] = pd.to_datetime(temperature_dataframe['DATE']).dt.date\n",
    "temperature_dataframe = temperature_dataframe.drop(columns=['TAVG']).rename(columns={'PRCP':'Precipitation',\n",
    "                                                                                     'TMAX':'Max Temp',\n",
    "                                                                                     'TMIN':'Min Temp',\n",
    "                                                                                     'DATE':'Date'}).dropna()\n",
    "\n",
    "# Write to file\n",
    "writer = pd.ExcelWriter('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/Temperature.xlsx')\n",
    "temperature_dataframe.to_excel(writer)\n",
    "writer.save()\n",
    "print('Temperature dataset processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waterways dataset processed.\n"
     ]
    }
   ],
   "source": [
    "# Waterways (1 layer)\n",
    "\n",
    "waterways = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Unprocessed/Hydro.csv')\n",
    "\n",
    "#Remove unnecessary columns and rename for consistency\n",
    "waterways_dataframe = waterways.loc[:,['the_geom']].rename(columns={'the_geom':'Outline'}).dropna()\n",
    "\n",
    "# Write to file\n",
    "writer = pd.ExcelWriter('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/Waterways.xlsx')\n",
    "waterways_dataframe.to_excel(writer)\n",
    "writer.save()\n",
    "print('Waterway dataset processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Street dataset processed.\n"
     ]
    }
   ],
   "source": [
    "# Major Streets (1 layer)\n",
    "\n",
    "streets = gp.read_file('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Unprocessed/Major_20Streets/Major_Streets.shp').to_crs(epsg=4326)\n",
    "\n",
    "#Remove unnecessary columns and rename for consistency\n",
    "street_dataframe = streets.loc[:,['geometry']].rename(columns={'geometry':'Centerline'}).dropna()\n",
    "\n",
    "# Write to file\n",
    "writer = pd.ExcelWriter('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/Streets.xlsx')\n",
    "street_dataframe.to_excel(writer)\n",
    "writer.save()\n",
    "print('Street dataset processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Park dataset processed.\n"
     ]
    }
   ],
   "source": [
    "# - Public Parks (1 layer)\n",
    "\n",
    "parks = gp.read_file('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Unprocessed/park boundaries/park boundaries.shp').to_crs(epsg=4326)\n",
    "\n",
    "#Remove unnecessary columns and rename for consistency\n",
    "park_dataframe = parks.loc[:,['geometry']].rename(columns={'geometry':'Outline'}).dropna()\n",
    "\n",
    "# Write to file\n",
    "writer = pd.ExcelWriter('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/Parks.xlsx')\n",
    "park_dataframe.to_excel(writer)\n",
    "writer.save()\n",
    "print('Park dataset processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest dataset processed.\n"
     ]
    }
   ],
   "source": [
    "# - Forests (1 layer)\n",
    "\n",
    "forests = gp.read_file('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Unprocessed/Forestry/Forestry.shp').to_crs(epsg=4326)\n",
    "\n",
    "#Remove unnecessary columns and rename for consistency\n",
    "forest_dataframe = forests.loc[:,['geometry']].rename(columns={'geometry':'Outline'}).dropna()\n",
    "\n",
    "# Write to file\n",
    "writer = pd.ExcelWriter('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/Forests.xlsx')\n",
    "forest_dataframe.to_excel(writer)\n",
    "writer.save()\n",
    "print('Forest dataset processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "School dataset processed.\n"
     ]
    }
   ],
   "source": [
    "# - Schools (1 layer)\n",
    "\n",
    "schools = gp.read_file('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Unprocessed/School_20Grounds/School_Grounds.shp').to_crs(epsg=4326)\n",
    "\n",
    "#Remove unnecessary columns and rename for consistency\n",
    "school_dataframe = schools.loc[:,['geometry']].rename(columns={'geometry':'Outline'}).dropna()\n",
    "\n",
    "# Write to file\n",
    "writer = pd.ExcelWriter('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/Schools.xlsx')\n",
    "school_dataframe.to_excel(writer)\n",
    "writer.save()\n",
    "print('School dataset processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library dataset processed.\n"
     ]
    }
   ],
   "source": [
    "# - Libraries (1 layer)\n",
    "\n",
    "libraries = pd.read_csv('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Unprocessed/libraries.csv')\n",
    "\n",
    "#Remove unnecessary columns and rename for consistency\n",
    "library_dataframe = libraries.loc[:,['LOCATION']].rename(columns={'LOCATION':'Location'}).dropna()\n",
    "\n",
    "# Write to file\n",
    "writer = pd.ExcelWriter('/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/Libraries.xlsx')\n",
    "library_dataframe.to_excel(writer)\n",
    "writer.save()\n",
    "print('Library dataset processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS230",
   "language": "python",
   "name": "cs230"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
