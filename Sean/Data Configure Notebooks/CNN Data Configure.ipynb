{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "# Shapely\n",
    "import shapely.wkt as wkt\n",
    "import shapely.geometry as geo\n",
    "import shapely.ops as ops\n",
    "import shapely.affinity as aff\n",
    "from shapely.prepared import prep\n",
    "import time\n",
    "import datetime as dtm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# CONSTANTS #\n",
    "#############\n",
    "X_MAX_PIXELS = 2048\n",
    "Y_MAX_PIXELS = X_MAX_PIXELS\n",
    "L_LINES = ['Green','Red','Brown','Purple','Yellow','Blue','Pink','Orange']\n",
    "BUSINESS_CATEGORIES = ['Food Service', 'Tobacco Sale', 'Alcohol Consumption', 'Package Store', 'Gas Station']\n",
    "SOCIO_INDICATORS = ['PERCENT OF HOUSING CROWDED', 'PERCENT HOUSEHOLDS BELOW POVERTY','PERCENT AGED 16+ UNEMPLOYED','PERCENT AGED 25+ WITHOUT HIGH SCHOOL DIPLOMA','PERCENT AGED UNDER 18 OR OVER 64', 'PER CAPITA INCOME ','HARDSHIP INDEX']\n",
    "START_DATE = dtm.date(2001, 1, 1)\n",
    "FINAL_DATE = dtm.date(2018, 1, 1)\n",
    "NUM_DAYS = (FINAL_DATE - START_DATE).days\n",
    "NUM_TIME_SLOTS = 12\n",
    "CNN_final_path = '/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/CNN Final/'\n",
    "processed_dataset_paths_xlsx = '/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/%s.xlsx' \n",
    "processed_dataset_paths_csv = '/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/%s.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UNUSED OR UNPROCESSED DATASETS\n",
    "\n",
    "# MMRP_permits = pd.read_excel(processed_dataset_paths_xlsx % 'MMRP Permits')\n",
    "# SBIF_grants = pd.read_excel(processed_dataset_paths_xlsx % 'SBIF Grants')\n",
    "# weather = pd.read_excel(processed_dataset_paths_xlsx % 'Weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building CNN Data\n",
    "# Data that varies with Time and Location:\n",
    "# - Crime (OUTPUT - YOLO with crime and location)\n",
    "# - L entries (8 layers - one per line)\n",
    "# - SBIF grants (1 layer)\n",
    "# - MMRP Permits (21 layers - types of permits)\n",
    "# - Life Expectancy (1 layer)\n",
    "#\n",
    "# Data that varies with Time Only:\n",
    "# - Temperature (3 layers - MIN, MAX, and PRECIPITATION)\n",
    "# - Date (43 layers - 12 for month, 31 for day)\n",
    "# - Time (5 layers - one for each time slot)\n",
    "#\n",
    "# Data that varies with Location Only:\n",
    "# - Businesses (5+ layers - types of businesses)\n",
    "# - Buildings (10 layers - stories|units|sqfeet for sound|minor repair|major repair.  Also uninhabitable or not.)\n",
    "# - Waterways (1 layer)\n",
    "# - Major Streets (1 layer)\n",
    "# - Libraries (1 layer)\n",
    "# - Public Parks (1 layer)\n",
    "# - Forests (1 layer)\n",
    "# - Schools (1 layer)\n",
    "\n",
    "# Total layers: 60 + (date layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# UTILITY FUNCTIONS #\n",
    "#####################\n",
    "communities = pd.read_csv(processed_dataset_paths_csv % 'Communities')\n",
    "# Rasterize the communities\n",
    "rastered_communities = communities.copy()\n",
    "rastered_communities['Coordinates'] = communities.Outline.apply(lambda x: points_within(transform_latlong_to_frame(wkt.loads(x))))\n",
    "\n",
    "# Preprocess necessary parameters for the transformations\n",
    "polygons = []\n",
    "for encoding in communities.Outline:\n",
    "    polygons.extend(wkt.loads(encoding).geoms)\n",
    "chicago = geo.MultiPolygon(polygons)\n",
    "x_scale = float(X_MAX_PIXELS) / (chicago.bounds[2] - chicago.bounds[0])\n",
    "y_scale = float(Y_MAX_PIXELS) / (chicago.bounds[3] - chicago.bounds[1])\n",
    "x_off = -chicago.bounds[0] * x_scale\n",
    "y_off = -chicago.bounds[1] * y_scale\n",
    "\n",
    "# Transform geometry to the current frame\n",
    "def transform_latlong_to_frame(geometry):\n",
    "    return aff.affine_transform(geometry, [x_scale, 0, 0, y_scale, x_off, y_off])\n",
    "\n",
    "# Return the points within the input polygon\n",
    "def points_within(polygon):\n",
    "    bounds = polygon.bounds\n",
    "    offset_x = int(np.floor(bounds[0]))\n",
    "    offset_y = int(np.floor(bounds[1]))\n",
    "    width = int(np.ceil(bounds[2])-offset_x)\n",
    "    height = int(np.ceil(bounds[3])-offset_y)\n",
    "    pixel_x = []\n",
    "    pixel_y = []\n",
    "    for x in range(offset_x, offset_x+width):\n",
    "        for y in range(offset_y, offset_y+height):\n",
    "            if polygon.contains(geo.Point(x,y)):\n",
    "                pixel_x.append(x)\n",
    "                pixel_y.append(y)\n",
    "    return (pixel_x, pixel_y)\n",
    "\n",
    "# Apply the value in column_name of the given record at given coordinates of given frame \n",
    "def column_value_at_coordinate(record, column_name, frame):\n",
    "    frame[record.Coordinates[0], record.Coordinates[1]] = record[column_name]\n",
    "\n",
    "# Convert a pair \"(XXXX, XXXX)\" to a geographic point\n",
    "def convert_pair_to_point(latlong):\n",
    "    lat, long = latlong.replace('(','').replace(')','').split(',')\n",
    "    return geo.Point(float(long), float(lat))\n",
    "\n",
    "# Function for converting and placing weird format latlong into frame pixels\n",
    "def mark_at_latlong(latlong, frame):\n",
    "    location = transform_latlong_to_frame(convert_pair_to_point(latlong))\n",
    "    frame[int(location.x)][int(location.y)] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "# EMPTY PIXELS #\n",
    "################\n",
    "pixel_points = []\n",
    "for x in range(X_MAX_PIXELS):\n",
    "    for y in range(Y_MAX_PIXELS):\n",
    "        pixel_points.append(geo.Point(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Street frame created and saved.\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# CREATE STREET FRAME #\n",
    "#######################\n",
    "streets = pd.read_excel(processed_dataset_paths_xlsx % 'Streets')\n",
    "\n",
    "# Load streets\n",
    "street_lines = []\n",
    "for encoding in streets.Centerline:\n",
    "    street_lines.append(wkt.loads(encoding))\n",
    "chicago_streets = transform_latlong_to_frame(geo.MultiLineString(street_lines))\n",
    "# Buffer streets\n",
    "buffered_streets = chicago_streets.buffer(0.5)\n",
    "# Rasterize to numpy array\n",
    "street_frame = np.array(list(map(prep(buffered_streets).contains, pixel_points))).reshape((X_MAX_PIXELS,Y_MAX_PIXELS))\n",
    "\n",
    "np.savez_compressed(CNN_final_path + 'Streets Frame', street_frame=street_frame)\n",
    "del streets, street_lines, chicago_streets, buffered_streets, street_frame\n",
    "print('Street frame created and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waterway frame created and saved.\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# CREATE WATERWAYS FRAME #\n",
    "##########################\n",
    "waterways = pd.read_csv(processed_dataset_paths_csv % 'Waterways')\n",
    "\n",
    "# Load waterways\n",
    "waterway_outlines = []\n",
    "for encoding in waterways.Outline:\n",
    "    waterway_outlines.extend(wkt.loads(encoding).geoms)\n",
    "chicago_waterways = transform_latlong_to_frame(geo.MultiPolygon(waterway_outlines))\n",
    "# Rasterize to numpy array\n",
    "waterway_frame = np.array(list(map(prep(chicago_waterways).contains, pixel_points))).reshape((X_MAX_PIXELS,Y_MAX_PIXELS))\n",
    "\n",
    "np.savez_compressed(CNN_final_path + 'Waterway Frame', waterway_frame=waterway_frame)\n",
    "del waterways, waterway_outlines, chicago_waterways, waterway_frame\n",
    "print('Waterway frame created and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Park frame created and saved.\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# PARKS FRAME #\n",
    "###############\n",
    "parks = pd.read_csv(processed_dataset_paths_csv % 'Parks')\n",
    "\n",
    "# Load parks\n",
    "park_outlines = []\n",
    "for encoding in parks.Outline:\n",
    "    park_outlines.append(wkt.loads(encoding))\n",
    "chicago_parks = transform_latlong_to_frame(geo.MultiPolygon(park_outlines))\n",
    "# Rasterize to numpy array\n",
    "park_frame = np.array(list(map(prep(chicago_parks).contains, pixel_points))).reshape((X_MAX_PIXELS,Y_MAX_PIXELS))\n",
    "\n",
    "np.savez_compressed(CNN_final_path + 'Park Frame', park_frame=park_frame)\n",
    "del parks, park_outlines, chicago_parks, park_frame\n",
    "print('Park frame created and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest frame created and saved.\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# FORESTS FRAME #\n",
    "#################\n",
    "forests = pd.read_csv(processed_dataset_paths_csv % 'Forests')\n",
    "\n",
    "# Load forests\n",
    "forest_outlines = []\n",
    "for encoding in forests.Outline:\n",
    "    forest = wkt.loads(encoding)\n",
    "    if type(forest) is geo.Polygon:\n",
    "        forest_outlines.append(forest)\n",
    "    elif type(forest) is geo.MultiPolygon:\n",
    "        forest_outlines.extend(forest.geoms)\n",
    "chicago_forests = transform_latlong_to_frame(geo.MultiPolygon(forest_outlines))\n",
    "# Rasterize to numpy array\n",
    "forest_frame = np.array(list(map(prep(chicago_forests).contains, pixel_points))).reshape((X_MAX_PIXELS,Y_MAX_PIXELS))\n",
    "\n",
    "np.savez_compressed(CNN_final_path + 'Forest Frame', forest_frame=forest_frame)\n",
    "del forests, forest_outlines, chicago_forests, forest_frame\n",
    "print('Forest frame created and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schools frame created and saved.\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# SCHOOLS FRAME #\n",
    "#################\n",
    "schools = pd.read_csv(processed_dataset_paths_csv % 'Schools')\n",
    "\n",
    "# Load schools\n",
    "school_outlines = []\n",
    "for encoding in schools.Outline:\n",
    "    school_outlines.append(wkt.loads(encoding))\n",
    "chicago_schools = transform_latlong_to_frame(geo.MultiPolygon(school_outlines))\n",
    "# Rasterize to numpy array\n",
    "school_frame = np.array(list(map(prep(chicago_schools).contains, pixel_points))).reshape((X_MAX_PIXELS,Y_MAX_PIXELS))\n",
    "\n",
    "np.savez_compressed(CNN_final_path + 'School Frame', school_frame=school_frame)\n",
    "del schools, school_outlines, chicago_schools, school_frame\n",
    "print('Schools frame created and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninhabitable buildings frame created.\n",
      "Sound building frames created.\n",
      "Minor repair building frames created.\n",
      "Major repair building frames created and saved.\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# BUILDINGS FRAME #\n",
    "###################\n",
    "buildings = pd.read_excel(processed_dataset_paths_xlsx % 'Buildings')\n",
    "\n",
    "# Load uninhabitable buildings\n",
    "uninhabitable_building_outlines = []\n",
    "for encoding in buildings.loc[buildings.Condition == 'UNINHABITABLE'].Footprint:\n",
    "    building = wkt.loads(encoding)\n",
    "    if type(building) is geo.Polygon:\n",
    "        uninhabitable_building_outlines.append(building)\n",
    "    elif type(building) is geo.MultiPolygon:\n",
    "        uninhabitable_building_outlines.extend(building.geoms)\n",
    "chicago_uninhabitable_buildings = transform_latlong_to_frame(geo.MultiPolygon(uninhabitable_building_outlines))\n",
    "# Rasterize to numpy array\n",
    "uninhabitable_building_frame = np.array(list(map(prep(chicago_uninhabitable_buildings).contains, pixel_points))).reshape((X_MAX_PIXELS,Y_MAX_PIXELS))\n",
    "print('Uninhabitable buildings frame created.')\n",
    "\n",
    "# Load habitable buildings\n",
    "# Find out which pixels are inside each buildings' footprint\n",
    "# Then, change the pixels in the frames accordingly\n",
    "\n",
    "# SOUND BUILDINGS\n",
    "sound_buildings = buildings[buildings.Condition == 'SOUND'].copy()\n",
    "sound_buildings['Coordinates'] = sound_buildings.Footprint.apply(lambda x: points_within(transform_latlong_to_frame(wkt.loads(x))))\n",
    "\n",
    "stories_of_sound_buildings_frame = np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "sound_buildings.apply(lambda building: column_value_at_coordinate(building, 'Stories', stories_of_sound_buildings_frame), axis=1)\n",
    "\n",
    "units_of_sound_buildings_frame = np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "sound_buildings.apply(lambda building: column_value_at_coordinate(building, 'Units', units_of_sound_buildings_frame), axis=1)\n",
    "\n",
    "area_of_sound_buildings_frame = np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "sound_buildings.apply(lambda building: column_value_at_coordinate(building, 'Square Footage', area_of_sound_buildings_frame), axis=1)\n",
    "print('Sound building frames created.')\n",
    "\n",
    "# MINOR REPAIR BUILDINGS\n",
    "minor_repair_buildings = buildings[buildings.Condition == 'NEEDS MINOR REPAIR'].copy()\n",
    "minor_repair_buildings['Coordinates'] = minor_repair_buildings.Footprint.apply(lambda x: points_within(transform_latlong_to_frame(wkt.loads(x))))\n",
    "\n",
    "stories_of_minor_repair_buildings_frame = np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "minor_repair_buildings.apply(lambda building: column_value_at_coordinate(building, 'Stories', stories_of_minor_repair_buildings_frame), axis=1)\n",
    "\n",
    "units_of_minor_repair_buildings_frame = np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "minor_repair_buildings.apply(lambda building: column_value_at_coordinate(building, 'Units', units_of_minor_repair_buildings_frame), axis=1)\n",
    "\n",
    "area_of_minor_repair_buildings_frame = np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "minor_repair_buildings.apply(lambda building: column_value_at_coordinate(building, 'Square Footage', area_of_minor_repair_buildings_frame), axis=1)\n",
    "print('Minor repair building frames created.')\n",
    "\n",
    "# MAJOR REPAIR BUILDINGS\n",
    "major_repair_buildings = buildings[buildings.Condition == 'NEEDS MAJOR REPAIR'].copy()\n",
    "major_repair_buildings['Coordinates'] = major_repair_buildings.Footprint.apply(lambda x: points_within(transform_latlong_to_frame(wkt.loads(x))))\n",
    "\n",
    "stories_of_major_repair_buildings_frame = np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "major_repair_buildings.apply(lambda building: column_value_at_coordinate(building, 'Stories', stories_of_major_repair_buildings_frame), axis=1)\n",
    "\n",
    "units_of_major_repair_buildings_frame = np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "major_repair_buildings.apply(lambda building: column_value_at_coordinate(building, 'Units', units_of_major_repair_buildings_frame), axis=1)\n",
    "\n",
    "area_of_major_repair_buildings_frame = np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "major_repair_buildings.apply(lambda building: column_value_at_coordinate(building, 'Square Footage', area_of_major_repair_buildings_frame), axis=1)\n",
    "\n",
    "np.savez_compressed(CNN_final_path + 'Building Frames',\n",
    "                    uninhabitable_building_frame=uninhabitable_building_frame,\n",
    "                    stories_of_sound_buildings_frame=stories_of_sound_buildings_frame,\n",
    "                    units_of_sound_buildings_frame=units_of_sound_buildings_frame,\n",
    "                    area_of_sound_buildings_frame=area_of_sound_buildings_frame,\n",
    "                    stories_of_minor_repair_buildings_frame=stories_of_minor_repair_buildings_frame,\n",
    "                    units_of_minor_repair_buildings_frame=units_of_minor_repair_buildings_frame,\n",
    "                    area_of_minor_repair_buildings_frame=area_of_minor_repair_buildings_frame,\n",
    "                    stories_of_major_repair_buildings_frame=stories_of_major_repair_buildings_frame,\n",
    "                    units_of_major_repair_buildings_frame=units_of_major_repair_buildings_frame,\n",
    "                    area_of_major_repair_buildings_frame=area_of_major_repair_buildings_frame)\n",
    "del area_of_major_repair_buildings_frame\n",
    "del units_of_major_repair_buildings_frame\n",
    "del stories_of_major_repair_buildings_frame\n",
    "del major_repair_buildings\n",
    "del area_of_minor_repair_buildings_frame\n",
    "del units_of_minor_repair_buildings_frame\n",
    "del stories_of_minor_repair_buildings_frame\n",
    "del minor_repair_buildings\n",
    "del area_of_sound_buildings_frame\n",
    "del units_of_sound_buildings_frame\n",
    "del stories_of_sound_buildings_frame\n",
    "del sound_buildings\n",
    "del uninhabitable_building_frame\n",
    "del chicago_uninhabitable_buildings\n",
    "del uninhabitable_building_outlines\n",
    "del buildings\n",
    "print('Major repair building frames created and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries frame created and saved.\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# LIBRARIES FRAME #\n",
    "###################\n",
    "libraries = pd.read_excel(processed_dataset_paths_xlsx % 'Libraries')\n",
    "\n",
    "# Create empty frame\n",
    "library_frame = np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "# Load libraries\n",
    "libraries.Location.apply(lambda library: mark_at_latlong(library, library_frame))\n",
    "\n",
    "np.savez_compressed(CNN_final_path + 'Library Frame', library_frame=library_frame)\n",
    "del libraries, library_frame\n",
    "print('Libraries frame created and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life expectancy frame created and saved.\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# LIFE EXPECTANCY FRAME #\n",
    "#########################\n",
    "life_expectancy = pd.read_excel(processed_dataset_paths_xlsx % 'Life Expectancy')\n",
    "\n",
    "# Assign rasterized community to each year\n",
    "chicago_life_expectancy = life_expectancy.merge(rastered_communities, left_on='Community Area', right_on='Number')\n",
    "# Create an empty numpy frame for each year\n",
    "life_expectancy_frame = np.zeros((len(chicago_life_expectancy.Year.unique()), X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "# Apply the data to the frames\n",
    "for year in range(2001, 2019):\n",
    "    chicago_life_expectancy[chicago_life_expectancy.Year == year].apply(lambda record: column_value_at_coordinate(record, 'Life Expectancy', life_expectancy_frame[2001-year]), axis=1)\n",
    "\n",
    "np.savez_compressed(CNN_final_path + 'Life Expectancy Frames', life_expectancy_frame=life_expectancy_frame)\n",
    "del life_expectancy, chicago_life_expectancy, life_expectancy_frame\n",
    "print('Life expectancy frame created and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business frames created and saved.\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# BUSINESS FRAME #\n",
    "##################\n",
    "businesses = pd.read_excel(processed_dataset_paths_xlsx % 'Businesses')\n",
    "\n",
    "# Create empty frame\n",
    "business_frames = {'Food Service':np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS)),\n",
    "                   'Tobacco Sale':np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS)),\n",
    "                   'Alcohol Consumption':np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS)),\n",
    "                   'Package Store':np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS)),\n",
    "                   'Gas Station':np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS))}\n",
    "# Load businesses into frames\n",
    "for column, frame in business_frames.items():\n",
    "    businesses[businesses[column]==True].Location.apply(lambda business: mark_at_latlong(business, frame))\n",
    "\n",
    "np.savez_compressed(CNN_final_path + 'Business Frames', **business_frames)\n",
    "del businesses, business_frames\n",
    "print('Business frames created and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L entry frames created and saved.\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# L ENTRIES FRAME #\n",
    "###################\n",
    "L_entries = pd.read_excel(processed_dataset_paths_xlsx % 'L Entries')\n",
    "\n",
    "# Transform points from latlong to frame \n",
    "translated_L_entries = L_entries.copy()\n",
    "translated_L_entries['Location'] = L_entries.Location.apply(lambda point: transform_latlong_to_frame(wkt.loads(point)))\n",
    "translated_L_entries['Date'] = pd.to_datetime(L_entries.Date).apply(lambda this_row_timestamp: (this_row_timestamp.date()-START_DATE).days)\n",
    "num_days = (FINAL_DATE - START_DATE).days\n",
    "\n",
    "# pandas dataframe:\n",
    "#  column is L line\n",
    "#  row is day number\n",
    "#  Cell is numpy array:\n",
    "#    row 1 is x coordinate of rail station\n",
    "#    row 2 is y coordinate of rail station\n",
    "#    row 3 is number of entries for rail station\n",
    "entries_dataframe = pd.DataFrame(columns=L_LINES)\n",
    "\n",
    "# Loop over every line\n",
    "for line in entries_dataframe.columns:\n",
    "    line_filtered_entries = translated_L_entries.loc[translated_L_entries[line + ' Line']]\n",
    "    # Loop over every day\n",
    "    for day_index in range(num_days):\n",
    "        # Filter for that line and day\n",
    "        # Create the numpy arrays\n",
    "        filtered_entries = line_filtered_entries.loc[translated_L_entries['Date']==day_index]\n",
    "        num_entries = filtered_entries['Entries'].values\n",
    "        station_x = filtered_entries['Location'].apply(lambda point: int(point.x)).values\n",
    "        station_y = filtered_entries['Location'].apply(lambda point: int(point.y)).values\n",
    "        # filter y with x\n",
    "        x_filter = (station_x<X_MAX_PIXELS)&(station_x>=0)\n",
    "        station_y = station_y[x_filter]\n",
    "        num_entries = num_entries[x_filter]\n",
    "        station_x = station_x[x_filter]\n",
    "        # filter x with y\n",
    "        y_filter = (station_y<Y_MAX_PIXELS)&(station_y>=0)\n",
    "        station_x = station_x[y_filter]\n",
    "        num_entries = num_entries[y_filter]\n",
    "        station_y = station_y[y_filter]\n",
    "        entries_dataframe.loc[day_index,[line]] = json.dumps(np.array([station_x, station_y, num_entries]).tolist())\n",
    "\n",
    "entries_dataframe.to_csv(CNN_final_path + 'L Entries.csv')\n",
    "del L_entries, translated_L_entries, entries_dataframe\n",
    "print('L entry frames created and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Socioeconomic frames created and saved.\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# SOCIOECONOMIC INDICATORS FRAME #\n",
    "##################################\n",
    "\n",
    "socioeconomics = pd.read_excel(processed_dataset_paths_xlsx % 'Socioeconomics')\n",
    "\n",
    "# Assign rasterized community to each year\n",
    "chicago_socioeconomics = socioeconomics.merge(rastered_communities, left_on='Community Area', right_on='Number')\n",
    "# Create an empty numpy frame for each year\n",
    "socioeconomic_frame = np.zeros((len(SOCIO_INDICATORS), X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "# Apply the data to the frames\n",
    "for column_index, column in enumerate(SOCIO_INDICATORS):\n",
    "    chicago_socioeconomics.apply(lambda record: column_value_at_coordinate(record, column, socioeconomic_frame[column_index]), axis=1)\n",
    "\n",
    "np.savez_compressed(CNN_final_path + 'Socioeconomic Frames', socioeconomic_frame=socioeconomic_frame)\n",
    "del socioeconomics, chicago_socioeconomics, socioeconomic_frame\n",
    "print('Socioeconomic frames created and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# CRIME OUTPUT FRAME #\n",
    "######################\n",
    "\n",
    "crimes = pd.read_csv(processed_dataset_paths_csv % 'Crimes')\n",
    "\n",
    "crimes['Date'] = pd.to_datetime(crimes['Date'])\n",
    "crimes['Category'], categories = crimes['Category'].factorize()\n",
    "# Convert to numpy array:\n",
    "# - axis 0: day index\n",
    "# - axis 1: time slot\n",
    "# - axis 2: crime category\n",
    "# - axis 3: 0 is x locations, 1 is y locations\n",
    "# - axis 4: crime locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def place_crime_in_frame(crime_record, frame):\n",
    "    # Calculate the necessary indices\n",
    "    crime_date = crime_record['Date'].date()\n",
    "    crime_timstamp = crime_record['Date']\n",
    "    day_index = (crime_date - START_DATE).days\n",
    "    time_slot = int((crime_timstamp.hour + crime_timstamp.minute/60.)/24.*NUM_TIME_SLOTS)\n",
    "    # Create the crime location\n",
    "    location = transform_latlong_to_frame(convert_pair_to_point(crime_record['Location']))\n",
    "    # Find out the index of this crime\n",
    "    next_location_index = np.argwhere(frame[day_index][time_slot][crime_record['Category']] == np.nan)\n",
    "    # Store the crime\n",
    "    frame[day_index][time_slot][crime_record['Category']][0][next_location_index] = location.x\n",
    "    frame[day_index][time_slot][crime_record['Category']][1][next_location_index] = location.y\n",
    "    \n",
    "# Initially, assume 200 crimes per time slot in Chicago\n",
    "crime_frame = np.full((NUM_DAYS, NUM_TIME_SLOTS, len(crimes['Category'].unique()), 2, 200), np.nan)\n",
    "# Place all the crimes in the crime frame\n",
    "crimes.apply(lambda crime_record: place_crime_in_frame(crime_record, crime_frame), axis=1)\n",
    "\n",
    "# np.savez_compressed(CNN_final_path + 'Crimes', crime_frame=crime_frame)\n",
    "# del crimes, crime_frame\n",
    "# print('Crime frames created and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow(socioeconomic_frame[6])\n",
    "# img.imsave('waterway.png', waterway_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.max(np.argwhere(crimes == np.nan)[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(crimes['Category'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
