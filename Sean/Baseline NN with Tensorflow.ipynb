{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 62,
>>>>>>> 7ff1162c2b356f0da0563a9bc8c74c68df9ee0fd
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "# For EC2\n",
    "# import boto3"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x7fcf0696f080>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure that GPUs are being used\n",
    "tf.Session(config=tf.ConfigProto(log_device_placement=True))"
=======
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# HYPERPARAMETERS #\n",
    "###################\n",
    "\n",
    "np.random.seed(0)\n",
    "dev_set_proportion = 0.01\n",
    "test_set_proportion = 0.01\n",
    "train_set_proportion = 1 - (dev_set_proportion + test_set_proportion)\n",
    "learning_rate = 0.001\n",
    "number_epochs = 10000\n",
    "epochs_between_prints = 100\n",
    "epochs_between_saving_cost = 1\n",
    "minibatch_size = np.inf\n",
    "hidden_units_per_layer = 100\n",
    "num_hidden_layers = 14\n",
    "trial_number = 31\n",
    "dataset = \"14_November\"\n",
    "optimizer_name = 'Adam'"
>>>>>>> 7ff1162c2b356f0da0563a9bc8c74c68df9ee0fd
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 17,
>>>>>>> 7ff1162c2b356f0da0563a9bc8c74c68df9ee0fd
   "metadata": {},
   "outputs": [],
   "source": [
    "# For EC2\n",
    "# bucket = \"cs230\"\n",
    "# file_name = \"25_October.xlsx\"\n",
    "\n",
    "# s3 = boto3.client('s3') \n",
    "# # 's3' is a key word. create connection to S3 using default config and all buckets within S3\n",
    "\n",
    "# obj = s3.get_object(Bucket= bucket, Key= file_name) \n",
    "# # get object and file (key) from bucket\n",
    "\n",
    "# crime_data = pd.read_excel(obj['Body']) # 'Body' is a key word\n",
    "\n",
    "# For Local Machine\n",
    "crime_data = pd.read_excel('/Volumes/GoogleDrive/My Drive/Crime Data/Final Folder/'+dataset+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": null,
>>>>>>> 7ff1162c2b356f0da0563a9bc8c74c68df9ee0fd
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# SAVE EPOCH DATA AND PICKLE MODEL PARAMETERS #\n",
    "###############################################\n",
    "\n",
    "def checkpoint_progress(saver, session, cost, train_accuracy, dev_accuracy):\n",
    "    # Save the model hyperparameters (if not already saved)\n",
    "    # Record training progress\n",
    "    # \n",
    "    hyperparameter_dataframe = pd.read_excel('Trials/Hyperparameters.xlsx')\n",
    "    best_run_yet = save_epoch_data_and_check_dev_accuracy(hyperparameter_dataframe,\n",
    "                                                          cost,\n",
    "                                                          train_accuracy,\n",
    "                                                          dev_accuracy)\n",
    "    # If the last trial was the best, save the model data\n",
    "    if best_run_yet:\n",
    "        # Pickle the model parameters, since we have achieved the highest dev accuracy.\n",
    "        pickle_model(saver, session)\n",
    "\n",
    "def trial_exists(trial_number):\n",
    "    hyperparameter_data = pd.read_excel('Trials/Hyperparameters.xlsx')\n",
    "    return not (len(hyperparameter_data.loc[hyperparameter_data['Trial #'] == trial_number]) == 0)\n",
    "\n",
    "# Returns whether or not the current trial performs the best so far on dev accuracy\n",
    "def save_epoch_data_and_check_dev_accuracy(hyperparameter_dataframe, cost, train_accuracy, dev_accuracy):\n",
    "    # Check to see if this trial exists\n",
    "    if not trial_exists(trial_number):\n",
    "        # Trial doesn't exist.  Create a trial and save the current hyperparameters\n",
    "        hyperparameter_dataframe = hyperparameter_dataframe.append({\n",
    "            \"Trial #\"          : trial_number,\n",
    "            \"Dataset\"          : dataset,\n",
    "            \"Hidden Layers\"    : num_hidden_layers,\n",
    "            \"Hidden Units\"     : hidden_units_per_layer,\n",
    "            \"Train Set\"        : train_set_proportion,\n",
    "            \"Dev Set\"          : dev_set_proportion,\n",
    "            \"Test Set\"         : test_set_proportion,\n",
    "            \"Learning Rate\"    : learning_rate,\n",
    "            \"Number of Epochs\" : number_epochs,\n",
    "            \"Minibatch Size\"   : minibatch_size,\n",
    "            \"Optimizer\"        : optimizer_name\n",
    "        }, ignore_index=True)\n",
    "        hyperparameter_dataframe.loc[hyperparameter_dataframe['Trial #'] == trial_number, 'Epoch Train Accuracies'] = json.dumps([])\n",
    "        hyperparameter_dataframe.loc[hyperparameter_dataframe['Trial #'] == trial_number, 'Epoch Dev Accuracies'] = json.dumps([])\n",
    "        hyperparameter_dataframe.loc[hyperparameter_dataframe['Trial #'] == trial_number, 'Epoch Costs'] = json.dumps([])\n",
    "    # Trial exists (or was created), save new epoch data\n",
    "    train_accuracies = json.loads(hyperparameter_dataframe.loc[hyperparameter_dataframe['Trial #'] == trial_number, 'Epoch Train Accuracies'].values[0])\n",
    "    dev_accuracies = json.loads(hyperparameter_dataframe.loc[hyperparameter_dataframe['Trial #'] == trial_number, 'Epoch Dev Accuracies'].values[0])\n",
    "    costs = json.loads(hyperparameter_dataframe.loc[hyperparameter_dataframe['Trial #'] == trial_number, 'Epoch Costs'].values[0])\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    dev_accuracies.append(dev_accuracy)\n",
    "    costs.append(cost)\n",
    "    hyperparameter_dataframe.loc[hyperparameter_dataframe['Trial #'] == trial_number, 'Epoch Train Accuracies'] = json.dumps(train_accuracies)\n",
    "    hyperparameter_dataframe.loc[hyperparameter_dataframe['Trial #'] == trial_number, 'Epoch Dev Accuracies'] = json.dumps(dev_accuracies)\n",
    "    hyperparameter_dataframe.loc[hyperparameter_dataframe['Trial #'] == trial_number, 'Epoch Costs'] = json.dumps(costs)\n",
    "    # Retrieve the index of the best dev accuracy so far\n",
    "    best_dev_index = dev_accuracies.index(max(dev_accuracies))\n",
    "    # If the last trial was the best, save the model data\n",
    "    if best_dev_index == len(dev_accuracies)-1:\n",
    "        # Retrieve the accuracies and save them\n",
    "        best_dev_accuracy = dev_accuracies[best_dev_index]\n",
    "        corresponding_train_accuracy = train_accuracies[best_dev_index]\n",
    "        hyperparameter_dataframe.loc[hyperparameter_dataframe['Trial #'] == trial_number, 'Best Dev Accuracy'] = best_dev_accuracy\n",
    "        hyperparameter_dataframe.loc[hyperparameter_dataframe['Trial #'] == trial_number, 'Corresponding Train Accuracy'] = corresponding_train_accuracy\n",
    "    # Save the dataframe to file\n",
    "    writer = pd.ExcelWriter('Trials/Hyperparameters.xlsx')\n",
    "    hyperparameter_dataframe.to_excel(writer)\n",
    "    writer.save()\n",
    "    return (best_dev_index == len(dev_accuracies)-1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 65,
>>>>>>> 7ff1162c2b356f0da0563a9bc8c74c68df9ee0fd
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# SAVE MODEL PARAMETERS #\n",
    "#########################\n",
    "\n",
    "def pickle_model(saver, session):\n",
    "    save_path = saver.save(session, 'Trials/Pickled Models/Trial ' + str(trial_number) + '.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# RECOVER MODEL PARAMETERS #\n",
    "############################\n",
    "\n",
    "def recover_pickled_model(saver, session):\n",
    "    saver.restore(session, 'Trials/Pickled Models/Trial ' + str(trial_number) + '.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    # Creates a list of random minibatches from (X, Y)\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    \n",
    "    if mini_batch_size > m:\n",
    "        mini_batches.append((X,Y))\n",
    "    else:\n",
    "        # Step 1: Shuffle (X, Y)\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        shuffled_X = X[:, permutation]\n",
    "        shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "        # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "        num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "        for k in range(0, num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[:, k*mini_batch_size: (k+1)*(mini_batch_size)]\n",
    "            mini_batch_Y = shuffled_Y[:, k*mini_batch_size: (k+1)*(mini_batch_size)]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        # Handling the end case (last mini-batch < mini_batch_size)\n",
    "        if m % mini_batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[:, int(mini_batch_size*np.floor(m/mini_batch_size)): m]\n",
    "            mini_batch_Y = shuffled_Y[:, int(mini_batch_size*np.floor(m/mini_batch_size)): m]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 72,
>>>>>>> 7ff1162c2b356f0da0563a9bc8c74c68df9ee0fd
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# CREATE AND CONDITION DATA #\n",
    "#############################\n",
    "\n",
    "# Convert the dataframe to numpy arrays for features and labels\n",
    "features = crime_data.drop(columns=['categoryCode','closestGroceryStore']).values.T\n",
    "labels = crime_data.loc[:,'categoryCode'].values.reshape((-1,1)).T\n",
    "# Drop all NAs that were caught in the transfer\n",
    "feature_cols_with_nans = np.isnan(features).any(axis=0)\n",
    "features = features[:,~feature_cols_with_nans]\n",
    "labels = labels[:,~feature_cols_with_nans]\n",
    "label_cols_with_nans = np.isnan(labels).any(axis=0)\n",
    "features = features[:,~label_cols_with_nans]\n",
    "labels = labels[:,~label_cols_with_nans]\n",
    "\n",
    "n_x, m = features.shape\n",
    "n_y = len(crime_data.loc[:,'categoryCode'].unique())\n",
    "\n",
    "# Shuffle the data\n",
    "order = np.argsort(np.random.random(m))\n",
    "features = features[:,order]\n",
    "labels = labels[:,order]\n",
    "\n",
    "# One Hot Encode the Labels\n",
    "one_hot = np.zeros((n_y,m))\n",
    "one_hot[labels,np.arange(m)] = 1\n",
    "labels = one_hot\n",
    "\n",
    "# Split between train, dev, and test\n",
    "# Data structure: [     TRAIN     ][ DEV ][ TEST ]\n",
    "dev_start_index = int(train_set_proportion*m)\n",
    "test_start_index = dev_start_index + int(dev_set_proportion*m)\n",
    "\n",
    "X_train = features[:, 0:dev_start_index]\n",
    "Y_train = labels[:, 0:dev_start_index]\n",
    "\n",
    "X_dev = features[:, dev_start_index:test_start_index]\n",
    "Y_dev = labels[:, dev_start_index:test_start_index]\n",
    "\n",
    "X_test = features[:, test_start_index:]\n",
    "Y_test = labels[:, test_start_index:]\n",
    "\n",
    "# Normalize the inputs and outputs based on the training set mean and variance\n",
    "x_mean = X_train.mean(axis=1).reshape(n_x,1)\n",
    "x_variance = X_train.var(axis=1).reshape(n_x,1)\n",
    "\n",
    "X_train = (X_train-x_mean)/x_variance\n",
    "X_dev = (X_dev-x_mean)/x_variance\n",
    "X_test = (X_test-x_mean)/x_variance"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    # Creates a list of random minibatches from (X, Y)\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    \n",
    "    if mini_batch_size > m:\n",
    "        mini_batches.append((X,Y))\n",
    "    else:\n",
    "        # Step 1: Shuffle (X, Y)\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        shuffled_X = X[:, permutation]\n",
    "        shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "        # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "        num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "        for k in range(0, num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[:, k*mini_batch_size: (k+1)*(mini_batch_size)]\n",
    "            mini_batch_Y = shuffled_Y[:, k*mini_batch_size: (k+1)*(mini_batch_size)]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        # Handling the end case (last mini-batch < mini_batch_size)\n",
    "        if m % mini_batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[:, int(mini_batch_size*np.floor(m/mini_batch_size)): m]\n",
    "            mini_batch_Y = shuffled_Y[:, int(mini_batch_size*np.floor(m/mini_batch_size)): m]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
=======
   "execution_count": 77,
>>>>>>> 7ff1162c2b356f0da0563a9bc8c74c68df9ee0fd
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "0 Epochs\n",
      "\tCost:  3.511787176132202\n",
      "\tTrain Accuracy:  0.12465708\n",
      "\tDev Accuracy:  0.12255603\n"
=======
      "INFO:tensorflow:Restoring parameters from Trials/Pickled Models/Trial 31.ckpt\n",
      "200 Epochs\n",
      "\tCost:  2.3477210998535156\n",
      "\tTrain Accuracy:  0.29176053\n",
      "\tDev Accuracy:  0.29299\n",
      "300 Epochs\n",
      "\tCost:  2.1914584636688232\n",
      "\tTrain Accuracy:  0.31405708\n",
      "\tDev Accuracy:  0.31206486\n",
      "400 Epochs\n",
      "\tCost:  2.17575740814209\n",
      "\tTrain Accuracy:  0.31952322\n",
      "\tDev Accuracy:  0.3149261\n",
      "500 Epochs\n",
      "\tCost:  2.1601927280426025\n",
      "\tTrain Accuracy:  0.32315594\n",
      "\tDev Accuracy:  0.32207915\n",
      "600 Epochs\n",
      "\tCost:  2.154679775238037\n",
      "\tTrain Accuracy:  0.32448134\n",
      "\tDev Accuracy:  0.32503575\n",
      "700 Epochs\n",
      "\tCost:  2.1515767574310303\n",
      "\tTrain Accuracy:  0.3263089\n",
      "\tDev Accuracy:  0.32656175\n",
      "800 Epochs\n",
      "\tCost:  2.14793062210083\n",
      "\tTrain Accuracy:  0.3274319\n",
      "\tDev Accuracy:  0.32846925\n",
      "900 Epochs\n",
      "\tCost:  2.1359925270080566\n",
      "\tTrain Accuracy:  0.33102766\n",
      "\tDev Accuracy:  0.33686218\n",
      "1000 Epochs\n",
      "\tCost:  2.121910572052002\n",
      "\tTrain Accuracy:  0.33400154\n",
      "\tDev Accuracy:  0.33619457\n",
      "1100 Epochs\n",
      "\tCost:  2.104613780975342\n",
      "\tTrain Accuracy:  0.33770046\n",
      "\tDev Accuracy:  0.34363377\n",
      "1200 Epochs\n",
      "\tCost:  2.0951616764068604\n",
      "\tTrain Accuracy:  0.33547294\n",
      "\tDev Accuracy:  0.33810204\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-511b36a2ebd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mminibatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mminibatch_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminibatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mepoch_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mminibatch_cost\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_minibatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
>>>>>>> 7ff1162c2b356f0da0563a9bc8c74c68df9ee0fd
     ]
    }
   ],
   "source": [
    "#################\n",
    "# EXECUTE MODEL #\n",
    "#################\n",
    "\n",
    "# Calculate the cost from the network prediction\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.transpose(Z_hat),\n",
    "                                                                 labels=tf.transpose(Y)))\n",
    "optimizer = None\n",
    "# Create the optimizer\n",
    "if optimizer_name == 'Adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "else:\n",
    "    optimizer_name = 'GD'\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# Formula for calculating set accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Z_hat), tf.argmax(Y)), \"float\"))\n",
    "\n",
    "# Run the tf session to train and test\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    # If the trial already exists, pick up where we left off\n",
    "    starting_epoch = 0\n",
    "    if trial_exists(trial_number):\n",
    "        recover_pickled_model(saver, session)\n",
    "        hyperparameter_data = pd.read_excel('Trials/Hyperparameters.xlsx')\n",
    "        starting_epoch = len(json.loads(hyperparameter_data.loc[hyperparameter_data['Trial #'] == trial_number, 'Epoch Costs'].values[0]))\n",
    "    for epoch in range(starting_epoch, number_epochs):\n",
    "        epoch_cost = 0.\n",
    "        num_minibatches = int(m / minibatch_size)\n",
    "        if num_minibatches < 1: num_minibatches=1\n",
    "        minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "        for minibatch in minibatches:\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            _ , minibatch_cost = session.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "            epoch_cost += minibatch_cost / num_minibatches\n",
    "        \n",
    "        # Data Analysis\n",
    "        if epoch % epochs_between_prints == 0:\n",
    "            print('%i Epochs' % epoch)\n",
    "            print('\\tCost: ', epoch_cost)\n",
    "            print('\\tTrain Accuracy: ', accuracy.eval({X: X_train, Y: Y_train}))\n",
    "            print('\\tDev Accuracy: ', accuracy.eval({X: X_dev, Y: Y_dev}))\n",
    "\n",
    "        if epoch % epochs_between_saving_cost == 0:\n",
    "            checkpoint_progress(saver,\n",
    "                                session,\n",
    "                                epoch_cost,\n",
    "                                float(accuracy.eval({X: X_train, Y: Y_train})),\n",
    "                                float(accuracy.eval({X: X_dev, Y: Y_dev})))\n",
    "\n",
    "    # Calculate the accuracy on the train and dev sets\n",
    "    print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "    print (\"Dev Accuracy:\", accuracy.eval({X: X_dev, Y: Y_dev}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
