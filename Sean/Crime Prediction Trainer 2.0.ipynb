{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "# For EC2\n",
    "# import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# NON-HYPERPARAMETER CONSTANTS #\n",
    "################################\n",
    "final_dataset_location = '/Volumes/GoogleDrive/My Drive/Crime Data/Final Folder/'\n",
    "playground_dataset_location = '/Volumes/GoogleDrive/My Drive/Crime Data/Composute Data/Sean Workspace/'\n",
    "trial_file_location = '/Users/sean/Documents/Education/Stanford/230/Project/Sean/Trials/'\n",
    "pickled_model_location = '/Users/sean/Documents/Education/Stanford/230/Project/Sean/Trials/Pickled Models/Trial %d.ckpt'\n",
    "trial_file_format = 'Trial %d.xlsx'\n",
    "epochs_between_prints = 100\n",
    "hyperparameter_file_columns = ['Epoch Cost',\n",
    "                               'Train Accuracy',\n",
    "                               'Dev Accuracy',\n",
    "                               'Duration',\n",
    "                               'Dev Set Proportion',\n",
    "                               'Test Set Proportion',\n",
    "                               'Train Set Proportion',\n",
    "                               'Learning Rate',\n",
    "                               'Goal Total Epochs',\n",
    "                               'Minibatch Size',\n",
    "                               'Hidden Units per Layer',\n",
    "                               'Hidden Layers',\n",
    "                               'Dataset',\n",
    "                               'Optimizer Name',\n",
    "                               'L2 Regularization Lambda']\n",
    "remove_columns = ['ID', 'Date']\n",
    "target_column = 'Primary Type' # Discrete\n",
    "continuous_columns = ['Latitude', 'Longitude', 'PRECIPITATION', 'MAX TEMP', 'MIN TEMP']\n",
    "discrete_columns = ['Beat', 'District', 'Ward', 'Community Area', 'YEAR', 'MONTH', 'DAY', 'WEEKDAY', 'TIME_OF_DAY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# HYPERPARAMETERS #\n",
    "###################\n",
    "np.random.seed(0)\n",
    "dev_set_proportion = 0.01\n",
    "test_set_proportion = 0.01\n",
    "train_set_proportion = 1 - (dev_set_proportion + test_set_proportion)\n",
    "learning_rate = 0.001\n",
    "goal_total_epochs = 10000\n",
    "minibatch_size = np.inf\n",
    "hidden_units_per_layer = 100\n",
    "num_hidden_layers = 14\n",
    "trial_number = 36\n",
    "dataset = \"25_November.xlsx\"\n",
    "optimizer_name = 'Adam'\n",
    "regular_lambda = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For EC2\n",
    "# bucket = \"cs230\"\n",
    "# file_name = \"25_October.xlsx\"\n",
    "\n",
    "# s3 = boto3.client('s3') \n",
    "# # 's3' is a key word. create connection to S3 using default config and all buckets within S3\n",
    "\n",
    "# obj = s3.get_object(Bucket= bucket, Key= file_name) \n",
    "# # get object and file (key) from bucket\n",
    "\n",
    "# crime_data = pd.read_excel(obj['Body']) # 'Body' is a key word\n",
    "\n",
    "# For Local Machine\n",
    "crime_data = pd.read_excel(final_dataset_location + dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions #\n",
    "\n",
    "####################\n",
    "# EPOCH MANAGEMENT #\n",
    "####################\n",
    "\n",
    "def restore_model(saver, session):\n",
    "    # Before epoch, check for trial # in trial files\n",
    "    if os.path.isfile(trial_file_location+trial_file_format % trial_number):\n",
    "        print('Model found.  Restoring parameters.')\n",
    "        # If trial exists:\n",
    "        # 1. roll back (cost, train & dev accuracy) to epoch with highest dev accuracy.\n",
    "        trial_hyperparameters = pd.read_excel(trial_file_location+trial_file_format % trial_number)\n",
    "        # Find highest dev accuracy\n",
    "        best_dev_index = np.argmax(trial_hyperparameters.loc[:,'Dev Accuracy'].values)\n",
    "        # Delete all rows after this epoch\n",
    "        trial_hyperparameters = trial_hyperparameters[:best_dev_index+1]\n",
    "        # 2. restore model for the best dev accuracy\n",
    "        saver.restore(session, pickled_model_location % trial_number)\n",
    "        # Save the edited/new hyperparameter trial file\n",
    "        writer = pd.ExcelWriter(trial_file_location+trial_file_format % trial_number)\n",
    "        trial_hyperparameters.to_excel(writer)\n",
    "        writer.save()\n",
    "        # Return the number of epochs already trained\n",
    "        return len(trial_hyperparameters)\n",
    "    else:\n",
    "        print('No saved model.  Using default parameter initialization.')\n",
    "        return 0\n",
    "\n",
    "def epoch_teardown(saver, session, cost, training_accuracy, dev_accuracy, duration):\n",
    "    trial_hyperparameters = pd.DataFrame(columns=hyperparameter_file_columns)\n",
    "    # After epoch, check for hyperparameter file\n",
    "    if os.path.isfile(trial_file_location+trial_file_format % trial_number):\n",
    "        trial_hyperparameters = pd.read_excel(trial_file_location+trial_file_format % trial_number)\n",
    "        # Compare dev accuracy with all other epochs\n",
    "        max_dev_accuracy = np.max(trial_hyperparameters['Dev Accuracy'].values)\n",
    "        if dev_accuracy > max_dev_accuracy:\n",
    "            # If greatest, save model\n",
    "            saver.save(session, pickled_model_location % trial_number)\n",
    "    # Save hyperparameters, epoch cost, and training & dev accuracies\n",
    "    trial_hyperparameters = trial_hyperparameters.append({\n",
    "        'Epoch Cost' : cost,\n",
    "        'Train Accuracy' : training_accuracy,\n",
    "        'Dev Accuracy' : dev_accuracy,\n",
    "        'Duration' : duration,\n",
    "        'Dev Set Proportion' : dev_set_proportion,\n",
    "        'Test Set Proportion' : test_set_proportion,\n",
    "        'Train Set Proportion' : train_set_proportion,\n",
    "        'Learning Rate' : learning_rate,\n",
    "        'Goal Total Epochs' : goal_total_epochs,\n",
    "        'Minibatch Size' : minibatch_size,\n",
    "        'Hidden Units per Layer' : hidden_units_per_layer,\n",
    "        'Hidden Layers' : num_hidden_layers,\n",
    "        'Dataset' : dataset,\n",
    "        'Optimizer Name' : optimizer_name,\n",
    "        'L2 Regularization Lambda' : regular_lambda\n",
    "    }, ignore_index=True)\n",
    "    # Save the edited/new hyperparameter trial file\n",
    "    writer = pd.ExcelWriter(trial_file_location+trial_file_format % trial_number)\n",
    "    trial_hyperparameters.to_excel(writer)\n",
    "    writer.save()\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    # Creates a list of random minibatches from (X, Y)\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    \n",
    "    if mini_batch_size > m:\n",
    "        mini_batches.append((X,Y))\n",
    "    else:\n",
    "        # Step 1: Shuffle (X, Y)\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        shuffled_X = X[:, permutation]\n",
    "        shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "        # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "        num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "        for k in range(0, num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[:, k*mini_batch_size: (k+1)*(mini_batch_size)]\n",
    "            mini_batch_Y = shuffled_Y[:, k*mini_batch_size: (k+1)*(mini_batch_size)]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        # Handling the end case (last mini-batch < mini_batch_size)\n",
    "        if m % mini_batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[:, int(mini_batch_size*np.floor(m/mini_batch_size)): m]\n",
    "            mini_batch_Y = shuffled_Y[:, int(mini_batch_size*np.floor(m/mini_batch_size)): m]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "###################################\n",
    "# CREATE NEURAL NETWORK STRUCTURE #\n",
    "###################################\n",
    "\n",
    "def create_NN_structure(n_x, n_y):\n",
    "    ops.reset_default_graph()\n",
    "\n",
    "    # Create placeholders for the featuers and labels\n",
    "    X = tf.placeholder(tf.float32, shape=(n_x, None), name='X')\n",
    "    Y = tf.placeholder(tf.int32, shape=(n_y, None), name='Y')\n",
    "\n",
    "    # Create the network parameters\n",
    "    parameters = {}\n",
    "    for layer in range(num_hidden_layers+1):\n",
    "        previous_layer_size = (n_x if layer == 0 else hidden_units_per_layer)\n",
    "        this_layer_size = (n_y if layer == num_hidden_layers else hidden_units_per_layer)\n",
    "        W_name = 'W'+str(layer+1)\n",
    "        b_name = 'b'+str(layer+1)\n",
    "        parameters[W_name] = tf.get_variable(W_name,\n",
    "                                             (this_layer_size,previous_layer_size),\n",
    "                                             initializer=tf.contrib.layers.xavier_initializer(seed=1, uniform=False))\n",
    "        parameters[b_name] = tf.get_variable(b_name,\n",
    "                                             (this_layer_size,1),\n",
    "                                             initializer=tf.zeros_initializer())\n",
    "\n",
    "    # Hook up the network layers\n",
    "    A = X\n",
    "    Z = X\n",
    "    for layer in range(num_hidden_layers+1):\n",
    "        W = parameters['W'+str(layer+1)]\n",
    "        b = parameters['b'+str(layer+1)]\n",
    "        Z = W@A+b\n",
    "        A = tf.nn.relu(Z)\n",
    "    Z_hat = Z\n",
    "    Y_hat = tf.argmax(tf.transpose(tf.nn.softmax(tf.transpose(Z_hat))), axis=0)\n",
    "    \n",
    "    return Z_hat, Y_hat, X, Y, parameters\n",
    "\n",
    "#############################\n",
    "# CREATE AND CONDITION DATA #\n",
    "#############################\n",
    "\n",
    "def expand_one_hot_columns(crime_data):\n",
    "    conditioned_data = crime_data.copy()\n",
    "    print('Expanding one-hot columns')\n",
    "    for column_name in discrete_columns:\n",
    "        one_hot_expanded_columns = pd.get_dummies(conditioned_data[column_name])\n",
    "        conditioned_data = pd.concat([conditioned_data, one_hot_expanded_columns], axis=1).dropna()\n",
    "        conditioned_data = conditioned_data.drop(columns=[column_name])\n",
    "    return conditioned_data\n",
    "\n",
    "def create_and_condition_data(crime_data):\n",
    "    # Drop unnecessary columns\n",
    "    conditioned_data = crime_data.drop(columns=remove_columns)\n",
    "    # Expand one-hot columns\n",
    "    conditioned_data = expand_one_hot_columns(conditioned_data)\n",
    "    # Convert the dataframe to numpy arrays for features and labels\n",
    "    features = conditioned_data.drop(columns=[target_column]).values.T\n",
    "    labels = pd.get_dummies(conditioned_data[target_column]).values.T\n",
    "\n",
    "    # Drop all NAs that were caught in the transfer\n",
    "    feature_cols_with_nans = np.isnan(features).any(axis=0)\n",
    "    features = features[:,~feature_cols_with_nans]\n",
    "    labels = labels[:,~feature_cols_with_nans]\n",
    "    label_cols_with_nans = np.isnan(labels).any(axis=0)\n",
    "    features = features[:,~label_cols_with_nans]\n",
    "    labels = labels[:,~label_cols_with_nans]\n",
    "\n",
    "    n_x, m = features.shape\n",
    "    n_y, _ = labels.shape\n",
    "\n",
    "    # Shuffle the data\n",
    "    order = np.argsort(np.random.random(m))\n",
    "    features = features[:,order]\n",
    "    labels = labels[:,order]\n",
    "\n",
    "    # Split between train, dev, and test\n",
    "    # Data structure: [     TRAIN     ][ DEV ][ TEST ]\n",
    "    dev_start_index = int(train_set_proportion*m)\n",
    "    test_start_index = dev_start_index + int(dev_set_proportion*m)\n",
    "\n",
    "    X_train = features[:, 0:dev_start_index]\n",
    "    Y_train = labels[:, 0:dev_start_index]\n",
    "\n",
    "    X_dev = features[:, dev_start_index:test_start_index]\n",
    "    Y_dev = labels[:, dev_start_index:test_start_index]\n",
    "\n",
    "    X_test = features[:, test_start_index:]\n",
    "    Y_test = labels[:, test_start_index:]\n",
    "\n",
    "    # Normalize the inputs and outputs based on the training set mean and variance\n",
    "    x_mean = X_train.mean(axis=1).reshape(n_x,1)\n",
    "    x_variance = X_train.var(axis=1).reshape(n_x,1)\n",
    "\n",
    "    X_train = (X_train-x_mean)/x_variance\n",
    "    X_dev = (X_dev-x_mean)/x_variance\n",
    "    X_test = (X_test-x_mean)/x_variance\n",
    "    \n",
    "    return X_train, Y_train, X_dev, Y_dev, X_test, Y_test\n",
    "\n",
    "#################\n",
    "# EXECUTE MODEL #\n",
    "#################\n",
    "\n",
    "def execute_model():\n",
    "    global optimizer_name, trial_file_location\n",
    "\n",
    "    print('Conditioning Data')\n",
    "    X_train, Y_train, X_dev, Y_dev, X_test, Y_test = create_and_condition_data(crime_data)\n",
    "    n_x, m = X_train.shape\n",
    "    n_y, _ = Y_train.shape\n",
    "    print('Creating Network Structure')\n",
    "    Z_hat, Y_hat, X, Y, parameters = create_NN_structure(n_x, n_y)\n",
    "\n",
    "    # Calculate the cost from the network prediction\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.transpose(Z_hat),\n",
    "                                                                     labels=tf.transpose(Y)))\n",
    "    # Regularize the cost\n",
    "    for name, weights in parameters.items():\n",
    "        cost += regular_lambda * tf.nn.l2_loss(weights)\n",
    "    \n",
    "    optimizer = None\n",
    "    # Create the optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    else:\n",
    "        optimizer_name = 'GD'\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    # Formula for calculating set accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Z_hat), tf.argmax(Y)), \"float\"))\n",
    "\n",
    "    # Run the tf session to train and test\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        # If the trial already exists, pick up where we left off\n",
    "        starting_epoch = restore_model(saver, session)\n",
    "        print('Beginning Training')\n",
    "        for epoch in range(starting_epoch, goal_total_epochs):\n",
    "            start_time = time.time()\n",
    "            epoch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size)\n",
    "            if num_minibatches < 1: num_minibatches=1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                _ , minibatch_cost = session.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            # Display epoch results every so often\n",
    "            if epoch % epochs_between_prints == 0:\n",
    "                print('%i Epochs' % epoch)\n",
    "                print('\\tCost: %f' % epoch_cost)\n",
    "                print('\\tTrain Accuracy: %f' % accuracy.eval({X: X_train, Y: Y_train}))\n",
    "                print('\\tDev Accuracy: %f' % accuracy.eval({X: X_dev, Y: Y_dev}))\n",
    "\n",
    "            # Epoch over, tear down\n",
    "            epoch_teardown(saver,\n",
    "                           session,\n",
    "                           epoch_cost,\n",
    "                           float(accuracy.eval({X: X_train, Y: Y_train})),\n",
    "                           float(accuracy.eval({X: X_dev, Y: Y_dev})),\n",
    "                           elapsed_time)\n",
    "\n",
    "        # Calculate the accuracy on the train and dev sets\n",
    "        print('Reached Goal Number of Epochs.')\n",
    "        print('Final Train Accuracy: %f' % accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print('Final Dev Accuracy: %f' % accuracy.eval({X: X_dev, Y: Y_dev}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditioning Data\n",
      "Expanding one-hot columns\n",
      "Creating Network Structure\n",
      "No saved model.  Using default parameter initialization.\n",
      "Beginning Training\n",
      "0 Epochs\n",
      "\tCost: 11.201967\n",
      "\tTrain Accuracy: 0.154509\n",
      "\tDev Accuracy: 0.158798\n"
     ]
    }
   ],
   "source": [
    "execute_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
