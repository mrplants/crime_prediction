{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "# For EC2\n",
    "# import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# NON-HYPERPARAMETER CONSTANTS #\n",
    "################################\n",
    "processed_dataset_paths_xlsx = '/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/%s.xlsx' \n",
    "dataset_location = '/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/CNN Final/'\n",
    "trial_file_location = '/Users/sean/Documents/Education/Stanford/230/Project/Sean/Trials/'\n",
    "pickled_model_location = '/Users/sean/Documents/Education/Stanford/230/Project/Sean/Trials/Pickled Models/CNN Trial %d.ckpt'\n",
    "trial_file_format = 'CNN Trial %d.xlsx'\n",
    "epochs_between_prints = 100\n",
    "hyperparameter_file_columns = ['Epoch Cost',\n",
    "                               'Train Accuracy',\n",
    "                               'Dev Accuracy',\n",
    "                               'Duration',\n",
    "                               'Dev Set Proportion',\n",
    "                               'Test Set Proportion',\n",
    "                               'Train Set Proportion',\n",
    "                               'Learning Rate',\n",
    "                               'Goal Total Epochs',\n",
    "                               'Minibatch Size',\n",
    "                               'Hidden Units per Layer',\n",
    "                               'Hidden Layers',\n",
    "                               'Dataset',\n",
    "                               'Optimizer Name',\n",
    "                               'L2 Regularization Lambda']\n",
    "FIRST_DATE = datetime(2001, 1, 1)\n",
    "LAST_DATE = datetime(2018, 1, 1)\n",
    "NUM_DAYS = (LAST_DATE-FIRST_DATE).days\n",
    "# 25 channels + date channels (17+12+31+6) = 91\n",
    "X_MAX_PIXELS = 2048\n",
    "Y_MAX_PIXELS = X_MAX_PIXELS\n",
    "NUM_STATIC_CHANNELS = 21\n",
    "STREET_CHANNEL, WATERWAY_CHANNEL, PARK_CHANNEL, FOREST_CHANNEL, SCHOOL_CHANNEL, LIBRARY_CHANNEL, BUILDING_CHANNELS,_,_,_,_,_,_,_,_,_, BUSINESS_CHANNELS,_,_,_,_ = range(NUM_STATIC_CHANNELS)\n",
    "NUM_DYNAMIC_CHANNELS = 12\n",
    "MIN_TEMP_CHANNEL, MAX_TEMP_CHANNEL, PRECIPITATION_CHANNEL, LIFE_EXPECTANCY_CHANNEL, GREEN_LINE_CHANNEL, RED_LINE_CHANNEL, BROWN_LINE_CHANNEL, PURPLE_LINE_CHANNEL, YELLOW_LINE_CHANNEL, BLUE_LINE_CHANNEL, PINK_LINE_CHANNEL, ORANGE_LINE_CHANNEL = range(NUM_STATIC_CHANNELS,NUM_STATIC_CHANNELS+NUM_DYNAMIC_CHANNELS)\n",
    "YEAR_CHANNEL = NUM_STATIC_CHANNELS + NUM_DYNAMIC_CHANNELS\n",
    "MONTH_CHANNEL = YEAR_CHANNEL + 17\n",
    "DAY_CHANNEL = MONTH_CHANNEL + 12\n",
    "NUM_TIME_SLOTS = 12\n",
    "TIME_CHANNEL = DAY_CHANNEL + 31\n",
    "NUM_INPUT_CHANNELS = TIME_CHANNEL + NUM_TIME_SLOTS\n",
    "L_LINES = ['Green','Red','Brown','Purple','Yellow','Blue','Pink','Orange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# HYPERPARAMETERS #\n",
    "###################\n",
    "np.random.seed(0)\n",
    "dev_set_proportion = 0.01\n",
    "test_set_proportion = 0.01\n",
    "train_set_proportion = 1 - (dev_set_proportion + test_set_proportion)\n",
    "learning_rate = 0.0001\n",
    "goal_total_epochs = 10000\n",
    "minibatch_size = np.inf\n",
    "hidden_units_per_layer = 100\n",
    "num_hidden_layers = 14\n",
    "trial_number = 45\n",
    "optimizer_name = 'Adam'\n",
    "regular_lambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# IMPORT PROCESSED DATA #\n",
    "#########################\n",
    "weather = pd.read_excel(processed_dataset_paths_xlsx % 'Weather')\n",
    "weather['Date'] = pd.to_datetime(weather['Date'])\n",
    "# Create fast-access weather arrays\n",
    "min_temp_lookup = np.full((NUM_DAYS), np.nan)\n",
    "max_temp_lookup = np.full((NUM_DAYS), np.nan)\n",
    "precipitation_lookup = np.full((NUM_DAYS), np.nan)\n",
    "# Insert the weather data\n",
    "weather.apply(lambda record: extract_data_for_date(record, max_temp_lookup, 'Max Temp'), axis=1)\n",
    "weather.apply(lambda record: extract_data_for_date(record, min_temp_lookup, 'Min Temp'), axis=1)\n",
    "weather.apply(lambda record: extract_data_for_date(record, precipitation_lookup, 'Precipitation'), axis=1)\n",
    "# Interpolate over any NaN values\n",
    "nans, x= nan_helper(min_temp_lookup)\n",
    "min_temp_lookup[nans]= np.interp(x(nans), x(~nans), min_temp_lookup[~nans])\n",
    "nans, x= nan_helper(max_temp_lookup)\n",
    "max_temp_lookup[nans]= np.interp(x(nans), x(~nans), max_temp_lookup[~nans])\n",
    "nans, x= nan_helper(precipitation_lookup)\n",
    "precipitation_lookup[nans]= np.interp(x(nans), x(~nans), precipitation_lookup[~nans])\n",
    "    \n",
    "# For Local Machine\n",
    "street_frame = np.load(dataset_location + 'Streets Frame.npz')['street_frame']\n",
    "waterway_frame = np.load(dataset_location + 'Waterway Frame.npz')['waterway_frame']\n",
    "park_frame = np.load(dataset_location + 'Park Frame.npz')['park_frame']\n",
    "forest_frame = np.load(dataset_location + 'Forest Frame.npz')['forest_frame']\n",
    "school_frame = np.load(dataset_location + 'School Frame.npz')['school_frame']\n",
    "library_frame = np.load(dataset_location + 'Library Frame.npz')['library_frame']\n",
    "uninhabitable_building_frame = np.load(dataset_location + 'Building Frames.npz')['uninhabitable_building_frame']\n",
    "building_frames = {'Sound':{},\n",
    "                   'Minor Repair':{},\n",
    "                   'Major Repair':{}}\n",
    "with np.load(dataset_location + 'Building Frames.npz') as data:\n",
    "    building_frames['Sound']['Stories'] = data['stories_of_sound_buildings_frame']\n",
    "    building_frames['Sound']['Area'] = data['area_of_sound_buildings_frame']\n",
    "    building_frames['Sound']['Units'] = data['units_of_sound_buildings_frame']\n",
    "\n",
    "    building_frames['Minor Repair']['Stories'] = data['stories_of_minor_repair_buildings_frame']\n",
    "    building_frames['Minor Repair']['Area'] = data['area_of_minor_repair_buildings_frame']\n",
    "    building_frames['Minor Repair']['Units'] = data['units_of_minor_repair_buildings_frame']\n",
    "\n",
    "    building_frames['Major Repair']['Stories'] = data['stories_of_major_repair_buildings_frame']\n",
    "    building_frames['Major Repair']['Area'] = data['area_of_major_repair_buildings_frame']\n",
    "    building_frames['Major Repair']['Units'] = data['units_of_major_repair_buildings_frame']\n",
    "life_expectancy_frame = np.load(dataset_location + 'Life Expectancy Frames.npz')['life_expectancy_frame']\n",
    "business_frames = {}\n",
    "with np.load(dataset_location + 'Business Frames.npz') as data:\n",
    "    business_frames['Food Service'] = data['Food Service']\n",
    "    business_frames['Tobacco Sale'] = data['Tobacco Sale']\n",
    "    business_frames['Alcohol Consumption'] = data['Alcohol Consumption']\n",
    "    business_frames['Package Store'] = data['Package Store']\n",
    "    business_frames['Gas Station'] = data['Gas Station']\n",
    "L_entries_compressed = pd.read_csv(dataset_location + 'L Entries.csv')\n",
    "# Unpack the json strings to numpy\n",
    "for line in L_LINES:\n",
    "    L_entries_compressed[line] = L_entries_compressed['Line'].apply(lambda array_string: np.array(json.loads(array_string)))\n",
    "# L Entries is a pandas dataframe:\n",
    "#  column is L line\n",
    "#  row is day number\n",
    "#  Cell is numpy array:\n",
    "#    row 1 is x coordinate of rail station\n",
    "#    row 2 is y coordinate of rail station\n",
    "#    row 3 is number of entries for rail station\n",
    "\n",
    "# Combine static channels\n",
    "static_channels = np.zeros((NUM_STATIC_CHANNELS, X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "static_channels[STREET_CHANNEL] = street_frame\n",
    "static_channels[WATERWAY_CHANNEL] = waterway_frame\n",
    "static_channels[PARK_CHANNEL] = park_frame\n",
    "static_channels[FOREST_CHANNEL] = forest_frame\n",
    "static_channels[SCHOOL_CHANNEL] = school_frame\n",
    "static_channels[LIBRARY_CHANNEL] = library_frame\n",
    "static_channels[BUILDING_CHANNELS + 0] = uninhabitable_building_frame\n",
    "static_channels[BUILDING_CHANNELS + 1] = building_frames['Sound']['Stories']\n",
    "static_channels[BUILDING_CHANNELS + 2] = building_frames['Sound']['Area']\n",
    "static_channels[BUILDING_CHANNELS + 3] = building_frames['Sound']['Units']\n",
    "static_channels[BUILDING_CHANNELS + 4] = building_frames['Minor Repair']['Stories']\n",
    "static_channels[BUILDING_CHANNELS + 5] = building_frames['Minor Repair']['Area']\n",
    "static_channels[BUILDING_CHANNELS + 6] = building_frames['Minor Repair']['Units']\n",
    "static_channels[BUILDING_CHANNELS + 7] = building_frames['Major Repair']['Stories']\n",
    "static_channels[BUILDING_CHANNELS + 8] = building_frames['Major Repair']['Area']\n",
    "static_channels[BUILDING_CHANNELS + 9] = building_frames['Major Repair']['Units']\n",
    "static_channels[BUSINESS_CHANNELS + 0] = business_frames['Food Service']\n",
    "static_channels[BUSINESS_CHANNELS + 1] = business_frames['Tobacco Sale']\n",
    "static_channels[BUSINESS_CHANNELS + 2] = business_frames['Alcohol Consumption']\n",
    "static_channels[BUSINESS_CHANNELS + 3] = business_frames['Package Store']\n",
    "static_channels[BUSINESS_CHANNELS + 4] = business_frames['Gas Station']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[ 674 1057 1583 1344 1537  858 1360 1568 1522  962 1584  770 1546  911\\n  1645 1457 1151 1581 1547 1597 1468  817 1580  720 1200]\\n [1311 1303  852 1302 1305 1314  727  959 1305 1309  815 1312 1271 1312\\n   734  727 1297  890 1012  733 1304 1313  930 1311 1296]\\n [ 633  483  364  246 2059  700  540  199 1080  405  248  141  700  346\\n   391  230  357  427  448  144  217  399  211  170  213]]'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_entries_compressed['Green'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building CNN Data\n",
    "# Data that varies with Time and Location:\n",
    "# - Crime (OUTPUT - YOLO with crime and location)\n",
    "# - L entries (8 layers - one per line)\n",
    "# - Life Expectancy (1 layer)\n",
    "#\n",
    "# Data that varies with Time Only:\n",
    "# - Weather (3 layers - MIN TEMP, MAX TEMP, and PRECIPITATION)\n",
    "# - Date\n",
    "# - Time\n",
    "#\n",
    "# Data that varies with Location Only:\n",
    "# - Businesses (5 layers - types of businesses)\n",
    "# - Buildings (10 layers - stories|units|sqfeet for sound|minor repair|major repair.  Also uninhabitable or not.)\n",
    "# - Waterways (1 layer)\n",
    "# - Major Streets (1 layer)\n",
    "# - Libraries (1 layer)\n",
    "# - Public Parks (1 layer)\n",
    "# - Forests (1 layer)\n",
    "# - Schools (1 layer)\n",
    "\n",
    "def get_example_for_datetime(day_index, day, month, year, time_slot):\n",
    "    # Need to filter for any days during which we don't have L data\n",
    "    \n",
    "    # Add static channels first\n",
    "    input_data = np.zeros((NUM_INPUT_CHANNELS, X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "    input_data[:NUM_STATIC_CHANNELS] = static_channels\n",
    "    # Weather channels\n",
    "    input_data[MIN_TEMP_CHANNEL] = np.full((X_MAX_PIXELS, Y_MAX_PIXELS), min_temp_lookup[day_index])\n",
    "    input_data[MAX_TEMP_CHANNEL] = np.full((X_MAX_PIXELS, Y_MAX_PIXELS), max_temp_lookup[day_index])\n",
    "    input_data[PRECIPITATION_CHANNEL] = np.full((X_MAX_PIXELS, Y_MAX_PIXELS), precipitation_lookup[day_index])\n",
    "    # Life Expectancy Channel\n",
    "    input_data[LIFE_EXPECTANCY_CHANNEL] = life_expectancy_frame[year_index-FIRST_DATE.year]\n",
    "    # L Entry Channels\n",
    "    input_data[GREEN_LINE_CHANNEL, L_entries_compressed['Green'][0], L_entries_compressed['Green'][1]] = L_entries_compressed['Green'][2]\n",
    "    input_data[RED_LINE_CHANNEL, L_entries_compressed['Red'][0], L_entries_compressed['Red'][1]] = L_entries_compressed['Red'][2]\n",
    "    input_data[BROWN_LINE_CHANNEL, L_entries_compressed['Brown'][0], L_entries_compressed['Brown'][1]] = L_entries_compressed['Brown'][2]\n",
    "    input_data[PURPLE_LINE_CHANNEL, L_entries_compressed['Purple'][0], L_entries_compressed['Purple'][1]] = L_entries_compressed['Purple'][2]\n",
    "    input_data[YELLOW_LINE_CHANNEL, L_entries_compressed['Yellow'][0], L_entries_compressed['Yellow'][1]] = L_entries_compressed['Yellow'][2]\n",
    "    input_data[BLUE_LINE_CHANNEL, L_entries_compressed['Blue'][0], L_entries_compressed['Blue'][1]] = L_entries_compressed['Blue'][2]\n",
    "    input_data[PINK_LINE_CHANNEL, L_entries_compressed['Pink'][0], L_entries_compressed['Pink'][1]] = L_entries_compressed['Pink'][2]\n",
    "    input_data[ORANGE_LINE_CHANNEL, L_entries_compressed['Orange'][0], L_entries_compressed['Orange'][1]] = L_entries_compressed['Orange'][2]\n",
    "    # Date and Time channels\n",
    "    input_data[YEAR_CHANNEL + year_index] = np.ones((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "    input_data[MONTH_CHANNEL + month_index] = np.ones((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "    input_data[DAY_CHANNEL + requested_datetime.day] = np.ones((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "    input_data[TIME_CHANNEL + time_slot] = np.ones((X_MAX_PIXELS, Y_MAX_PIXELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9746479988098145\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "requested_datetime = datetime(2005, 10, 30, 5, 32)\n",
    "for _ in range(10):\n",
    "    get_example_for_datetime(requested_datetime)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions #\n",
    "\n",
    "####################\n",
    "# EPOCH MANAGEMENT #\n",
    "####################\n",
    "\n",
    "def extract_data_for_date(record, fast_lookup, column):\n",
    "    record_date = datetime(record.Date.year, record.Date.month, record.Date.day)\n",
    "    index = (record_date - FIRST_DATE).days\n",
    "    if index < NUM_DAYS and index >= 0:\n",
    "        fast_lookup[index] = record[column]\n",
    "\n",
    "def nan_helper(y):\n",
    "    \"\"\"Helper to handle indices and logical indices of NaNs.\n",
    "    Reference: https://stackoverflow.com/questions/6518811/interpolate-nan-values-in-a-numpy-array\n",
    "\n",
    "    Input:\n",
    "        - y, 1d numpy array with possible NaNs\n",
    "    Output:\n",
    "        - nans, logical indices of NaNs\n",
    "        - index, a function, with signature indices= index(logical_indices),\n",
    "          to convert logical indices of NaNs to 'equivalent' indices\n",
    "    Example:\n",
    "        >>> # linear interpolation of NaNs\n",
    "        >>> nans, x= nan_helper(y)\n",
    "        >>> y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    \"\"\"\n",
    "\n",
    "    return np.isnan(y), lambda z: z.nonzero()[0]\n",
    "\n",
    "def restore_model(saver, session):\n",
    "    # Before epoch, check for trial # in trial files\n",
    "    if os.path.isfile(trial_file_location+trial_file_format % trial_number):\n",
    "        print('Model found.  Restoring parameters.')\n",
    "        # If trial exists:\n",
    "        # 1. roll back (cost, train & dev accuracy) to epoch with highest dev accuracy.\n",
    "        trial_hyperparameters = pd.read_excel(trial_file_location+trial_file_format % trial_number)\n",
    "        # Find highest dev accuracy\n",
    "        best_dev_index = np.argmax(trial_hyperparameters.loc[:,'Dev Accuracy'].values)\n",
    "        # Delete all rows after this epoch\n",
    "        trial_hyperparameters = trial_hyperparameters[:best_dev_index+1]\n",
    "        # 2. restore model for the best dev accuracy\n",
    "        saver.restore(session, pickled_model_location % trial_number)\n",
    "        # Save the edited/new hyperparameter trial file\n",
    "        writer = pd.ExcelWriter(trial_file_location+trial_file_format % trial_number)\n",
    "        trial_hyperparameters.to_excel(writer)\n",
    "        writer.save()\n",
    "        # Return the number of epochs already trained\n",
    "        return len(trial_hyperparameters)\n",
    "    else:\n",
    "        print('No saved model.  Using default parameter initialization.')\n",
    "        return 0\n",
    "\n",
    "def epoch_teardown(saver, session, cost, training_accuracy, dev_accuracy, duration):\n",
    "    trial_hyperparameters = pd.DataFrame(columns=hyperparameter_file_columns)\n",
    "    # After epoch, check for hyperparameter file\n",
    "    if os.path.isfile(trial_file_location+trial_file_format % trial_number):\n",
    "        trial_hyperparameters = pd.read_excel(trial_file_location+trial_file_format % trial_number)\n",
    "        # Compare dev accuracy with all other epochs\n",
    "        max_dev_accuracy = np.max(trial_hyperparameters['Dev Accuracy'].values)\n",
    "        if dev_accuracy > max_dev_accuracy:\n",
    "            # If greatest, save model\n",
    "            saver.save(session, pickled_model_location % trial_number)\n",
    "    # Save hyperparameters, epoch cost, and training & dev accuracies\n",
    "    trial_hyperparameters = trial_hyperparameters.append({\n",
    "        'Epoch Cost' : cost,\n",
    "        'Train Accuracy' : training_accuracy,\n",
    "        'Dev Accuracy' : dev_accuracy,\n",
    "        'Duration' : duration,\n",
    "        'Dev Set Proportion' : dev_set_proportion,\n",
    "        'Test Set Proportion' : test_set_proportion,\n",
    "        'Train Set Proportion' : train_set_proportion,\n",
    "        'Learning Rate' : learning_rate,\n",
    "        'Goal Total Epochs' : goal_total_epochs,\n",
    "        'Minibatch Size' : minibatch_size,\n",
    "        'Hidden Units per Layer' : hidden_units_per_layer,\n",
    "        'Hidden Layers' : num_hidden_layers,\n",
    "        'Dataset' : dataset,\n",
    "        'Optimizer Name' : optimizer_name,\n",
    "        'L2 Regularization Lambda' : regular_lambda\n",
    "    }, ignore_index=True)\n",
    "    # Save the edited/new hyperparameter trial file\n",
    "    writer = pd.ExcelWriter(trial_file_location+trial_file_format % trial_number)\n",
    "    trial_hyperparameters.to_excel(writer)\n",
    "    writer.save()\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    # Creates a list of random minibatches from (X, Y)\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    \n",
    "    if mini_batch_size > m:\n",
    "        mini_batches.append((X,Y))\n",
    "    else:\n",
    "        # Step 1: Shuffle (X, Y)\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        shuffled_X = X[:, permutation]\n",
    "        shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "        # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "        num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "        for k in range(0, num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[:, k*mini_batch_size: (k+1)*(mini_batch_size)]\n",
    "            mini_batch_Y = shuffled_Y[:, k*mini_batch_size: (k+1)*(mini_batch_size)]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "\n",
    "        # Handling the end case (last mini-batch < mini_batch_size)\n",
    "        if m % mini_batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[:, int(mini_batch_size*np.floor(m/mini_batch_size)): m]\n",
    "            mini_batch_Y = shuffled_Y[:, int(mini_batch_size*np.floor(m/mini_batch_size)): m]\n",
    "            mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "            mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "###################################\n",
    "# CREATE NEURAL NETWORK STRUCTURE #\n",
    "###################################\n",
    "\n",
    "def create_NN_structure(n_x, n_y):\n",
    "    ops.reset_default_graph()\n",
    "\n",
    "    # Create placeholders for the featuers and labels\n",
    "    X = tf.placeholder(tf.float32, shape=(n_x, None), name='X')\n",
    "    Y = tf.placeholder(tf.int32, shape=(n_y, None), name='Y')\n",
    "\n",
    "    # Create the network parameters\n",
    "    parameters = {}\n",
    "    for layer in range(num_hidden_layers+1):\n",
    "        previous_layer_size = (n_x if layer == 0 else hidden_units_per_layer)\n",
    "        this_layer_size = (n_y if layer == num_hidden_layers else hidden_units_per_layer)\n",
    "        W_name = 'W'+str(layer+1)\n",
    "        b_name = 'b'+str(layer+1)\n",
    "        parameters[W_name] = tf.get_variable(W_name,\n",
    "                                             (this_layer_size,previous_layer_size),\n",
    "                                             initializer=tf.contrib.layers.xavier_initializer(seed=1, uniform=False))\n",
    "        parameters[b_name] = tf.get_variable(b_name,\n",
    "                                             (this_layer_size,1),\n",
    "                                             initializer=tf.zeros_initializer())\n",
    "\n",
    "    # Hook up the network layers\n",
    "    A = X\n",
    "    Z = X\n",
    "    for layer in range(num_hidden_layers+1):\n",
    "        W = parameters['W'+str(layer+1)]\n",
    "        b = parameters['b'+str(layer+1)]\n",
    "        Z = W@A+b\n",
    "        A = tf.nn.relu(Z)\n",
    "    Z_hat = Z\n",
    "    Y_hat = tf.argmax(tf.transpose(tf.nn.softmax(tf.transpose(Z_hat))), axis=0)\n",
    "    \n",
    "    return Z_hat, Y_hat, X, Y, parameters\n",
    "\n",
    "#############################\n",
    "# CREATE AND CONDITION DATA #\n",
    "#############################\n",
    "\n",
    "def expand_one_hot_columns(crime_data):\n",
    "    conditioned_data = crime_data.copy()\n",
    "    print('Expanding one-hot columns')\n",
    "    for column_name in discrete_columns:\n",
    "        one_hot_expanded_columns = pd.get_dummies(conditioned_data[column_name])\n",
    "        conditioned_data = pd.concat([conditioned_data, one_hot_expanded_columns], axis=1).dropna()\n",
    "        conditioned_data = conditioned_data.drop(columns=[column_name])\n",
    "    return conditioned_data\n",
    "\n",
    "def create_and_condition_data(crime_data):\n",
    "    # Drop unnecessary columns\n",
    "    conditioned_data = crime_data.drop(columns=remove_columns)\n",
    "    # Expand one-hot columns\n",
    "    conditioned_data = expand_one_hot_columns(conditioned_data)\n",
    "    # Convert the dataframe to numpy arrays for features and labels\n",
    "    features = conditioned_data.drop(columns=[target_column]).values.T\n",
    "    labels = pd.get_dummies(conditioned_data[target_column]).values.T\n",
    "\n",
    "    # Drop all NAs that were caught in the transfer\n",
    "    feature_cols_with_nans = np.isnan(features).any(axis=0)\n",
    "    features = features[:,~feature_cols_with_nans]\n",
    "    labels = labels[:,~feature_cols_with_nans]\n",
    "    label_cols_with_nans = np.isnan(labels).any(axis=0)\n",
    "    features = features[:,~label_cols_with_nans]\n",
    "    labels = labels[:,~label_cols_with_nans]\n",
    "\n",
    "    _, m = features.shape\n",
    "    _, _ = labels.shape\n",
    "\n",
    "    # Shuffle the data\n",
    "    print('Shuffling data')\n",
    "    order = np.argsort(np.random.random(m))\n",
    "    features = features[:,order]\n",
    "    labels = labels[:,order]\n",
    "\n",
    "    # Split between train, dev, and test\n",
    "    # Data structure: [     TRAIN     ][ DEV ][ TEST ]\n",
    "    dev_start_index = int(train_set_proportion*m)\n",
    "    test_start_index = dev_start_index + int(dev_set_proportion*m)\n",
    "\n",
    "    X_train = features[:, 0:dev_start_index]\n",
    "    Y_train = labels[:, 0:dev_start_index]\n",
    "\n",
    "    X_dev = features[:, dev_start_index:test_start_index]\n",
    "    Y_dev = labels[:, dev_start_index:test_start_index]\n",
    "\n",
    "    X_test = features[:, test_start_index:]\n",
    "    Y_test = labels[:, test_start_index:]\n",
    "\n",
    "    x_variance = X_train.var(axis=1).reshape(-1,1)\n",
    "    # Check if variance is zero.  State which features will be removed.\n",
    "    should_keep = (x_variance!=0).reshape(-1)\n",
    "    removed_features = conditioned_data.drop(columns=[target_column]).columns[should_keep==False].tolist()\n",
    "    if len(removed_features) != 0:\n",
    "        print('Removed the following columns (variance = 0): ' + str(removed_features))\n",
    "    X_train = X_train[(x_variance!=0).reshape(-1),:]\n",
    "    X_dev = X_dev[(x_variance!=0).reshape(-1),:]\n",
    "    X_test = X_test[(x_variance!=0).reshape(-1),:]\n",
    "    \n",
    "    # Normalize the inputs and outputs based on the training set mean and variance\n",
    "    print('Normalizing input data')\n",
    "    x_mean = X_train.mean(axis=1).reshape(-1,1)\n",
    "    x_variance = X_train.var(axis=1).reshape(-1,1)\n",
    "    X_train = (X_train-x_mean)/x_variance\n",
    "    X_dev = (X_dev-x_mean)/x_variance\n",
    "    X_test = (X_test-x_mean)/x_variance\n",
    "    \n",
    "    return X_train, Y_train, X_dev, Y_dev, X_test, Y_test\n",
    "\n",
    "#################\n",
    "# EXECUTE MODEL #\n",
    "#################\n",
    "\n",
    "def execute_model():\n",
    "    global optimizer_name, trial_file_location\n",
    "\n",
    "    print('Conditioning Data')\n",
    "    X_train, Y_train, X_dev, Y_dev, X_test, Y_test = create_and_condition_data(crime_data)\n",
    "    n_x, m = X_train.shape\n",
    "    n_y, _ = Y_train.shape\n",
    "    print('Creating Network Structure')\n",
    "    Z_hat, Y_hat, X, Y, parameters = create_NN_structure(n_x, n_y)\n",
    "\n",
    "    # Calculate the cost from the network prediction\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.transpose(Z_hat),\n",
    "                                                                     labels=tf.transpose(Y)))\n",
    "    # Regularize the cost\n",
    "    for name, weights in parameters.items():\n",
    "        cost += regular_lambda * tf.nn.l2_loss(weights)\n",
    "    \n",
    "    optimizer = None\n",
    "    # Create the optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    else:\n",
    "        optimizer_name = 'GD'\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    # Formula for calculating set accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Z_hat), tf.argmax(Y)), \"float\"))\n",
    "\n",
    "    # Run the tf session to train and test\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        # If the trial already exists, pick up where we left off\n",
    "        starting_epoch = restore_model(saver, session)\n",
    "        print('Beginning Training')\n",
    "        for epoch in range(starting_epoch, goal_total_epochs):\n",
    "            start_time = time.time()\n",
    "            epoch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size)\n",
    "            if num_minibatches < 1: num_minibatches=1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                _ , minibatch_cost = session.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            # Display epoch results every so often\n",
    "            if epoch % epochs_between_prints == 0:\n",
    "                print('%i Epochs' % epoch)\n",
    "                print('\\tCost: %f' % epoch_cost)\n",
    "                print('\\tTrain Accuracy: %f' % accuracy.eval({X: X_train, Y: Y_train}))\n",
    "                print('\\tDev Accuracy: %f' % accuracy.eval({X: X_dev, Y: Y_dev}))\n",
    "\n",
    "            # Epoch over, tear down\n",
    "            epoch_teardown(saver,\n",
    "                           session,\n",
    "                           epoch_cost,\n",
    "                           float(accuracy.eval({X: X_train, Y: Y_train})),\n",
    "                           float(accuracy.eval({X: X_dev, Y: Y_dev})),\n",
    "                           elapsed_time)\n",
    "\n",
    "        # Calculate the accuracy on the train and dev sets\n",
    "        print('Reached Goal Number of Epochs.')\n",
    "        print('Final Train Accuracy: %f' % accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print('Final Dev Accuracy: %f' % accuracy.eval({X: X_dev, Y: Y_dev}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditioning Data\n",
      "Expanding one-hot columns\n",
      "Shuffling data\n",
      "Normalizing input data\n",
      "Creating Network Structure\n",
      "No saved model.  Using default parameter initialization.\n",
      "Beginning Training\n",
      "0 Epochs\n",
      "\tCost: 80.423340\n",
      "\tTrain Accuracy: 0.043532\n",
      "\tDev Accuracy: 0.044921\n",
      "100 Epochs\n",
      "\tCost: 67.558609\n",
      "\tTrain Accuracy: 0.225869\n",
      "\tDev Accuracy: 0.228994\n",
      "200 Epochs\n",
      "\tCost: 57.122234\n",
      "\tTrain Accuracy: 0.225815\n",
      "\tDev Accuracy: 0.228898\n",
      "300 Epochs\n",
      "\tCost: 48.506077\n",
      "\tTrain Accuracy: 0.225815\n",
      "\tDev Accuracy: 0.228898\n",
      "400 Epochs\n",
      "\tCost: 41.278923\n",
      "\tTrain Accuracy: 0.225815\n",
      "\tDev Accuracy: 0.228898\n",
      "500 Epochs\n",
      "\tCost: 35.191925\n",
      "\tTrain Accuracy: 0.225815\n",
      "\tDev Accuracy: 0.228898\n",
      "600 Epochs\n",
      "\tCost: 30.058361\n",
      "\tTrain Accuracy: 0.225815\n",
      "\tDev Accuracy: 0.228898\n",
      "700 Epochs\n",
      "\tCost: 25.724384\n",
      "\tTrain Accuracy: 0.225815\n",
      "\tDev Accuracy: 0.228898\n",
      "800 Epochs\n",
      "\tCost: 22.063263\n",
      "\tTrain Accuracy: 0.225815\n",
      "\tDev Accuracy: 0.228898\n",
      "900 Epochs\n",
      "\tCost: 18.968952\n",
      "\tTrain Accuracy: 0.225815\n",
      "\tDev Accuracy: 0.228898\n",
      "1000 Epochs\n",
      "\tCost: 16.354263\n",
      "\tTrain Accuracy: 0.225815\n",
      "\tDev Accuracy: 0.228898\n",
      "1100 Epochs\n",
      "\tCost: 14.146118\n",
      "\tTrain Accuracy: 0.225815\n",
      "\tDev Accuracy: 0.228898\n",
      "1200 Epochs\n",
      "\tCost: 12.282611\n",
      "\tTrain Accuracy: 0.225815\n",
      "\tDev Accuracy: 0.228898\n",
      "1300 Epochs\n",
      "\tCost: 10.712046\n",
      "\tTrain Accuracy: 0.225815\n",
      "\tDev Accuracy: 0.228898\n",
      "1400 Epochs\n",
      "\tCost: 9.390465\n",
      "\tTrain Accuracy: 0.225815\n",
      "\tDev Accuracy: 0.228898\n",
      "1500 Epochs\n",
      "\tCost: 8.280418\n",
      "\tTrain Accuracy: 0.225815\n",
      "\tDev Accuracy: 0.228898\n",
      "1600 Epochs\n",
      "\tCost: 7.350519\n",
      "\tTrain Accuracy: 0.225813\n",
      "\tDev Accuracy: 0.228898\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-f9624ca72d44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexecute_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-89-af9660177690>\u001b[0m in \u001b[0;36mexecute_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mminibatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mminibatch_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminibatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m                 \u001b[0mepoch_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mminibatch_cost\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_minibatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "execute_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
