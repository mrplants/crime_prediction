{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import datetime as dtm\n",
    "import random\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "# For EC2\n",
    "# import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# NON-HYPERPARAMETER CONSTANTS #\n",
    "################################\n",
    "processed_dataset_paths_xlsx = '/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/%s.xlsx' \n",
    "dataset_location = '/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/CNN Final/'\n",
    "trial_file_location = '/Users/sean/Documents/Education/Stanford/230/Project/Sean/Trials/'\n",
    "pickled_model_location = '/Users/sean/Documents/Education/Stanford/230/Project/Sean/Trials/Pickled Models/CNN Trial %d.ckpt'\n",
    "trial_file_format = 'CNN Trial %d.xlsx'\n",
    "epochs_between_prints = 100\n",
    "hyperparameter_file_columns = ['Epoch Cost',\n",
    "                               'Train Accuracy',\n",
    "                               'Dev Accuracy',\n",
    "                               'Duration',\n",
    "                               'Dev Set Proportion',\n",
    "                               'Test Set Proportion',\n",
    "                               'Train Set Proportion',\n",
    "                               'Learning Rate',\n",
    "                               'Goal Total Epochs',\n",
    "                               'Minibatch Size',\n",
    "                               'Hidden Units per Layer',\n",
    "                               'Hidden Layers',\n",
    "                               'Dataset',\n",
    "                               'Optimizer Name',\n",
    "                               'L2 Regularization Lambda']\n",
    "FIRST_DATE = datetime(2001, 1, 1)\n",
    "LAST_DATE = datetime(2018, 1, 1)\n",
    "NUM_DAYS = (LAST_DATE-FIRST_DATE).days\n",
    "# 25 channels + date channels (17+12+31+6) = 91\n",
    "X_MAX_PIXELS = 2048\n",
    "Y_MAX_PIXELS = X_MAX_PIXELS\n",
    "X_WINDOW_MAX_PIXELS = 64\n",
    "Y_WINDOW_MAX_PIXELS = X_WINDOW_MAX_PIXELS\n",
    "X_HALF_WINDOW_PIXELS = int(X_WINDOW_MAX_PIXELS/2)\n",
    "Y_HALF_WINDOW_PIXELS = X_HALF_WINDOW_PIXELS\n",
    "NUM_STATIC_CHANNELS = 28\n",
    "STREET_CHANNEL, WATERWAY_CHANNEL, PARK_CHANNEL, FOREST_CHANNEL, SCHOOL_CHANNEL, LIBRARY_CHANNEL, BUILDING_CHANNELS,_,_,_,_,_,_,_,_,_, BUSINESS_CHANNELS,_,_,_,_, SOCIO_CHANNELS,_,_,_,_,_,_ = range(NUM_STATIC_CHANNELS)\n",
    "NUM_DYNAMIC_CHANNELS = 12\n",
    "MIN_TEMP_CHANNEL, MAX_TEMP_CHANNEL, PRECIPITATION_CHANNEL, LIFE_EXPECTANCY_CHANNEL, L_CHANNELS,_,_,_,_,_,_,_ = range(NUM_STATIC_CHANNELS,NUM_STATIC_CHANNELS+NUM_DYNAMIC_CHANNELS)\n",
    "YEAR_CHANNEL = NUM_STATIC_CHANNELS + NUM_DYNAMIC_CHANNELS\n",
    "MONTH_CHANNEL = YEAR_CHANNEL # + 17\n",
    "DAY_CHANNEL = MONTH_CHANNEL + 12\n",
    "NUM_TIME_SLOTS = 12\n",
    "TIME_CHANNEL = DAY_CHANNEL + 31\n",
    "NUM_INPUT_CHANNELS = TIME_CHANNEL + NUM_TIME_SLOTS\n",
    "L_LINES = ['Green','Red','Brown','Purple','Yellow','Blue','Pink','Orange']\n",
    "CRIME_CATEGORIES = ['BATTERY', 'OTHER OFFENSE', 'ROBBERY', 'NARCOTICS', 'CRIMINAL DAMAGE',\n",
    "                    'WEAPONS VIOLATION', 'THEFT', 'BURGLARY', 'MOTOR VEHICLE THEFT',\n",
    "                    'PUBLIC PEACE VIOLATION', 'ASSAULT', 'CRIMINAL TRESPASS',\n",
    "                    'CRIM SEXUAL ASSAULT', 'INTERFERENCE WITH PUBLIC OFFICER', 'ARSON',\n",
    "                    'DECEPTIVE PRACTICE', 'LIQUOR LAW VIOLATION', 'KIDNAPPING',\n",
    "                    'SEX OFFENSE', 'OFFENSE INVOLVING CHILDREN', 'PROSTITUTION', 'HOMICIDE',\n",
    "                    'GAMBLING', 'INTIMIDATION', 'STALKING', 'OBSCENITY', 'PUBLIC INDECENCY',\n",
    "                    'HUMAN TRAFFICKING', 'CONCEALED CARRY LICENSE VIOLATION',\n",
    "                    'OTHER NARCOTIC VIOLATION', 'NON - CRIMINAL', 'NON-CRIMINAL',\n",
    "                    'NON-CRIMINAL (SUBJECT SPECIFIED)', 'RITUALISM', 'DOMESTIC VIOLENCE']\n",
    "INPUT_DATA_NORMALIZED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# HYPERPARAMETERS #\n",
    "###################\n",
    "np.random.seed(0)\n",
    "dev_set_proportion = 0.01\n",
    "test_set_proportion = 0.01\n",
    "train_set_proportion = 1 - (dev_set_proportion + test_set_proportion)\n",
    "learning_rate = 0.0001\n",
    "goal_total_epochs = 10000\n",
    "minibatch_size = np.inf\n",
    "hidden_units_per_layer = 100\n",
    "num_hidden_layers = 14\n",
    "trial_number = 45\n",
    "optimizer_name = 'Adam'\n",
    "regular_lambda = 0.1\n",
    "\n",
    "# Not added to file yet:\n",
    "ACCURACY_EVAL_BATCH_SIZE = 100\n",
    "MAX_NUM_CRIMES = 15\n",
    "MINIBATCH_SIZE = 100\n",
    "NUM_MINIBATCHES = 100\n",
    "# OUTPUT_GRID_X = 1\n",
    "# OUTPUT_GRID_Y = OUTPUT_GRID_X\n",
    "# NUM_ANCHOR_BOXES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# DATA PROCESSING UTILITY FUNCTIONS #\n",
    "#####################################\n",
    "\n",
    "def extract_data_for_date(record, fast_lookup, column):\n",
    "    record_date = datetime(record.Date.year, record.Date.month, record.Date.day)\n",
    "    index = (record_date - FIRST_DATE).days\n",
    "    if index < NUM_DAYS and index >= 0:\n",
    "        fast_lookup[index] = record[column]\n",
    "\n",
    "def nan_helper(y):\n",
    "    \"\"\"Helper to handle indices and logical indices of NaNs.\n",
    "    Reference: https://stackoverflow.com/questions/6518811/interpolate-nan-values-in-a-numpy-array\n",
    "\n",
    "    Input:\n",
    "        - y, 1d numpy array with possible NaNs\n",
    "    Output:\n",
    "        - nans, logical indices of NaNs\n",
    "        - index, a function, with signature indices= index(logical_indices),\n",
    "          to convert logical indices of NaNs to 'equivalent' indices\n",
    "    Example:\n",
    "        >>> # linear interpolation of NaNs\n",
    "        >>> nans, x= nan_helper(y)\n",
    "        >>> y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    \"\"\"\n",
    "\n",
    "    return np.isnan(y), lambda z: z.nonzero()[0]\n",
    "\n",
    "######################\n",
    "# DATAFRAME CREATION #\n",
    "######################\n",
    "\n",
    "# Building CNN Data\n",
    "# Data that varies with Time and Location:\n",
    "# - Crime (OUTPUT - YOLO with crime and location)\n",
    "# - L entries (8 layers - one per line)\n",
    "# - Life Expectancy (1 layer)\n",
    "#\n",
    "# Data that varies with Time Only:\n",
    "# - Weather (3 layers - MIN TEMP, MAX TEMP, and PRECIPITATION)\n",
    "# - Date\n",
    "# - Time\n",
    "#\n",
    "# Data that varies with Location Only:\n",
    "# - Businesses (5 layers - types of businesses)\n",
    "# - Buildings (10 layers - stories|units|sqfeet for sound|minor repair|major repair.  Also uninhabitable or not.)\n",
    "# - Waterways (1 layer)\n",
    "# - Major Streets (1 layer)\n",
    "# - Libraries (1 layer)\n",
    "# - Public Parks (1 layer)\n",
    "# - Forests (1 layer)\n",
    "# - Schools (1 layer)\n",
    "# - Socioeconomic channels (7 socioeconomic indicators)\n",
    "\n",
    "def normalize_input_data(layer_means, layer_variances):\n",
    "    global normalized_zeros, normalized_static_channels, normalized_min_temp_lookup, normalized_max_temp_lookup\n",
    "    global normalized_precipitation_lookup, normalized_life_expectancy_frame, normalized_L_entries_compressed\n",
    "    global normalized_months, normalized_days, normalized_time_slots, INPUT_DATA_NORMALIZED\n",
    "    \n",
    "    # Calculate the normalized zero with which to fill the minibatch inputs\n",
    "    zero_variance_locations = np.where(layer_variances == 0)\n",
    "    fixed_variances = np.copy(layer_variances)\n",
    "    fixed_variances[zero_variance_locations] = 1\n",
    "    normalized_zeros = layer_means / fixed_variances\n",
    "    # Normalize the static channels\n",
    "    normalized_static_channels = ((static_channels.transpose(1,2,0) - layer_means[:NUM_STATIC_CHANNELS]) / fixed_variances[:NUM_STATIC_CHANNELS]).transpose(2,0,1)\n",
    "    # Normalize the weather data\n",
    "    normalized_min_temp_lookup = (min_temp_lookup - layer_means[MIN_TEMP_CHANNEL]) / fixed_variances[MIN_TEMP_CHANNEL]\n",
    "    normalized_max_temp_lookup = (max_temp_lookup - layer_means[MAX_TEMP_CHANNEL]) / fixed_variances[MAX_TEMP_CHANNEL]\n",
    "    normalized_precipitation_lookup = (precipitation_lookup - layer_means[PRECIPITATION_CHANNEL]) / fixed_variances[PRECIPITATION_CHANNEL]\n",
    "    # Normalize life expectancy\n",
    "    normalized_life_expectancy_frame = (life_expectancy_frame - layer_means[LIFE_EXPECTANCY_CHANNEL]) / fixed_variances[LIFE_EXPECTANCY_CHANNEL]\n",
    "    # Normalize L Entries\n",
    "    normalized_L_entries_compressed = pd.DataFrame(columns = L_LINES)\n",
    "    def normalize_L_entries(location_entries, means, variances):\n",
    "        for line_index,line in enumerate(L_LINES):\n",
    "            normalized_entries = np.copy(location_entries[line].astype(np.float64))\n",
    "            normalized_entries[2] = (normalized_entries[2] - means[line_index]) / variances[line_index]\n",
    "            normalized_L_entries_compressed.loc[location_entries.name, [line]] = [normalized_entries]\n",
    "    L_entries_compressed.apply(normalize_L_entries,\n",
    "                               args = (layer_means[L_CHANNELS:L_CHANNELS+len(L_LINES)], fixed_variances[L_CHANNELS:L_CHANNELS+len(L_LINES)]),\n",
    "                               axis=1)\n",
    "    # Calculate normalized date and time 'ones'\n",
    "    normalized_months = (1-layer_means[MONTH_CHANNEL:MONTH_CHANNEL+12]) / fixed_variances[MONTH_CHANNEL:MONTH_CHANNEL+12]\n",
    "    normalized_days = (1-layer_means[DAY_CHANNEL:DAY_CHANNEL+31]) / fixed_variances[DAY_CHANNEL:DAY_CHANNEL+31]\n",
    "    normalized_time_slots = (1-layer_means[TIME_CHANNEL:TIME_CHANNEL+NUM_TIME_SLOTS]) / fixed_variances[TIME_CHANNEL:TIME_CHANNEL+NUM_TIME_SLOTS]\n",
    "    INPUT_DATA_NORMALIZED = True\n",
    "    \n",
    "def choose_random_crime():\n",
    "    # Sample randomly and uniformly from the crimes\n",
    "    this_crime_index = crime_indices[np.random.randint(len(crime_indices))]\n",
    "    random_day = this_crime_index[0]\n",
    "    random_time_slot = this_crime_index[1]\n",
    "    random_category = this_crime_index[2]\n",
    "    random_crime_index = this_crime_index[4]\n",
    "    return random_day, random_time_slot, random_category, random_crime_index\n",
    "\n",
    "def generate_random_mini_batch(batch_size, avoid_these_samples, layer_means = None, layer_variances = None):\n",
    "    # Generate samples\n",
    "    return generate_mini_batch(sample_index_and_location(batch_size, avoid_these_samples),\n",
    "                               layer_means = layer_means,\n",
    "                               layer_variances = layer_variances)\n",
    "\n",
    "def generate_mini_batch(samples, layer_means = None, layer_variances = None):\n",
    "    # Normalize the input data if necessary\n",
    "    if not INPUT_DATA_NORMALIZED:\n",
    "        if layer_means is None or layer_variances is None:\n",
    "            print('Usage Error: Please normalize the input data or pass in the layer means and variances to generate_mini_batch().')\n",
    "            print('Generating mini-batch with non-normalized input data.')\n",
    "        else:\n",
    "            normalize_input_data(layer_means, layer_variances)\n",
    "    day_index_samples = samples['day_index']\n",
    "    batch_size = len(day_index_samples)\n",
    "    years_for_samples = year_fast_lookup[day_index_samples]\n",
    "    months_for_samples = month_fast_lookup[day_index_samples]\n",
    "    days_for_samples = day_fast_lookup[day_index_samples]\n",
    "    time_slot_samples = samples['time_slot']\n",
    "    location_x_samples = samples['location_x']\n",
    "    location_y_samples = samples['location_y']\n",
    "\n",
    "    # Fill the mini-batches with normlized zeros\n",
    "    mini_batch_input = np.repeat(np.repeat(np.repeat(normalized_zeros[:,np.newaxis],\n",
    "                                                     X_WINDOW_MAX_PIXELS,\n",
    "                                                     axis=1)[:,:,np.newaxis],\n",
    "                                           Y_WINDOW_MAX_PIXELS,\n",
    "                                           axis=2)[np.newaxis,:,:,:],\n",
    "                                 batch_size,\n",
    "                                 axis=0)\n",
    "    mini_batch_output = np.zeros((batch_size, X_WINDOW_MAX_PIXELS, Y_WINDOW_MAX_PIXELS))\n",
    "\n",
    "    # Since indices are batched, cannot use the get_input, get_output locations above (they are single use only)\n",
    "    # Add static channels first (cannot vectorize here)\n",
    "    for batch_index in range(batch_size):\n",
    "        mini_batch_input[batch_index,:NUM_STATIC_CHANNELS] = normalized_static_channels[:,location_x_samples[batch_index]-X_HALF_WINDOW_PIXELS:location_x_samples[batch_index]+X_HALF_WINDOW_PIXELS, location_y_samples[batch_index]-Y_HALF_WINDOW_PIXELS:location_y_samples[batch_index]+Y_HALF_WINDOW_PIXELS]\n",
    "    # Weather channels\n",
    "    mini_batch_input[:,MIN_TEMP_CHANNEL] = np.repeat(np.repeat(normalized_min_temp_lookup[day_index_samples][:,np.newaxis], X_WINDOW_MAX_PIXELS, axis=1)[:,:,np.newaxis], Y_WINDOW_MAX_PIXELS, axis=2)\n",
    "    mini_batch_input[:,MAX_TEMP_CHANNEL] = np.repeat(np.repeat(normalized_max_temp_lookup[day_index_samples][:,np.newaxis], X_WINDOW_MAX_PIXELS, axis=1)[:,:,np.newaxis], Y_WINDOW_MAX_PIXELS, axis=2)\n",
    "    mini_batch_input[:,PRECIPITATION_CHANNEL] = np.repeat(np.repeat(normalized_precipitation_lookup[day_index_samples][:,np.newaxis], X_WINDOW_MAX_PIXELS, axis=1)[:,:,np.newaxis], Y_WINDOW_MAX_PIXELS, axis=2)\n",
    "    # Life Expectancy Channel (cannot vectorize here)\n",
    "    for batch_index in range(batch_size):\n",
    "        mini_batch_input[batch_index,LIFE_EXPECTANCY_CHANNEL] = normalized_life_expectancy_frame[years_for_samples[batch_index]-FIRST_DATE.year, location_x_samples[batch_index]-X_HALF_WINDOW_PIXELS:location_x_samples[batch_index]+X_HALF_WINDOW_PIXELS, location_y_samples[batch_index]-Y_HALF_WINDOW_PIXELS:location_y_samples[batch_index]+Y_HALF_WINDOW_PIXELS]\n",
    "    # L Entry Channels (cannot vectorize here)\n",
    "    for batch_index in range(batch_size):\n",
    "        for line_index, line in enumerate(L_LINES):\n",
    "            station_x_samples = (normalized_L_entries_compressed[line][day_index_samples[batch_index]][0] - location_x_samples[batch_index]-X_HALF_WINDOW_PIXELS).astype(np.int64)\n",
    "            station_y_samples = (normalized_L_entries_compressed[line][day_index_samples[batch_index]][1] - location_y_samples[batch_index]-Y_HALF_WINDOW_PIXELS).astype(np.int64)\n",
    "            entries = normalized_L_entries_compressed[line][day_index_samples[batch_index]][2]\n",
    "            x_filter = (station_x_samples >= 0) & (station_x_samples < X_WINDOW_MAX_PIXELS)\n",
    "            station_x_samples = station_x_samples[x_filter]\n",
    "            station_y_samples = station_y_samples[x_filter]\n",
    "            entries = entries[x_filter]\n",
    "            y_filter = (station_y_samples >= 0) & (station_y_samples < Y_WINDOW_MAX_PIXELS)\n",
    "            station_x_samples = station_x_samples[y_filter]\n",
    "            station_y_samples = station_y_samples[y_filter]\n",
    "            entries = entries[y_filter]\n",
    "            mini_batch_input[batch_index, L_CHANNELS+line_index, station_x_samples, station_y_samples] = entries\n",
    "    # Date and Time Channels\n",
    "#     mini_batch_input[:, YEAR_CHANNEL + years_for_samples - FIRST_DATE.year] = np.ones((batch_size, X_WINDOW_MAX_PIXELS, Y_WINDOW_MAX_PIXELS))\n",
    "    mini_batch_input[np.arange(batch_size), MONTH_CHANNEL + months_for_samples] = np.repeat(np.repeat(normalized_months[months_for_samples, np.newaxis, np.newaxis], X_WINDOW_MAX_PIXELS, axis=1), Y_WINDOW_MAX_PIXELS, axis=2)\n",
    "    mini_batch_input[np.arange(batch_size), DAY_CHANNEL + days_for_samples] = np.repeat(np.repeat(normalized_days[days_for_samples, np.newaxis, np.newaxis], X_WINDOW_MAX_PIXELS, axis=1), Y_WINDOW_MAX_PIXELS, axis=2)\n",
    "    mini_batch_input[np.arange(batch_size), TIME_CHANNEL + time_slot_samples] = np.repeat(np.repeat(normalized_time_slots[time_slot_samples, np.newaxis, np.newaxis], X_WINDOW_MAX_PIXELS, axis=1), Y_WINDOW_MAX_PIXELS, axis=2)\n",
    "    # Generate the corresponding output (Combine all categories for now)\n",
    "    # Place the crimes on the map\n",
    "    # - axis 0: day index\n",
    "    # - axis 1: time slot\n",
    "    # - axis 2: crime category\n",
    "    # - axis 3: 0 is x locations, 1 is y locations, 2 is categories\n",
    "    # - axis 4: crime locations\n",
    "    # (cannot vectorize here)\n",
    "    for batch_index in range(batch_size):\n",
    "        for category_index in range(len(CRIME_CATEGORIES)):\n",
    "            last_crime = np.argwhere(crime_frames[day_index_samples[batch_index]][time_slot_samples[batch_index]][category_index][0] == -1)[0][0]\n",
    "            crimes_x = crime_frames[day_index_samples[batch_index]][time_slot_samples[batch_index]][category_index][0][:last_crime]\n",
    "            crimes_y = crime_frames[day_index_samples[batch_index]][time_slot_samples[batch_index]][category_index][1][:last_crime]\n",
    "            crimes_x = crimes_x - (location_x_samples[batch_index] - X_HALF_WINDOW_PIXELS)\n",
    "            crimes_y = crimes_y - (location_y_samples[batch_index] - Y_HALF_WINDOW_PIXELS)\n",
    "            x_filter = (crimes_x >= 0) & (crimes_x < X_WINDOW_MAX_PIXELS)\n",
    "            crimes_x = crimes_x[x_filter]\n",
    "            crimes_y = crimes_y[x_filter]\n",
    "            y_filter = (crimes_y >= 0) & (crimes_y < Y_WINDOW_MAX_PIXELS)\n",
    "            crimes_x = crimes_x[y_filter]\n",
    "            crimes_y = crimes_y[y_filter]\n",
    "            mini_batch_output[batch_index,crimes_x,crimes_y] = 1\n",
    "    return mini_batch_input, mini_batch_output\n",
    "\n",
    "EMPTY_SAMPLE = {'day_index' :np.array([]),\n",
    "                'time_slot' :np.array([]),\n",
    "                'location_x':np.array([]),\n",
    "                'location_y':np.array([])}\n",
    "\n",
    "def sample_index_and_location(num_samples, avoid_these_samples={'Test':EMPTY_SAMPLE,'Dev':EMPTY_SAMPLE}):\n",
    "    # Randomly sample the time and location in our range.\n",
    "    # Use this function primarily to avoid the test and dev sets.\n",
    "    # avoid_these_samples is a dictionary with 'Test' and 'Dev' set indices as numpy arrays:\n",
    "    # - day_index\n",
    "    # - time_slot\n",
    "    # - location_x\n",
    "    # - location_y\n",
    "    avoid_day_index = np.concatenate((avoid_these_samples['Test']['day_index'], avoid_these_samples['Dev']['day_index']))\n",
    "    avoid_time_slot = np.concatenate((avoid_these_samples['Test']['time_slot'], avoid_these_samples['Dev']['time_slot']))\n",
    "    avoid_location_x = np.concatenate((avoid_these_samples['Test']['location_x'], avoid_these_samples['Dev']['location_x']))\n",
    "    avoid_location_y = np.concatenate((avoid_these_samples['Test']['location_y'], avoid_these_samples['Dev']['location_y']))\n",
    "    # Create the empty sample arrays\n",
    "    num_samples_taken = 0\n",
    "    day_index_samples = np.zeros(num_samples, dtype=np.int64)\n",
    "    time_slot_samples = np.zeros(num_samples, dtype=np.int64)\n",
    "    x_location_samples = np.zeros(num_samples, dtype=np.int64)\n",
    "    y_location_samples = np.zeros(num_samples, dtype=np.int64)\n",
    "    # Generate the samples, avoiding the specified ones as necessary\n",
    "    while (num_samples_taken < num_samples):\n",
    "        day_index, time_slot, category, crime_index = choose_random_crime()\n",
    "        location_x = crime_frames[day_index][time_slot][category][0][crime_index]\n",
    "        location_y = crime_frames[day_index][time_slot][category][1][crime_index]\n",
    "        # Need to randomize the location so that the crime is not in the center (watch out for the image boundaries)\n",
    "        window_min_x = max(X_HALF_WINDOW_PIXELS, location_x - X_HALF_WINDOW_PIXELS+1)\n",
    "        window_max_x = min(X_MAX_PIXELS-X_HALF_WINDOW_PIXELS, location_x + X_HALF_WINDOW_PIXELS-1)\n",
    "        window_min_y = max(Y_HALF_WINDOW_PIXELS, location_y - Y_HALF_WINDOW_PIXELS+1)\n",
    "        window_max_y = min(Y_MAX_PIXELS-Y_HALF_WINDOW_PIXELS, location_y + Y_HALF_WINDOW_PIXELS-1)\n",
    "        location_x = (window_min_x + np.random.randint(window_max_x - window_min_x)) if window_max_x > window_min_x else window_min_x\n",
    "        location_y = (window_min_y + np.random.randint(window_max_y - window_min_y)) if window_max_y > window_min_y else window_min_y\n",
    "        # Only accept these if they do not overlap with samples we are avoiding\n",
    "        day_should_be_avoided = np.isin(day_index_samples, day_index)\n",
    "        time_slot_should_be_avoided = np.isin(time_slot_samples, time_slot)\n",
    "        location_x_should_be_avoided = ((x_location_samples + X_HALF_WINDOW_PIXELS) > location_x) & ((x_location_samples - X_HALF_WINDOW_PIXELS) <= location_x)\n",
    "        location_y_should_be_avoided = ((y_location_samples + Y_HALF_WINDOW_PIXELS) > location_y) & ((y_location_samples - Y_HALF_WINDOW_PIXELS) <= location_y)\n",
    "        if np.any(day_should_be_avoided & time_slot_should_be_avoided & location_x_should_be_avoided & location_y_should_be_avoided):\n",
    "            continue\n",
    "        else:\n",
    "            # No need to avoid this sample.  Add it to our test set.\n",
    "            day_index_samples[num_samples_taken] = day_index\n",
    "            time_slot_samples[num_samples_taken] = time_slot\n",
    "            x_location_samples[num_samples_taken] = location_x\n",
    "            y_location_samples[num_samples_taken] = location_y\n",
    "            num_samples_taken+=1\n",
    "    return {'day_index' : day_index_samples,\n",
    "            'time_slot' :time_slot_samples,\n",
    "            'location_x':x_location_samples,\n",
    "            'location_y':y_location_samples}\n",
    "\n",
    "def get_input_full_single(day_index, day, month, year, time_slot):\n",
    "    input_data = np.zeros((NUM_INPUT_CHANNELS, X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "    # Add static channels first\n",
    "    input_data[:NUM_STATIC_CHANNELS] = static_channels\n",
    "    # Weather channels\n",
    "    input_data[MIN_TEMP_CHANNEL] = np.full((X_MAX_PIXELS, Y_MAX_PIXELS), min_temp_lookup[day_index])\n",
    "    input_data[MAX_TEMP_CHANNEL] = np.full((X_MAX_PIXELS, Y_MAX_PIXELS), max_temp_lookup[day_index])\n",
    "    input_data[PRECIPITATION_CHANNEL] = np.full((X_MAX_PIXELS, Y_MAX_PIXELS), precipitation_lookup[day_index])\n",
    "    # Life Expectancy Channel\n",
    "    input_data[LIFE_EXPECTANCY_CHANNEL] = life_expectancy_frame[year-FIRST_DATE.year]\n",
    "    # L Entry Channels\n",
    "    input_data[L_CHANNELS+0, L_entries_compressed['Green'][day_index][0], L_entries_compressed['Green'][day_index][1]] = L_entries_compressed['Green'][day_index][2]\n",
    "    input_data[L_CHANNELS+1, L_entries_compressed['Red'][day_index][0], L_entries_compressed['Red'][day_index][1]] = L_entries_compressed['Red'][day_index][2]\n",
    "    input_data[L_CHANNELS+2, L_entries_compressed['Brown'][day_index][0], L_entries_compressed['Brown'][day_index][1]] = L_entries_compressed['Brown'][day_index][2]\n",
    "    input_data[L_CHANNELS+3, L_entries_compressed['Purple'][day_index][0], L_entries_compressed['Purple'][day_index][1]] = L_entries_compressed['Purple'][day_index][2]\n",
    "    input_data[L_CHANNELS+4, L_entries_compressed['Yellow'][day_index][0], L_entries_compressed['Yellow'][day_index][1]] = L_entries_compressed['Yellow'][day_index][2]\n",
    "    input_data[L_CHANNELS+5, L_entries_compressed['Blue'][day_index][0], L_entries_compressed['Blue'][day_index][1]] = L_entries_compressed['Blue'][day_index][2]\n",
    "    input_data[L_CHANNELS+6, L_entries_compressed['Pink'][day_index][0], L_entries_compressed['Pink'][day_index][1]] = L_entries_compressed['Pink'][day_index][2]\n",
    "    input_data[L_CHANNELS+7, L_entries_compressed['Orange'][day_index][0], L_entries_compressed['Orange'][day_index][1]] = L_entries_compressed['Orange'][day_index][2]\n",
    "    # Date and Time channels\n",
    "#     input_data[YEAR_CHANNEL + year - FIRST_DATE.year] = np.ones((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "    input_data[MONTH_CHANNEL + month] = np.ones((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "    input_data[DAY_CHANNEL + day] = np.ones((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "    input_data[TIME_CHANNEL + time_slot] = np.ones((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "    return input_data\n",
    "\n",
    "def get_expected_output_full_single(day_index, time_slot, category):\n",
    "    # Create the map\n",
    "    output_data = np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "    # Place the crimes on the map\n",
    "    # - axis 0: day index\n",
    "    # - axis 1: time slot\n",
    "    # - axis 2: crime category\n",
    "    # - axis 3: 0 is x locations, 1 is y locations, 2 is categories\n",
    "    # - axis 4: crime locations\n",
    "    last_crime = np.argwhere(crime_frames[day_index][time_slot][category][0] == -1)[0][0]\n",
    "    x_locations = crime_frames[day_index][time_slot][category][0][:last_crime]\n",
    "    y_locations = crime_frames[day_index][time_slot][category][1][:last_crime]\n",
    "    output_data[x_locations,y_locations] = 1\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# IMPORT PROCESSED DATA #\n",
    "#########################\n",
    "\n",
    "weather = pd.read_excel(processed_dataset_paths_xlsx % 'Weather')\n",
    "weather['Date'] = pd.to_datetime(weather['Date'])\n",
    "# Create fast-access weather arrays\n",
    "min_temp_lookup = np.full((NUM_DAYS), np.nan)\n",
    "max_temp_lookup = np.full((NUM_DAYS), np.nan)\n",
    "precipitation_lookup = np.full((NUM_DAYS), np.nan)\n",
    "# Insert the weather data\n",
    "weather.apply(lambda record: extract_data_for_date(record, max_temp_lookup, 'Max Temp'), axis=1)\n",
    "weather.apply(lambda record: extract_data_for_date(record, min_temp_lookup, 'Min Temp'), axis=1)\n",
    "weather.apply(lambda record: extract_data_for_date(record, precipitation_lookup, 'Precipitation'), axis=1)\n",
    "# Interpolate over any NaN values\n",
    "nans, x= nan_helper(min_temp_lookup)\n",
    "min_temp_lookup[nans]= np.interp(x(nans), x(~nans), min_temp_lookup[~nans])\n",
    "nans, x= nan_helper(max_temp_lookup)\n",
    "max_temp_lookup[nans]= np.interp(x(nans), x(~nans), max_temp_lookup[~nans])\n",
    "nans, x= nan_helper(precipitation_lookup)\n",
    "precipitation_lookup[nans]= np.interp(x(nans), x(~nans), precipitation_lookup[~nans])\n",
    "    \n",
    "# Load static data\n",
    "street_frame = np.load(dataset_location + 'Streets Frame.npz')['street_frame']\n",
    "waterway_frame = np.load(dataset_location + 'Waterway Frame.npz')['waterway_frame']\n",
    "park_frame = np.load(dataset_location + 'Park Frame.npz')['park_frame']\n",
    "forest_frame = np.load(dataset_location + 'Forest Frame.npz')['forest_frame']\n",
    "school_frame = np.load(dataset_location + 'School Frame.npz')['school_frame']\n",
    "library_frame = np.load(dataset_location + 'Library Frame.npz')['library_frame']\n",
    "uninhabitable_building_frame = np.load(dataset_location + 'Building Frames.npz')['uninhabitable_building_frame']\n",
    "building_frames = {'Sound':{},\n",
    "                   'Minor Repair':{},\n",
    "                   'Major Repair':{}}\n",
    "with np.load(dataset_location + 'Building Frames.npz') as data:\n",
    "    building_frames['Sound']['Stories'] = data['stories_of_sound_buildings_frame']\n",
    "    building_frames['Sound']['Area'] = data['area_of_sound_buildings_frame']\n",
    "    building_frames['Sound']['Units'] = data['units_of_sound_buildings_frame']\n",
    "\n",
    "    building_frames['Minor Repair']['Stories'] = data['stories_of_minor_repair_buildings_frame']\n",
    "    building_frames['Minor Repair']['Area'] = data['area_of_minor_repair_buildings_frame']\n",
    "    building_frames['Minor Repair']['Units'] = data['units_of_minor_repair_buildings_frame']\n",
    "\n",
    "    building_frames['Major Repair']['Stories'] = data['stories_of_major_repair_buildings_frame']\n",
    "    building_frames['Major Repair']['Area'] = data['area_of_major_repair_buildings_frame']\n",
    "    building_frames['Major Repair']['Units'] = data['units_of_major_repair_buildings_frame']\n",
    "life_expectancy_frame = np.load(dataset_location + 'Life Expectancy Frames.npz')['life_expectancy_frame']\n",
    "business_frames = {}\n",
    "with np.load(dataset_location + 'Business Frames.npz') as data:\n",
    "    business_frames['Food Service'] = data['Food Service']\n",
    "    business_frames['Tobacco Sale'] = data['Tobacco Sale']\n",
    "    business_frames['Alcohol Consumption'] = data['Alcohol Consumption']\n",
    "    business_frames['Package Store'] = data['Package Store']\n",
    "    business_frames['Gas Station'] = data['Gas Station']\n",
    "L_entries_compressed = pd.read_csv(dataset_location + 'L Entries.csv')\n",
    "L_entries_compressed_as_array = np.zeros((len(L_entries_compressed), len(L_LINES)))\n",
    "# Unpack the json strings to numpy\n",
    "for line in L_LINES:\n",
    "    L_entries_compressed[line] = L_entries_compressed[line].apply(lambda array_string: np.array(json.loads(array_string)))\n",
    "# L Entries is a pandas dataframe:\n",
    "#  column is L line\n",
    "#  row is day number\n",
    "#  Cell is numpy array:\n",
    "#    row 1 is x coordinate of rail station\n",
    "#    row 2 is y coordinate of rail station\n",
    "#    row 3 is number of entries for rail station\n",
    "socioeconomic_frames = np.load(dataset_location + 'Socioeconomic Frames.npz')['socioeconomic_frame']\n",
    "crime_frames = np.load(dataset_location + 'Crimes.npz')['crime_frame']\n",
    "# Make it easy to convert from day_index to year, month, day\n",
    "year_fast_lookup = np.vectorize(lambda day_index: (FIRST_DATE+dtm.timedelta(days=int(day_index))).year)(np.arange(NUM_DAYS))\n",
    "month_fast_lookup = np.vectorize(lambda day_index: (FIRST_DATE+dtm.timedelta(days=int(day_index))).month)(np.arange(NUM_DAYS))-1\n",
    "day_fast_lookup = np.vectorize(lambda day_index: (FIRST_DATE+dtm.timedelta(days=int(day_index))).day)(np.arange(NUM_DAYS))-1\n",
    "# Make it easy to randomly choose crimes\n",
    "crime_indices = np.argwhere(crime_frames != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# CREATE STATIC CHANNELS #\n",
    "##########################\n",
    "static_channels = np.zeros((NUM_STATIC_CHANNELS, X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "static_channels[STREET_CHANNEL] = street_frame\n",
    "static_channels[WATERWAY_CHANNEL] = waterway_frame\n",
    "static_channels[PARK_CHANNEL] = park_frame\n",
    "static_channels[FOREST_CHANNEL] = forest_frame\n",
    "static_channels[SCHOOL_CHANNEL] = school_frame\n",
    "static_channels[LIBRARY_CHANNEL] = library_frame\n",
    "static_channels[BUILDING_CHANNELS + 0] = uninhabitable_building_frame\n",
    "static_channels[BUILDING_CHANNELS + 1] = building_frames['Sound']['Stories']\n",
    "static_channels[BUILDING_CHANNELS + 2] = building_frames['Sound']['Area']\n",
    "static_channels[BUILDING_CHANNELS + 3] = building_frames['Sound']['Units']\n",
    "static_channels[BUILDING_CHANNELS + 4] = building_frames['Minor Repair']['Stories']\n",
    "static_channels[BUILDING_CHANNELS + 5] = building_frames['Minor Repair']['Area']\n",
    "static_channels[BUILDING_CHANNELS + 6] = building_frames['Minor Repair']['Units']\n",
    "static_channels[BUILDING_CHANNELS + 7] = building_frames['Major Repair']['Stories']\n",
    "static_channels[BUILDING_CHANNELS + 8] = building_frames['Major Repair']['Area']\n",
    "static_channels[BUILDING_CHANNELS + 9] = building_frames['Major Repair']['Units']\n",
    "static_channels[BUSINESS_CHANNELS + 0] = business_frames['Food Service']\n",
    "static_channels[BUSINESS_CHANNELS + 1] = business_frames['Tobacco Sale']\n",
    "static_channels[BUSINESS_CHANNELS + 2] = business_frames['Alcohol Consumption']\n",
    "static_channels[BUSINESS_CHANNELS + 3] = business_frames['Package Store']\n",
    "static_channels[BUSINESS_CHANNELS + 4] = business_frames['Gas Station']\n",
    "static_channels[SOCIO_CHANNELS + 0] = socioeconomic_frames[0]\n",
    "static_channels[SOCIO_CHANNELS + 1] = socioeconomic_frames[1]\n",
    "static_channels[SOCIO_CHANNELS + 2] = socioeconomic_frames[2]\n",
    "static_channels[SOCIO_CHANNELS + 3] = socioeconomic_frames[3]\n",
    "static_channels[SOCIO_CHANNELS + 4] = socioeconomic_frames[4]\n",
    "static_channels[SOCIO_CHANNELS + 5] = socioeconomic_frames[5]\n",
    "static_channels[SOCIO_CHANNELS + 6] = socioeconomic_frames[6]\n",
    "\n",
    "# Generate global normalized data arrays\n",
    "normalized_zeros = np.zeros(NUM_INPUT_CHANNELS)\n",
    "# Normalize the static channels\n",
    "normalized_static_channels = static_channels\n",
    "# Normalize the weather data\n",
    "normalized_min_temp_lookup = min_temp_lookup\n",
    "normalized_max_temp_lookup = max_temp_lookup\n",
    "normalized_precipitation_lookup = precipitation_lookup\n",
    "# Normalize life expectancy\n",
    "normalized_life_expectancy_frame = life_expectancy_frame\n",
    "# Normalize L Entries\n",
    "normalized_L_entries_compressed = L_entries_compressed\n",
    "# Calculate normalized date and time 'ones'\n",
    "normalized_months = np.ones(12)\n",
    "normalized_days = np.ones(31)\n",
    "normalized_time_slots = np.ones(NUM_TIME_SLOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_and_variance(test, dev, sample_size=1000):\n",
    "    global INPUT_DATA_NORMALIZED\n",
    "    INPUT_DATA_NORMALIZED = False\n",
    "    # Generate a large number of inputs\n",
    "    inputs, outputs = generate_random_mini_batch(sample_size,\n",
    "                                                 {'Test':test, 'Dev': dev},\n",
    "                                                 layer_means=np.zeros(NUM_INPUT_CHANNELS),\n",
    "                                                 layer_variances=np.ones(NUM_INPUT_CHANNELS))\n",
    "    # Calculate the mean and variance of each channel for normalizing all future mini-batches\n",
    "    layer_means = np.zeros(NUM_INPUT_CHANNELS)\n",
    "    layer_variances = np.ones(NUM_INPUT_CHANNELS)\n",
    "    for layer in range(NUM_INPUT_CHANNELS):\n",
    "        layer_means[layer] = np.mean(inputs[:,layer,:,:])\n",
    "        layer_variances[layer] = np.var(inputs[:,layer,:,:])\n",
    "    return layer_means, layer_variances\n",
    "\n",
    "def transform_to_CNN_data(inputs, outputs):\n",
    "    m = inputs.shape[0]\n",
    "    x = np.transpose(inputs, (0,2,3,1))\n",
    "    y = np.zeros((m, MAX_NUM_CRIMES))\n",
    "    y[np.arange(m), np.sum(np.sum(outputs, axis=1), axis=1, dtype=np.int64)] = 1\n",
    "    return x, y\n",
    "\n",
    "def create_placeholders(input_width, input_height, input_channels, output_classes):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    input_height -- scalar, height of an input image\n",
    "    input_width -- scalar, width of an input image\n",
    "    input_channels -- scalar, number of channels of the input\n",
    "    output_classes -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input\n",
    "    Y -- placeholder for the input labels\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(None, input_height, input_width, input_channels))\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, output_classes))\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build the CNN. The shapes are:\n",
    "                        W1 : [4, 4, NUM_INPUT_CHANNELS, 8]\n",
    "                        W2 : [2, 2, 8, 16]\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "            \n",
    "    W1 = tf.get_variable(\"W1\", shape=(4,4,NUM_INPUT_CHANNELS,8), initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W2 = tf.get_variable(\"W2\", shape=(2,2,8,16), initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    # CONV2D: stride of 1, padding 'SAME'\n",
    "    Z1 = tf.nn.conv2d(X,W1, strides = [1,1,1,1], padding = 'SAME')\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    # MAXPOOL: window 8x8, sride 8, padding 'SAME'\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,8,8,1], strides = [1,8,8,1], padding = 'SAME')\n",
    "    # CONV2D: filters W2, stride 1, padding 'SAME'\n",
    "    Z2 = tf.nn.conv2d(P1,W2, strides = [1,1,1,1], padding = 'SAME')\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1,4,4,1], strides = [1,4,4,1], padding = 'SAME')\n",
    "    # FLATTEN\n",
    "    P2 = tf.contrib.layers.flatten(P2)\n",
    "    # FULLY-CONNECTED without non-linear activation function (no softmax quite yet).\n",
    "    # MAX_NUM_CRIMES neurons in output layer.\n",
    "    Z3 = tf.contrib.layers.fully_connected(P2, MAX_NUM_CRIMES, activation_fn=None)\n",
    "\n",
    "    return Z3\n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (MAX_NUM_CRIMES, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = Z3, labels = Y))\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def restore_model(saver, session):\n",
    "    # Before epoch, check for trial # in trial files\n",
    "    if os.path.isfile(trial_file_location+trial_file_format % trial_number):\n",
    "        print('Model found.  Restoring parameters.')\n",
    "        # If trial exists:\n",
    "        # 1. roll back (cost, train & dev accuracy) to epoch with highest dev accuracy.\n",
    "        trial_hyperparameters = pd.read_excel(trial_file_location+trial_file_format % trial_number)\n",
    "        # Find highest dev accuracy\n",
    "        best_dev_index = np.argmax(trial_hyperparameters.loc[:,'Dev Accuracy'].values)\n",
    "        # Delete all rows after this epoch\n",
    "        trial_hyperparameters = trial_hyperparameters[:best_dev_index+1]\n",
    "        # 2. restore model for the best dev accuracy\n",
    "        saver.restore(session, pickled_model_location % trial_number)\n",
    "        # Save the edited/new hyperparameter trial file\n",
    "        writer = pd.ExcelWriter(trial_file_location+trial_file_format % trial_number)\n",
    "        trial_hyperparameters.to_excel(writer)\n",
    "        writer.save()\n",
    "        # Return the number of epochs already trained\n",
    "        return len(trial_hyperparameters)\n",
    "    else:\n",
    "        print('No saved model.  Using default parameter initialization.')\n",
    "        return 0\n",
    "\n",
    "def epoch_teardown(saver, session, cost, training_accuracy, dev_accuracy, duration):\n",
    "    trial_hyperparameters = pd.DataFrame(columns=hyperparameter_file_columns)\n",
    "    # After epoch, check for hyperparameter file\n",
    "    if os.path.isfile(trial_file_location+trial_file_format % trial_number):\n",
    "        trial_hyperparameters = pd.read_excel(trial_file_location+trial_file_format % trial_number)\n",
    "        # Compare dev accuracy with all other epochs\n",
    "        max_dev_accuracy = np.max(trial_hyperparameters['Dev Accuracy'].values)\n",
    "        if dev_accuracy > max_dev_accuracy:\n",
    "            # If greatest, save model\n",
    "            saver.save(session, pickled_model_location % trial_number)\n",
    "    # Save hyperparameters, epoch cost, and training & dev accuracies\n",
    "    trial_hyperparameters = trial_hyperparameters.append({\n",
    "        'Epoch Cost' : cost,\n",
    "        'Train Accuracy' : training_accuracy,\n",
    "        'Dev Accuracy' : dev_accuracy,\n",
    "        'Duration' : duration,\n",
    "        'Dev Set Proportion' : dev_set_proportion,\n",
    "        'Test Set Proportion' : test_set_proportion,\n",
    "        'Train Set Proportion' : train_set_proportion,\n",
    "        'Learning Rate' : learning_rate,\n",
    "        'Goal Total Epochs' : goal_total_epochs,\n",
    "        'Minibatch Size' : minibatch_size,\n",
    "        'Hidden Units per Layer' : hidden_units_per_layer,\n",
    "        'Hidden Layers' : num_hidden_layers,\n",
    "        'Dataset' : dataset,\n",
    "        'Optimizer Name' : optimizer_name,\n",
    "        'L2 Regularization Lambda' : regular_lambda\n",
    "    }, ignore_index=True)\n",
    "    # Save the edited/new hyperparameter trial file\n",
    "    writer = pd.ExcelWriter(trial_file_location+trial_file_format % trial_number)\n",
    "    trial_hyperparameters.to_excel(writer)\n",
    "    writer.save()\n",
    "\n",
    "def execute_model():\n",
    "    \"\"\"\n",
    "    Implements a three-layer ConvNet in Tensorflow:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the dev images for calculating dev accuracy during training\n",
    "    x_dev, y_dev = transform_to_CNN_data(*generate_mini_batch(dev))\n",
    "    x_sample, y_sample = transform_to_CNN_data(*generate_random_mini_batch(1, {'Test':EMPTY_SAMPLE, 'Dev': EMPTY_SAMPLE}))\n",
    "\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n",
    "    _, n_H0, n_W0, n_C0 = x_sample.shape             \n",
    "    _, n_y = y_sample.shape                          \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    # Create Placeholders of the correct shape\n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    softmax_Z3 = tf.transpose(tf.nn.softmax(tf.transpose(Z3)))\n",
    "    # Formula for calculating set accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Z3, 1), tf.argmax(Y, 1)), \"float\"))\n",
    "\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as session:\n",
    "        # Run the initialization\n",
    "        session.run(init)\n",
    "        # If the trial already exists, pick up where we left off\n",
    "        starting_epoch = restore_model(saver, session)\n",
    "        # Do the training loop\n",
    "        for epoch in range(goal_total_epochs):\n",
    "            print('Beginning Training')\n",
    "            start_time = time.time()\n",
    "            epoch_cost = 0.\n",
    "            for _ in range(NUM_MINIBATCHES):\n",
    "                x_batch_train, y_batch_train = transform_to_CNN_data(*generate_random_mini_batch(MINIBATCH_SIZE, {'Test':test, 'Dev': dev}))\n",
    "                _ , batch_cost = session.run([optimizer, cost], feed_dict={X: x_batch_train, Y: y_batch_train})\n",
    "                epoch_cost += batch_cost / NUM_MINIBATCHES\n",
    "            elapsed_time = time.time() - start_time\n",
    "            x_train, y_train = transform_to_CNN_data(*generate_random_mini_batch(ACCURACY_EVAL_BATCH_SIZE, {'Test':test, 'Dev': dev}))\n",
    "            train_accuracy = accuracy.eval({X: x_train, Y: y_train})\n",
    "            dev_accuracy = accuracy.eval({X: x_dev, Y: y_dev})\n",
    "            # Display epoch results every so often\n",
    "            if epoch % epochs_between_prints == 0:\n",
    "                print('%i Epochs' % epoch)\n",
    "                print('\\tCost: %f' % epoch_cost)\n",
    "                print('\\tTrain Accuracy: %f' % train_accuracy)\n",
    "                print('\\tDev Accuracy: %f' % dev_accuracy)\n",
    "            # Epoch over, tear down\n",
    "            epoch_teardown(saver,\n",
    "                           session,\n",
    "                           epoch_cost,\n",
    "                           float(train_accuracy),\n",
    "                           float(dev_accuracy),\n",
    "                           elapsed_time)                \n",
    "\n",
    "        x_train, y_train = transform_to_CNN_data(*generate_random_mini_batch(ACCURACY_EVAL_BATCH_SIZE, {'Test':test, 'Dev': dev}))\n",
    "        # Calculate the accuracy on the train and dev sets\n",
    "        print('Reached Goal Number of Epochs.')\n",
    "        print('Final Train Accuracy: %f' % accuracy.eval({X: x_train, Y: y_train}))\n",
    "        print('Final Dev Accuracy: %f' % accuracy.eval({X: x_dev, Y: y_dev}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test and Dev samples generated.\n",
      "Mean and variance calculated.\n",
      "Input data normalized\n"
     ]
    }
   ],
   "source": [
    "# Create the Test and Dev sets\n",
    "test = sample_index_and_location(500)\n",
    "dev = sample_index_and_location(500, {'Test':test, 'Dev':EMPTY_SAMPLE})\n",
    "print('Test and Dev samples generated.')\n",
    "# Ensure the data is not normalized\n",
    "normalize_input_data(np.zeros(NUM_INPUT_CHANNELS), np.ones(NUM_INPUT_CHANNELS))\n",
    "layer_means, layer_variances = calculate_mean_and_variance(test, dev, sample_size=1000)\n",
    "print('Mean and variance calculated.')\n",
    "normalize_input_data(layer_means, layer_variances)\n",
    "print('Input data normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved model.  Using default parameter initialization.\n",
      "Beginning Training\n"
     ]
    }
   ],
   "source": [
    "execute_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
