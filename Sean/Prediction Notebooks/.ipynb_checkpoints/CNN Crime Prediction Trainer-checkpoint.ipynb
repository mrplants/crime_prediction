{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import datetime as dtm\n",
    "import random\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "USING_EC2 = True\n",
    "# For EC2\n",
    "if USING_EC2:\n",
    "    import boto3\n",
    "    from io import BytesIO\n",
    "\n",
    "###################\n",
    "# HYPERPARAMETERS #\n",
    "###################\n",
    "np.random.seed(0)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "goal_total_epochs = 1000\n",
    "trial_number = 7\n",
    "optimizer_name = 'Adam'\n",
    "regular_lambda = 0\n",
    "accuracy_eval_batch_size = 1000\n",
    "max_num_crimes = 20\n",
    "minibatch_size = 1000\n",
    "num_minibatches = 10\n",
    "output_grid_width = 1 # x position\n",
    "output_grid_height = output_grid_width # y position\n",
    "num_anchor_boxes = 5\n",
    "input_image_width = 64\n",
    "input_image_height = 64\n",
    "test_set_size = 1000\n",
    "dev_set_size = 1000\n",
    "network_description = 'CONV(f=4,s=1,\"same\")x8 => MAX-POOL(f=8) => CONV(f=4,s=1,\"same\")x16 => MAX-POOL(f=4) => FC(100) => FC(100) => SOFT(NUM_CRIMES)'\n",
    "num_samples_for_normalization = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# NON-HYPERPARAMETER CONSTANTS #\n",
    "################################\n",
    "# For EC2\n",
    "if USING_EC2:\n",
    "    bucket = 'cs230'\n",
    "    s3 = boto3.client('s3')\n",
    "processed_dataset_paths_xlsx = '/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/Processed/%s.xlsx' \n",
    "dataset_location = '/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/CNN Final/'\n",
    "trial_file_location = '/Users/sean/Documents/Education/Stanford/230/Project/Sean/Trials/'\n",
    "AWS_trial_file_location = '/home/ec2-user/cs230/crime_prediction/Sean/Trials/'\n",
    "if USING_EC2:\n",
    "    trial_file_location = AWS_trial_file_location\n",
    "pickled_model_location = '/Users/sean/Documents/Education/Stanford/230/Project/Sean/Trials/Pickled Models/CNN Trial %d.ckpt'\n",
    "AWS_pickled_model_location = '/home/ec2-user/cs230/crime_prediction/Sean/Trials/Pickled Models/CNN Trial %d.ckpt'\n",
    "if USING_EC2:\n",
    "    pickled_model_location = AWS_pickled_model_location\n",
    "trial_file_format = 'CNN Trial %d.xlsx'\n",
    "epochs_between_prints = 1\n",
    "hyperparameter_file_columns = [\n",
    "    'Training Cost',\n",
    "    'Dev Cost',\n",
    "    'Accuracy Evaluation Batch Size',\n",
    "    'Maximum Crimes per Window',\n",
    "    'Number of Anchor Boxes per Window',\n",
    "    'Input Image Width',\n",
    "    'Input Image Height',\n",
    "    'Output Grid Width',\n",
    "    'Output Grid Height',\n",
    "    'Train Accuracy',\n",
    "    'Dev Accuracy',\n",
    "    'Duration',\n",
    "    'Learning Rate',\n",
    "    'Goal Total Epochs',\n",
    "    'Minibatch Size',\n",
    "    'Number of Minibatches',\n",
    "    'Optimizer Name',\n",
    "    'L2 Regularization Lambda',\n",
    "    'Test Set Size',\n",
    "    'Dev Set Size',\n",
    "    'Network Description',\n",
    "    'num_samples_for_normalization']\n",
    "FIRST_DATE = datetime(2001, 1, 1)\n",
    "LAST_DATE = datetime(2018, 1, 1)\n",
    "NUM_DAYS = (LAST_DATE-FIRST_DATE).days\n",
    "# 25 channels + date channels (17+12+31+6) = 91\n",
    "X_MAX_PIXELS = 2048\n",
    "Y_MAX_PIXELS = X_MAX_PIXELS\n",
    "X_WINDOW_MAX_PIXELS = input_image_width\n",
    "Y_WINDOW_MAX_PIXELS = input_image_height\n",
    "X_HALF_WINDOW_PIXELS = int(X_WINDOW_MAX_PIXELS/2)\n",
    "Y_HALF_WINDOW_PIXELS = X_HALF_WINDOW_PIXELS\n",
    "NUM_STATIC_CHANNELS = 28\n",
    "STREET_CHANNEL, WATERWAY_CHANNEL, PARK_CHANNEL, FOREST_CHANNEL, SCHOOL_CHANNEL, LIBRARY_CHANNEL, BUILDING_CHANNELS,_,_,_,_,_,_,_,_,_, BUSINESS_CHANNELS,_,_,_,_, SOCIO_CHANNELS,_,_,_,_,_,_ = range(NUM_STATIC_CHANNELS)\n",
    "NUM_DYNAMIC_CHANNELS = 9#12\n",
    "LIFE_EXPECTANCY_CHANNEL, L_CHANNELS,_,_,_,_,_,_,_ = range(NUM_STATIC_CHANNELS,NUM_STATIC_CHANNELS+NUM_DYNAMIC_CHANNELS)\n",
    "NUM_TIME_SLOTS = 12\n",
    "NUM_INPUT_CHANNELS = NUM_STATIC_CHANNELS+NUM_DYNAMIC_CHANNELS\n",
    "L_LINES = ['Green','Red','Brown','Purple','Yellow','Blue','Pink','Orange']\n",
    "CRIME_CATEGORIES = ['BATTERY', 'OTHER OFFENSE', 'ROBBERY', 'NARCOTICS', 'CRIMINAL DAMAGE',\n",
    "                    'WEAPONS VIOLATION', 'THEFT', 'BURGLARY', 'MOTOR VEHICLE THEFT',\n",
    "                    'PUBLIC PEACE VIOLATION', 'ASSAULT', 'CRIMINAL TRESPASS',\n",
    "                    'CRIM SEXUAL ASSAULT', 'INTERFERENCE WITH PUBLIC OFFICER', 'ARSON',\n",
    "                    'DECEPTIVE PRACTICE', 'LIQUOR LAW VIOLATION', 'KIDNAPPING',\n",
    "                    'SEX OFFENSE', 'OFFENSE INVOLVING CHILDREN', 'PROSTITUTION', 'HOMICIDE',\n",
    "                    'GAMBLING', 'INTIMIDATION', 'STALKING', 'OBSCENITY', 'PUBLIC INDECENCY',\n",
    "                    'HUMAN TRAFFICKING', 'CONCEALED CARRY LICENSE VIOLATION',\n",
    "                    'OTHER NARCOTIC VIOLATION', 'NON - CRIMINAL', 'NON-CRIMINAL',\n",
    "                    'NON-CRIMINAL (SUBJECT SPECIFIED)', 'RITUALISM', 'DOMESTIC VIOLENCE']\n",
    "\n",
    "#####################################\n",
    "# DATA PROCESSING UTILITY FUNCTIONS #\n",
    "#####################################\n",
    "\n",
    "def extract_data_for_date(record, fast_lookup, column):\n",
    "    record_date = datetime(record.Date.year, record.Date.month, record.Date.day)\n",
    "    index = (record_date - FIRST_DATE).days\n",
    "    if index < NUM_DAYS and index >= 0:\n",
    "        fast_lookup[index] = record[column]\n",
    "\n",
    "def nan_helper(y):\n",
    "    \"\"\"Helper to handle indices and logical indices of NaNs.\n",
    "    Reference: https://stackoverflow.com/questions/6518811/interpolate-nan-values-in-a-numpy-array\n",
    "\n",
    "    Input:\n",
    "        - y, 1d numpy array with possible NaNs\n",
    "    Output:\n",
    "        - nans, logical indices of NaNs\n",
    "        - index, a function, with signature indices= index(logical_indices),\n",
    "          to convert logical indices of NaNs to 'equivalent' indices\n",
    "    Example:\n",
    "        >>> # linear interpolation of NaNs\n",
    "        >>> nans, x= nan_helper(y)\n",
    "        >>> y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    \"\"\"\n",
    "\n",
    "    return np.isnan(y), lambda z: z.nonzero()[0]\n",
    "\n",
    "######################\n",
    "# DATAFRAME CREATION #\n",
    "######################\n",
    "\n",
    "# Building CNN Data\n",
    "# Data that varies with Time and Location:\n",
    "# - Crime (OUTPUT - YOLO with crime and location)\n",
    "# - L entries (8 layers - one per line)\n",
    "# - Life Expectancy (1 layer)\n",
    "#\n",
    "# Data that varies with Time Only:\n",
    "# - Weather (3 layers - MIN TEMP, MAX TEMP, and PRECIPITATION)\n",
    "# - Date\n",
    "# - Time\n",
    "#\n",
    "# Data that varies with Location Only:\n",
    "# - Businesses (5 layers - types of businesses)\n",
    "# - Buildings (10 layers - stories|units|sqfeet for sound|minor repair|major repair.  Also uninhabitable or not.)\n",
    "# - Waterways (1 layer)\n",
    "# - Major Streets (1 layer)\n",
    "# - Libraries (1 layer)\n",
    "# - Public Parks (1 layer)\n",
    "# - Forests (1 layer)\n",
    "# - Schools (1 layer)\n",
    "# - Socioeconomic channels (7 socioeconomic indicators)\n",
    "\n",
    "def normalize_input_data(layer_means, layer_variances):\n",
    "    global normalized_zeros, normalized_static_channels, normalized_min_temp_lookup, normalized_max_temp_lookup\n",
    "    global normalized_precipitation_lookup, normalized_life_expectancy_frame, normalized_L_entries_compressed\n",
    "    global normalized_months, normalized_days, normalized_time_slots, INPUT_DATA_NORMALIZED\n",
    "    \n",
    "    # Calculate the normalized zero with which to fill the minibatch inputs\n",
    "    zero_variance_locations = np.where(layer_variances == 0)\n",
    "    fixed_variances = np.copy(layer_variances)\n",
    "    fixed_variances[zero_variance_locations] = 1\n",
    "    normalized_zeros = layer_means / fixed_variances\n",
    "    # Normalize the static channels\n",
    "    normalized_static_channels = ((static_channels.transpose(1,2,0) - layer_means[:NUM_STATIC_CHANNELS]) / fixed_variances[:NUM_STATIC_CHANNELS]).transpose(2,0,1)\n",
    "    # Normalize the weather data\n",
    "    normalized_min_temp_lookup = (min_temp_lookup - np.mean(min_temp_lookup)) / np.var(min_temp_lookup)\n",
    "    normalized_max_temp_lookup = (max_temp_lookup - np.mean(max_temp_lookup)) / np.var(max_temp_lookup)\n",
    "    normalized_precipitation_lookup = (precipitation_lookup - np.mean(precipitation_lookup)) / np.var(precipitation_lookup)\n",
    "    # Normalize life expectancy\n",
    "    normalized_life_expectancy_frame = (life_expectancy_frame - layer_means[LIFE_EXPECTANCY_CHANNEL]) / fixed_variances[LIFE_EXPECTANCY_CHANNEL]\n",
    "    # Normalize L Entries\n",
    "    normalized_L_entries_compressed = pd.DataFrame(columns = L_LINES)\n",
    "    def normalize_L_entries(location_entries, means, variances):\n",
    "        for line_index,line in enumerate(L_LINES):\n",
    "            normalized_entries = np.copy(location_entries[line].astype(np.float64))\n",
    "            normalized_entries[2] = (normalized_entries[2] - means[line_index]) / variances[line_index]\n",
    "            normalized_L_entries_compressed.loc[location_entries.name, [line]] = [normalized_entries]\n",
    "    L_entries_compressed.apply(normalize_L_entries,\n",
    "                               args = (layer_means[L_CHANNELS:L_CHANNELS+len(L_LINES)], fixed_variances[L_CHANNELS:L_CHANNELS+len(L_LINES)]),\n",
    "                               axis=1)\n",
    "    # Calculate normalized date and time 'ones'\n",
    "    INPUT_DATA_NORMALIZED = True\n",
    "    \n",
    "def choose_random_crime():\n",
    "    # Sample randomly and uniformly from the crimes\n",
    "    this_crime_index = crime_indices[np.random.randint(len(crime_indices))]\n",
    "    random_day = this_crime_index[0]\n",
    "    random_time_slot = this_crime_index[1]\n",
    "    random_category = this_crime_index[2]\n",
    "    random_crime_index = this_crime_index[4]\n",
    "    return random_day, random_time_slot, random_category, random_crime_index\n",
    "\n",
    "def generate_random_mini_batch(batch_size, avoid_these_samples, layer_means = None, layer_variances = None, normalize = True):\n",
    "    # Generate samples\n",
    "    return generate_mini_batch(sample_index_and_location(batch_size, avoid_these_samples),\n",
    "                               layer_means = layer_means,\n",
    "                               layer_variances = layer_variances,\n",
    "                               normalize = normalize)\n",
    "\n",
    "def generate_mini_batch(samples, layer_means = None, layer_variances = None, normalize = True):\n",
    "    # Normalize the input data if necessary\n",
    "    if not INPUT_DATA_NORMALIZED:\n",
    "        if layer_means is None or layer_variances is None:\n",
    "            print('Usage Error: Please normalize the input data or pass in the layer means and variances to generate_mini_batch().')\n",
    "            print('Generating mini-batch with non-normalized input data.')\n",
    "        elif normalize:\n",
    "            normalize_input_data(layer_means, layer_variances)\n",
    "    day_index_samples = samples['day_index']\n",
    "    batch_size = len(day_index_samples)\n",
    "    years_for_samples = year_fast_lookup[day_index_samples]\n",
    "    months_for_samples = month_fast_lookup[day_index_samples]\n",
    "    days_for_samples = day_fast_lookup[day_index_samples]\n",
    "    time_slot_samples = samples['time_slot']\n",
    "    location_x_samples = samples['location_x']\n",
    "    location_y_samples = samples['location_y']\n",
    "\n",
    "    # Fill the mini-batches with normlized zeros\n",
    "    mini_batch_input = np.repeat(np.repeat(np.repeat(normalized_zeros[:,np.newaxis],\n",
    "                                                     X_WINDOW_MAX_PIXELS,\n",
    "                                                     axis=1)[:,:,np.newaxis],\n",
    "                                           Y_WINDOW_MAX_PIXELS,\n",
    "                                           axis=2)[np.newaxis,:,:,:],\n",
    "                                 batch_size,\n",
    "                                 axis=0)\n",
    "    mini_batch_output = np.zeros((batch_size, X_WINDOW_MAX_PIXELS, Y_WINDOW_MAX_PIXELS))\n",
    "\n",
    "    # Since indices are batched, cannot use the get_input, get_output locations above (they are single use only)\n",
    "    # Add static channels first (cannot vectorize here)\n",
    "    for batch_index in range(batch_size):\n",
    "        mini_batch_input[batch_index,:NUM_STATIC_CHANNELS] = normalized_static_channels[:,location_x_samples[batch_index]-X_HALF_WINDOW_PIXELS:location_x_samples[batch_index]+X_HALF_WINDOW_PIXELS, location_y_samples[batch_index]-Y_HALF_WINDOW_PIXELS:location_y_samples[batch_index]+Y_HALF_WINDOW_PIXELS]\n",
    "    # Life Expectancy Channel (cannot vectorize here)\n",
    "    for batch_index in range(batch_size):\n",
    "        mini_batch_input[batch_index,LIFE_EXPECTANCY_CHANNEL] = normalized_life_expectancy_frame[years_for_samples[batch_index]-FIRST_DATE.year, location_x_samples[batch_index]-X_HALF_WINDOW_PIXELS:location_x_samples[batch_index]+X_HALF_WINDOW_PIXELS, location_y_samples[batch_index]-Y_HALF_WINDOW_PIXELS:location_y_samples[batch_index]+Y_HALF_WINDOW_PIXELS]\n",
    "    # L Entry Channels (cannot vectorize here)\n",
    "    for batch_index in range(batch_size):\n",
    "        for line_index, line in enumerate(L_LINES):\n",
    "            station_x_samples = (normalized_L_entries_compressed[line][day_index_samples[batch_index]][0] - location_x_samples[batch_index]-X_HALF_WINDOW_PIXELS).astype(np.int64)\n",
    "            station_y_samples = (normalized_L_entries_compressed[line][day_index_samples[batch_index]][1] - location_y_samples[batch_index]-Y_HALF_WINDOW_PIXELS).astype(np.int64)\n",
    "            entries = normalized_L_entries_compressed[line][day_index_samples[batch_index]][2]\n",
    "            x_filter = (station_x_samples >= 0) & (station_x_samples < X_WINDOW_MAX_PIXELS)\n",
    "            station_x_samples = station_x_samples[x_filter]\n",
    "            station_y_samples = station_y_samples[x_filter]\n",
    "            entries = entries[x_filter]\n",
    "            y_filter = (station_y_samples >= 0) & (station_y_samples < Y_WINDOW_MAX_PIXELS)\n",
    "            station_x_samples = station_x_samples[y_filter]\n",
    "            station_y_samples = station_y_samples[y_filter]\n",
    "            entries = entries[y_filter]\n",
    "            mini_batch_input[batch_index, L_CHANNELS+line_index, station_x_samples, station_y_samples] = entries\n",
    "    # Generate the corresponding output (Combine all categories for now)\n",
    "    # Place the crimes on the map\n",
    "    # - axis 0: day index\n",
    "    # - axis 1: time slot\n",
    "    # - axis 2: crime category\n",
    "    # - axis 3: 0 is x locations, 1 is y locations, 2 is categories\n",
    "    # - axis 4: crime locations\n",
    "    # (cannot vectorize here)\n",
    "    for batch_index in range(batch_size):\n",
    "        for category_index in range(len(CRIME_CATEGORIES)):\n",
    "            last_crime = np.argwhere(crime_frames[day_index_samples[batch_index]][time_slot_samples[batch_index]][category_index][0] == -1)[0][0]\n",
    "            crimes_x = crime_frames[day_index_samples[batch_index]][time_slot_samples[batch_index]][category_index][0][:last_crime]\n",
    "            crimes_y = crime_frames[day_index_samples[batch_index]][time_slot_samples[batch_index]][category_index][1][:last_crime]\n",
    "            crimes_x = crimes_x - (location_x_samples[batch_index] - X_HALF_WINDOW_PIXELS)\n",
    "            crimes_y = crimes_y - (location_y_samples[batch_index] - Y_HALF_WINDOW_PIXELS)\n",
    "            x_filter = (crimes_x >= 0) & (crimes_x < X_WINDOW_MAX_PIXELS)\n",
    "            crimes_x = crimes_x[x_filter]\n",
    "            crimes_y = crimes_y[x_filter]\n",
    "            y_filter = (crimes_y >= 0) & (crimes_y < Y_WINDOW_MAX_PIXELS)\n",
    "            crimes_x = crimes_x[y_filter]\n",
    "            crimes_y = crimes_y[y_filter]\n",
    "            mini_batch_output[batch_index,crimes_x,crimes_y] = 1\n",
    "    \n",
    "    months_for_samples = month_fast_lookup[day_index_samples]\n",
    "    one_hot_months = np.zeros((batch_size, 12))\n",
    "    one_hot_months[np.arange(batch_size),months_for_samples] = 1\n",
    "    \n",
    "    days_for_samples = day_fast_lookup[day_index_samples]\n",
    "    one_hot_days = np.zeros((batch_size, 31))\n",
    "    one_hot_days[np.arange(batch_size),days_for_samples] = 1\n",
    "    \n",
    "    one_hot_time_slots = np.zeros((batch_size, NUM_TIME_SLOTS))\n",
    "    one_hot_time_slots[np.arange(batch_size),time_slot_samples] = 1\n",
    "    \n",
    "    mini_batch_FC_inputs = np.concatenate([one_hot_months, one_hot_days, one_hot_time_slots, normalized_min_temp_lookup[day_index_samples,np.newaxis], normalized_max_temp_lookup[day_index_samples,np.newaxis], normalized_precipitation_lookup[day_index_samples,np.newaxis]], axis=1)\n",
    "    \n",
    "    return mini_batch_input, mini_batch_output, mini_batch_FC_inputs\n",
    "\n",
    "EMPTY_SAMPLE = {'day_index' :np.array([]),\n",
    "                'time_slot' :np.array([]),\n",
    "                'location_x':np.array([]),\n",
    "                'location_y':np.array([])}\n",
    "\n",
    "def sample_index_and_location(num_samples, avoid_these_samples={'Test':EMPTY_SAMPLE,'Dev':EMPTY_SAMPLE}):\n",
    "    # Randomly sample the time and location in our range.\n",
    "    # Use this function primarily to avoid the test and dev sets.\n",
    "    # avoid_these_samples is a dictionary with 'Test' and 'Dev' set indices as numpy arrays:\n",
    "    # - day_index\n",
    "    # - time_slot\n",
    "    # - location_x\n",
    "    # - location_y\n",
    "    avoid_day_index = np.concatenate((avoid_these_samples['Test']['day_index'], avoid_these_samples['Dev']['day_index']))\n",
    "    avoid_time_slot = np.concatenate((avoid_these_samples['Test']['time_slot'], avoid_these_samples['Dev']['time_slot']))\n",
    "    avoid_location_x = np.concatenate((avoid_these_samples['Test']['location_x'], avoid_these_samples['Dev']['location_x']))\n",
    "    avoid_location_y = np.concatenate((avoid_these_samples['Test']['location_y'], avoid_these_samples['Dev']['location_y']))\n",
    "    # Create the empty sample arrays\n",
    "    num_samples_taken = 0\n",
    "    day_index_samples = np.zeros(num_samples, dtype=np.int64)\n",
    "    time_slot_samples = np.zeros(num_samples, dtype=np.int64)\n",
    "    x_location_samples = np.zeros(num_samples, dtype=np.int64)\n",
    "    y_location_samples = np.zeros(num_samples, dtype=np.int64)\n",
    "    # Generate the samples, avoiding the specified ones as necessary\n",
    "    while (num_samples_taken < num_samples):\n",
    "        day_index, time_slot, category, crime_index = choose_random_crime()\n",
    "        location_x = crime_frames[day_index][time_slot][category][0][crime_index]\n",
    "        location_y = crime_frames[day_index][time_slot][category][1][crime_index]\n",
    "        # Need to randomize the location so that the crime is not in the center (watch out for the image boundaries)\n",
    "        window_min_x = max(X_HALF_WINDOW_PIXELS, location_x - X_HALF_WINDOW_PIXELS+1)\n",
    "        window_max_x = min(X_MAX_PIXELS-X_HALF_WINDOW_PIXELS, location_x + X_HALF_WINDOW_PIXELS-1)\n",
    "        window_min_y = max(Y_HALF_WINDOW_PIXELS, location_y - Y_HALF_WINDOW_PIXELS+1)\n",
    "        window_max_y = min(Y_MAX_PIXELS-Y_HALF_WINDOW_PIXELS, location_y + Y_HALF_WINDOW_PIXELS-1)\n",
    "        location_x = (window_min_x + np.random.randint(window_max_x - window_min_x)) if window_max_x > window_min_x else window_min_x\n",
    "        location_y = (window_min_y + np.random.randint(window_max_y - window_min_y)) if window_max_y > window_min_y else window_min_y\n",
    "        # Only accept these if they do not overlap with samples we are avoiding\n",
    "        day_should_be_avoided = np.isin(day_index_samples, day_index)\n",
    "        time_slot_should_be_avoided = np.isin(time_slot_samples, time_slot)\n",
    "        location_x_should_be_avoided = ((x_location_samples + X_HALF_WINDOW_PIXELS) > location_x) & ((x_location_samples - X_HALF_WINDOW_PIXELS) <= location_x)\n",
    "        location_y_should_be_avoided = ((y_location_samples + Y_HALF_WINDOW_PIXELS) > location_y) & ((y_location_samples - Y_HALF_WINDOW_PIXELS) <= location_y)\n",
    "        if np.any(day_should_be_avoided & time_slot_should_be_avoided & location_x_should_be_avoided & location_y_should_be_avoided):\n",
    "            continue\n",
    "        else:\n",
    "            # No need to avoid this sample.  Add it to our test set.\n",
    "            day_index_samples[num_samples_taken] = day_index\n",
    "            time_slot_samples[num_samples_taken] = time_slot\n",
    "            x_location_samples[num_samples_taken] = location_x\n",
    "            y_location_samples[num_samples_taken] = location_y\n",
    "            num_samples_taken+=1\n",
    "    return {'day_index' : day_index_samples,\n",
    "            'time_slot' :time_slot_samples,\n",
    "            'location_x':x_location_samples,\n",
    "            'location_y':y_location_samples}\n",
    "\n",
    "def get_input_full_single(day_index, day, month, year, time_slot):\n",
    "    input_data = np.zeros((NUM_INPUT_CHANNELS, X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "    # Add static channels first\n",
    "    input_data[:NUM_STATIC_CHANNELS] = static_channels\n",
    "    # Weather channels\n",
    "    input_data[MIN_TEMP_CHANNEL] = np.full((X_MAX_PIXELS, Y_MAX_PIXELS), min_temp_lookup[day_index])\n",
    "    input_data[MAX_TEMP_CHANNEL] = np.full((X_MAX_PIXELS, Y_MAX_PIXELS), max_temp_lookup[day_index])\n",
    "    input_data[PRECIPITATION_CHANNEL] = np.full((X_MAX_PIXELS, Y_MAX_PIXELS), precipitation_lookup[day_index])\n",
    "    # Life Expectancy Channel\n",
    "    input_data[LIFE_EXPECTANCY_CHANNEL] = life_expectancy_frame[year-FIRST_DATE.year]\n",
    "    # L Entry Channels\n",
    "    input_data[L_CHANNELS+0, L_entries_compressed['Green'][day_index][0], L_entries_compressed['Green'][day_index][1]] = L_entries_compressed['Green'][day_index][2]\n",
    "    input_data[L_CHANNELS+1, L_entries_compressed['Red'][day_index][0], L_entries_compressed['Red'][day_index][1]] = L_entries_compressed['Red'][day_index][2]\n",
    "    input_data[L_CHANNELS+2, L_entries_compressed['Brown'][day_index][0], L_entries_compressed['Brown'][day_index][1]] = L_entries_compressed['Brown'][day_index][2]\n",
    "    input_data[L_CHANNELS+3, L_entries_compressed['Purple'][day_index][0], L_entries_compressed['Purple'][day_index][1]] = L_entries_compressed['Purple'][day_index][2]\n",
    "    input_data[L_CHANNELS+4, L_entries_compressed['Yellow'][day_index][0], L_entries_compressed['Yellow'][day_index][1]] = L_entries_compressed['Yellow'][day_index][2]\n",
    "    input_data[L_CHANNELS+5, L_entries_compressed['Blue'][day_index][0], L_entries_compressed['Blue'][day_index][1]] = L_entries_compressed['Blue'][day_index][2]\n",
    "    input_data[L_CHANNELS+6, L_entries_compressed['Pink'][day_index][0], L_entries_compressed['Pink'][day_index][1]] = L_entries_compressed['Pink'][day_index][2]\n",
    "    input_data[L_CHANNELS+7, L_entries_compressed['Orange'][day_index][0], L_entries_compressed['Orange'][day_index][1]] = L_entries_compressed['Orange'][day_index][2]\n",
    "    # Date and Time channels\n",
    "#     input_data[YEAR_CHANNEL + year - FIRST_DATE.year] = np.ones((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "#     input_data[MONTH_CHANNEL + month] = np.ones((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "#     input_data[DAY_CHANNEL + day] = np.ones((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "#     input_data[TIME_CHANNEL + time_slot] = np.ones((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "    return input_data\n",
    "\n",
    "def get_expected_output_full_single(day_index, time_slot, category):\n",
    "    # Create the map\n",
    "    output_data = np.zeros((X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "    # Place the crimes on the map\n",
    "    # - axis 0: day index\n",
    "    # - axis 1: time slot\n",
    "    # - axis 2: crime category\n",
    "    # - axis 3: 0 is x locations, 1 is y locations, 2 is categories\n",
    "    # - axis 4: crime locations\n",
    "    last_crime = np.argwhere(crime_frames[day_index][time_slot][category][0] == -1)[0][0]\n",
    "    x_locations = crime_frames[day_index][time_slot][category][0][:last_crime]\n",
    "    y_locations = crime_frames[day_index][time_slot][category][1][:last_crime]\n",
    "    output_data[x_locations,y_locations] = 1\n",
    "    return output_data\n",
    "\n",
    "def calculate_mean_and_variance(test, dev, sample_size=1000):\n",
    "    global INPUT_DATA_NORMALIZED\n",
    "    INPUT_DATA_NORMALIZED = False\n",
    "    # Generate a large number of inputs\n",
    "    inputs, outputs, _ = generate_random_mini_batch(sample_size,\n",
    "                                                    {'Test':test, 'Dev': dev},\n",
    "                                                    layer_means=np.zeros(NUM_INPUT_CHANNELS),\n",
    "                                                    layer_variances=np.ones(NUM_INPUT_CHANNELS),\n",
    "                                                    normalize=False)\n",
    "    # Calculate the mean and variance of each channel for normalizing all future mini-batches\n",
    "    layer_means = np.zeros(NUM_INPUT_CHANNELS)\n",
    "    layer_variances = np.ones(NUM_INPUT_CHANNELS)\n",
    "    for layer in range(NUM_INPUT_CHANNELS):\n",
    "        layer_means[layer] = np.mean(inputs[:,layer,:,:])\n",
    "        layer_variances[layer] = np.var(inputs[:,layer,:,:])\n",
    "    return layer_means, layer_variances\n",
    "\n",
    "def transform_to_CNN_data(inputs, outputs, fc_inputs):\n",
    "    m = inputs.shape[0]\n",
    "    x = np.transpose(inputs, (0,2,3,1))\n",
    "    y = np.sum(np.sum(outputs, axis=1), axis=1, dtype=np.int64)[:,np.newaxis]\n",
    "    return x, y, fc_inputs\n",
    "\n",
    "def create_placeholders():\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    input_height -- scalar, height of an input image\n",
    "    input_width -- scalar, width of an input image\n",
    "    input_channels -- scalar, number of channels of the input\n",
    "    output_classes -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input\n",
    "    Y -- placeholder for the input labels\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=(None, input_image_height, input_image_width, NUM_INPUT_CHANNELS))\n",
    "    X_FC = tf.placeholder(tf.float32, shape=(None, 12 + 31 + NUM_TIME_SLOTS + 3))\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "#     Y = tf.placeholder(tf.float32, shape=(None, num_anchor_boxes * output_grid_height * output_grid_width * 3 + 1))\n",
    "    \n",
    "    return X, Y, X_FC\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build the CNN. The shapes are:\n",
    "                        W1 : [4, 4, NUM_INPUT_CHANNELS, 8]\n",
    "                        W2 : [2, 2, 8, 16]\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "            \n",
    "    W1 = tf.get_variable(\"W1\", shape=(4,4,NUM_INPUT_CHANNELS,8), initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W2 = tf.get_variable(\"W2\", shape=(2,2,8,16), initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "\n",
    "    W4 = tf.get_variable(\"W4\", shape=(100, 64+(12+31+NUM_TIME_SLOTS+3)), initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b4 = tf.get_variable(\"b4\", shape=(100, 1), initializer=tf.zeros_initializer())\n",
    "    W5 = tf.get_variable(\"W5\", shape=(100, 100), initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b5 = tf.get_variable(\"b5\", shape=(100, 1), initializer=tf.zeros_initializer())\n",
    "    W6 = tf.get_variable(\"W6\", shape=(1, 100), initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b6 = tf.get_variable(\"b6\", shape=(1, 1), initializer=tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2,\n",
    "                  \"W4\":W4,\n",
    "                  \"b4\":b4,\n",
    "                  \"W5\":W5,\n",
    "                  \"b5\":b5,\n",
    "                  \"W6\":W6,\n",
    "                  \"b6\":b6}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, X_FC, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1'] # CONV layer 1\n",
    "    W2 = parameters['W2'] # CONV layer 2\n",
    "    # layer 3 is a max-pool\n",
    "    W4 = parameters['W4'] # FC layer 1\n",
    "    b4 = parameters['b4']\n",
    "    W5 = parameters['W5'] # FC layer 2\n",
    "    b5 = parameters['b5']\n",
    "    W6 = parameters['W6'] # FC layer 3\n",
    "    b6 = parameters['b6']\n",
    "    \n",
    "    # CONV2D: stride of 1, padding 'SAME'\n",
    "    Z1 = tf.nn.conv2d(X,W1, strides = [1,1,1,1], padding = 'SAME')\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    # MAXPOOL: window 8x8, sride 8, padding 'SAME'\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,8,8,1], strides = [1,8,8,1], padding = 'SAME')\n",
    "    # CONV2D: filters W2, stride 1, padding 'SAME'\n",
    "    Z2 = tf.nn.conv2d(P1,W2, strides = [1,1,1,1], padding = 'SAME')\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    A3 = tf.nn.max_pool(A2, ksize = [1,4,4,1], strides = [1,4,4,1], padding = 'SAME')\n",
    "    # FLATTEN\n",
    "    A3 = tf.contrib.layers.flatten(A3)\n",
    "    # Now combine the date layers\n",
    "    A3 = tf.transpose(tf.concat([A3, X_FC], 1))\n",
    "    Z4 = W4@A3+b4\n",
    "    A4 = tf.nn.relu(Z4)\n",
    "    Z5 = W5@A4+b5\n",
    "    A5 = tf.nn.relu(Z5)\n",
    "    # FULLY-CONNECTED without non-linear activation function\n",
    "    Z6 = W6@Z5+b6\n",
    "#     A6 = tf.nn.relu(Z6)\n",
    "\n",
    "    return tf.transpose(Z6)\n",
    "\n",
    "def compute_cost(Y_hat, Y):\n",
    "    # Regression problem, so use mean squared error loss\n",
    "#     cost = tf.reduce_mean((Y-Y_hat)**2)\n",
    "    # Calculate the cost form the network prediction\n",
    "    cost = tf.losses.mean_squared_error(labels=Y,\n",
    "                                        predictions=Y_hat,\n",
    "                                        reduction=tf.losses.Reduction.SUM)\n",
    "\n",
    "\n",
    "    # NOTE: a relu is applied for regression.  For YOLO, we want partial relu, partial sigmoid\n",
    "    # First output value is a regression:  We will use it as the K (number of crimes)\n",
    "    # X, Y, Pc per anchor box per grid cell\n",
    "    # num_crimes = tf.math.round(tf.nn.relu(Y_hat[0]))\n",
    "    # cost_num_crimes = tf.reduce_mean((Y-num_crimes)**2)\n",
    "    # anchor_grid = tf.reshape(tf.nn.sigmoid(Y_hat[1:]), (output_grid_width, output_grid_height, num_anchor_boxes, 3))\n",
    "    # predicted_crime_locations = tf.where(anchor_grid[:,:,:,0] >= 0.5)\n",
    "    # last_crime_index = tf.where(Y[:,0]==-1)[0]\n",
    "    # crime_locations = Y[:,:last_crime_index]\n",
    "    \n",
    "    \n",
    "    return cost\n",
    "\n",
    "def compute_accuracy(Y_hat, Y):\n",
    "    # Regression problem on number of crimes.\n",
    "    # Round output to nearest, then compare.\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.math.round(Y_hat), Y), \"float\"))\n",
    "\n",
    "def restore_model(saver, session):\n",
    "    # Before epoch, check for trial # in trial files\n",
    "    if os.path.isfile(trial_file_location+trial_file_format % trial_number):\n",
    "        print('Model found.  Restoring parameters.')\n",
    "        # If trial exists:\n",
    "        # 1. roll back (cost, train & dev accuracy) to epoch with highest dev accuracy.\n",
    "        trial_hyperparameters = pd.read_excel(trial_file_location+trial_file_format % trial_number)\n",
    "        # Find highest dev accuracy\n",
    "        best_dev_index = np.argmax(trial_hyperparameters.loc[:,'Dev Accuracy'].values)\n",
    "        # Delete all rows after this epoch\n",
    "        trial_hyperparameters = trial_hyperparameters[:best_dev_index+1]\n",
    "        # 2. restore model for the best dev accuracy\n",
    "        saver.restore(session, pickled_model_location % trial_number)\n",
    "        # Save the edited/new hyperparameter trial file\n",
    "        writer = pd.ExcelWriter(trial_file_location+trial_file_format % trial_number)\n",
    "        trial_hyperparameters.to_excel(writer)\n",
    "        writer.save()\n",
    "        # Return the number of epochs already trained\n",
    "        return len(trial_hyperparameters)\n",
    "    else:\n",
    "        print('No saved model.  Using default parameter initialization.')\n",
    "        return 0\n",
    "\n",
    "def epoch_teardown(saver, session, train_cost, dev_cost, training_accuracy, dev_accuracy, duration):\n",
    "    trial_hyperparameters = pd.DataFrame(columns=hyperparameter_file_columns)\n",
    "    # After epoch, check for hyperparameter file\n",
    "    if os.path.isfile(trial_file_location+trial_file_format % trial_number):\n",
    "        trial_hyperparameters = pd.read_excel(trial_file_location+trial_file_format % trial_number)\n",
    "        # Save the model parameters\n",
    "        saver.save(session, pickled_model_location % trial_number)\n",
    "    # Save hyperparameters, epoch cost, and training & dev accuracies\n",
    "    trial_hyperparameters = trial_hyperparameters.append({\n",
    "        'Training Cost' : train_cost,\n",
    "        'Dev Cost' : dev_cost,\n",
    "        'Accuracy Evaluation Batch Size' : accuracy_eval_batch_size,\n",
    "        'Maximum Crimes per Window' :max_num_crimes,\n",
    "        'Number of Anchor Boxes per Window' : num_anchor_boxes,\n",
    "        'Input Image Width' : input_image_width,\n",
    "        'Input Image Height' : input_image_height,\n",
    "        'Output Grid Width' : output_grid_width,\n",
    "        'Output Grid Height' : output_grid_height,\n",
    "        'Train Accuracy' : training_accuracy,\n",
    "        'Dev Accuracy' : dev_accuracy,\n",
    "        'Duration' : duration,\n",
    "        'Learning Rate' : learning_rate,\n",
    "        'Goal Total Epochs' : goal_total_epochs,\n",
    "        'Minibatch Size' : minibatch_size,\n",
    "        'Number of Minibatches' : num_minibatches,\n",
    "        'Optimizer Name' : optimizer_name,\n",
    "        'L2 Regularization Lambda' : regular_lambda,\n",
    "        'Test Set Size' : test_set_size,\n",
    "        'Dev Set Size' : dev_set_size,\n",
    "        'Network Description' : network_description,\n",
    "        'Samples for Normalization' : num_samples_for_normalization\n",
    "    }, ignore_index=True)\n",
    "    # Save the edited/new hyperparameter trial file\n",
    "    writer = pd.ExcelWriter(trial_file_location+trial_file_format % trial_number)\n",
    "    trial_hyperparameters.to_excel(writer)\n",
    "    writer.save()\n",
    "\n",
    "def execute_model():\n",
    "    global optimizer_name\n",
    "    \n",
    "    # Generate the dev images for calculating dev accuracy during training\n",
    "    x_dev, y_dev, x_fc_dev = transform_to_CNN_data(*generate_mini_batch(dev))\n",
    "\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n",
    "    \n",
    "    # Create Placeholders of the correct shape\n",
    "    X, Y, X_FC = create_placeholders()\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Y_hat = forward_propagation(X, X_FC, parameters)\n",
    "    \n",
    "    # Cost and Accuracies\n",
    "    cost = compute_cost(Y_hat, Y)\n",
    "    accuracy = compute_accuracy(Y_hat, Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    else:\n",
    "        optimizer_name = 'GD'\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "        # Run the initialization\n",
    "        session.run(init)\n",
    "        # If the trial already exists, pick up where we left off\n",
    "        starting_epoch = restore_model(saver, session)\n",
    "        # Do the training loop\n",
    "        print('Beginning Training')\n",
    "        for epoch in range(goal_total_epochs):\n",
    "            start_time = time.time()\n",
    "            epoch_cost = 0.\n",
    "            for _ in range(num_minibatches):\n",
    "                x_batch_train, y_batch_train, x_fc_batch_train = transform_to_CNN_data(*generate_random_mini_batch(minibatch_size, {'Test':test, 'Dev': dev}))\n",
    "                _ , batch_cost = session.run([optimizer, cost], feed_dict={X: x_batch_train,\n",
    "                                                                           Y: y_batch_train,\n",
    "                                                                           X_FC: x_fc_batch_train})\n",
    "                epoch_cost += batch_cost / num_minibatches\n",
    "            elapsed_time = time.time() - start_time\n",
    "            x_train, y_train, x_fc_train = transform_to_CNN_data(*generate_random_mini_batch(accuracy_eval_batch_size, {'Test':test, 'Dev': dev}))\n",
    "            train_accuracy = accuracy.eval({X: x_train,\n",
    "                                            Y: y_train,\n",
    "                                            X_FC: x_fc_train})\n",
    "            dev_accuracy = accuracy.eval({X: x_dev,\n",
    "                                          Y: y_dev,\n",
    "                                          X_FC: x_fc_dev})\n",
    "            # Display epoch results every so often\n",
    "            if epoch % epochs_between_prints == 0:\n",
    "                print('%i Epochs' % epoch)\n",
    "                print('\\tCost: %f' % epoch_cost)\n",
    "                print('\\tTrain Accuracy: %f' % train_accuracy)\n",
    "                print('\\tDev Accuracy: %f' % dev_accuracy)\n",
    "            # Epoch over, tear down\n",
    "            dev_cost = cost.eval({X: x_dev, Y: y_dev, X_FC: x_fc_dev})\n",
    "            epoch_teardown(saver,\n",
    "                           session,\n",
    "                           epoch_cost,\n",
    "                           dev_cost,\n",
    "                           float(train_accuracy),\n",
    "                           float(dev_accuracy),\n",
    "                           elapsed_time)                \n",
    "\n",
    "        x_train, y_train, x_fc_train = transform_to_CNN_data(*generate_random_mini_batch(accuracy_eval_batch_size, {'Test':test, 'Dev': dev}))\n",
    "        # Calculate the accuracy on the train and dev sets\n",
    "        print('Reached Goal Number of Epochs.')\n",
    "        print('Final Train Accuracy: %f' % accuracy.eval({X: x_train, Y: y_train, X_FC: x_fc_train}))\n",
    "        print('Final Dev Accuracy: %f' % accuracy.eval({X: x_dev, Y: y_dev, X_FC: x_fc_train}))\n",
    "\n",
    "##########################\n",
    "# CREATE STATIC CHANNELS #\n",
    "##########################\n",
    "\n",
    "def create_static_channels():\n",
    "    weather = None\n",
    "    if not USING_EC2:\n",
    "        weather = pd.read_excel(processed_dataset_paths_xlsx % 'Weather')\n",
    "    else:\n",
    "        weather = pd.read_excel(s3.get_object(Bucket= bucket, Key= 'CNN Input (Sean)/Weather.xlsx') ['Body'])\n",
    "    weather['Date'] = pd.to_datetime(weather['Date'])\n",
    "    # Create fast-access weather arrays\n",
    "    min_temp_lookup = np.full((NUM_DAYS), np.nan)\n",
    "    max_temp_lookup = np.full((NUM_DAYS), np.nan)\n",
    "    precipitation_lookup = np.full((NUM_DAYS), np.nan)\n",
    "    # Insert the weather data\n",
    "    weather.apply(lambda record: extract_data_for_date(record, max_temp_lookup, 'Max Temp'), axis=1)\n",
    "    weather.apply(lambda record: extract_data_for_date(record, min_temp_lookup, 'Min Temp'), axis=1)\n",
    "    weather.apply(lambda record: extract_data_for_date(record, precipitation_lookup, 'Precipitation'), axis=1)\n",
    "    # Interpolate over any NaN values\n",
    "    nans, x= nan_helper(min_temp_lookup)\n",
    "    min_temp_lookup[nans]= np.interp(x(nans), x(~nans), min_temp_lookup[~nans])\n",
    "    nans, x= nan_helper(max_temp_lookup)\n",
    "    max_temp_lookup[nans]= np.interp(x(nans), x(~nans), max_temp_lookup[~nans])\n",
    "    nans, x= nan_helper(precipitation_lookup)\n",
    "    precipitation_lookup[nans]= np.interp(x(nans), x(~nans), precipitation_lookup[~nans])\n",
    "\n",
    "    # Load static data\n",
    "    street_frame = None\n",
    "    waterway_frame = None\n",
    "    park_frame = None\n",
    "    forest_frame = None\n",
    "    school_frame = None\n",
    "    library_frame = None\n",
    "    uninhabitable_building_frame = None\n",
    "    building_frames = {'Sound':{},\n",
    "                       'Minor Repair':{},\n",
    "                       'Major Repair':{}}\n",
    "    life_expectancy_frame = None\n",
    "    business_frames = {}\n",
    "    L_entries_compressed = None\n",
    "    socioeconomic_frames = None\n",
    "    if not USING_EC2:\n",
    "        street_frame = np.load(dataset_location + 'Streets Frame.npz')['street_frame']\n",
    "        waterway_frame = np.load(dataset_location + 'Waterway Frame.npz')['waterway_frame']\n",
    "        park_frame = np.load(dataset_location + 'Park Frame.npz')['park_frame']\n",
    "        forest_frame = np.load(dataset_location + 'Forest Frame.npz')['forest_frame']\n",
    "        school_frame = np.load(dataset_location + 'School Frame.npz')['school_frame']\n",
    "        library_frame = np.load(dataset_location + 'Library Frame.npz')['library_frame']\n",
    "        uninhabitable_building_frame = np.load(dataset_location + 'Building Frames.npz')['uninhabitable_building_frame']\n",
    "        with np.load(dataset_location + 'Building Frames.npz') as data:\n",
    "            building_frames['Sound']['Stories'] = data['stories_of_sound_buildings_frame']\n",
    "            building_frames['Sound']['Area'] = data['area_of_sound_buildings_frame']\n",
    "            building_frames['Sound']['Units'] = data['units_of_sound_buildings_frame']\n",
    "\n",
    "            building_frames['Minor Repair']['Stories'] = data['stories_of_minor_repair_buildings_frame']\n",
    "            building_frames['Minor Repair']['Area'] = data['area_of_minor_repair_buildings_frame']\n",
    "            building_frames['Minor Repair']['Units'] = data['units_of_minor_repair_buildings_frame']\n",
    "\n",
    "            building_frames['Major Repair']['Stories'] = data['stories_of_major_repair_buildings_frame']\n",
    "            building_frames['Major Repair']['Area'] = data['area_of_major_repair_buildings_frame']\n",
    "            building_frames['Major Repair']['Units'] = data['units_of_major_repair_buildings_frame']\n",
    "        life_expectancy_frame = np.load(dataset_location + 'Life Expectancy Frames.npz')['life_expectancy_frame']\n",
    "        with np.load(dataset_location + 'Business Frames.npz') as data:\n",
    "            business_frames['Food Service'] = data['Food Service']\n",
    "            business_frames['Tobacco Sale'] = data['Tobacco Sale']\n",
    "            business_frames['Alcohol Consumption'] = data['Alcohol Consumption']\n",
    "            business_frames['Package Store'] = data['Package Store']\n",
    "            business_frames['Gas Station'] = data['Gas Station']\n",
    "        L_entries_compressed = pd.read_csv(dataset_location + 'L Entries.csv')\n",
    "        L_entries_compressed_as_array = np.zeros((len(L_entries_compressed), len(L_LINES)))\n",
    "        # Unpack the json strings to numpy\n",
    "        for line in L_LINES:\n",
    "            L_entries_compressed[line] = L_entries_compressed[line].apply(lambda array_string: np.array(json.loads(array_string)))\n",
    "        # L Entries is a pandas dataframe:\n",
    "        #  column is L line\n",
    "        #  row is day number\n",
    "        #  Cell is numpy array:\n",
    "        #    row 1 is x coordinate of rail station\n",
    "        #    row 2 is y coordinate of rail station\n",
    "        #    row 3 is number of entries for rail station\n",
    "        socioeconomic_frames = np.load(dataset_location + 'Socioeconomic Frames.npz')['socioeconomic_frame']\n",
    "    else:\n",
    "        street_frame = np.load(BytesIO(s3.get_object(Bucket=bucket, Key='CNN Input (Sean)/Streets Frame.npz')['Body'].read()))['street_frame']\n",
    "        waterway_frame = np.load(BytesIO(s3.get_object(Bucket=bucket, Key='CNN Input (Sean)/Waterway Frame.npz')['Body'].read()))['waterway_frame']\n",
    "        park_frame = np.load(BytesIO(s3.get_object(Bucket=bucket, Key='CNN Input (Sean)/Park Frame.npz')['Body'].read()))['park_frame']\n",
    "        forest_frame = np.load(BytesIO(s3.get_object(Bucket=bucket, Key='CNN Input (Sean)/Forest Frame.npz')['Body'].read()))['forest_frame']\n",
    "        school_frame = np.load(BytesIO(s3.get_object(Bucket=bucket, Key='CNN Input (Sean)/School Frame.npz')['Body'].read()))['school_frame']\n",
    "        library_frame = np.load(BytesIO(s3.get_object(Bucket=bucket, Key='CNN Input (Sean)/Library Frame.npz')['Body'].read()))['library_frame']\n",
    "        uninhabitable_building_frame = np.load(BytesIO(s3.get_object(Bucket=bucket, Key='CNN Input (Sean)/Building Frames.npz')['Body'].read()))['uninhabitable_building_frame']\n",
    "        with np.load(BytesIO(s3.get_object(Bucket=bucket, Key='CNN Input (Sean)/Building Frames.npz')['Body'].read())) as data:\n",
    "            building_frames['Sound']['Stories'] = data['stories_of_sound_buildings_frame']\n",
    "            building_frames['Sound']['Area'] = data['area_of_sound_buildings_frame']\n",
    "            building_frames['Sound']['Units'] = data['units_of_sound_buildings_frame']\n",
    "\n",
    "            building_frames['Minor Repair']['Stories'] = data['stories_of_minor_repair_buildings_frame']\n",
    "            building_frames['Minor Repair']['Area'] = data['area_of_minor_repair_buildings_frame']\n",
    "            building_frames['Minor Repair']['Units'] = data['units_of_minor_repair_buildings_frame']\n",
    "\n",
    "            building_frames['Major Repair']['Stories'] = data['stories_of_major_repair_buildings_frame']\n",
    "            building_frames['Major Repair']['Area'] = data['area_of_major_repair_buildings_frame']\n",
    "            building_frames['Major Repair']['Units'] = data['units_of_major_repair_buildings_frame']\n",
    "        life_expectancy_frame = np.load(BytesIO(s3.get_object(Bucket=bucket, Key='CNN Input (Sean)/Life Expectancy Frames.npz')['Body'].read()))['life_expectancy_frame']\n",
    "        with np.load(BytesIO(s3.get_object(Bucket=bucket, Key='CNN Input (Sean)/Business Frames.npz')['Body'].read())) as data:\n",
    "            business_frames['Food Service'] = data['Food Service']\n",
    "            business_frames['Tobacco Sale'] = data['Tobacco Sale']\n",
    "            business_frames['Alcohol Consumption'] = data['Alcohol Consumption']\n",
    "            business_frames['Package Store'] = data['Package Store']\n",
    "            business_frames['Gas Station'] = data['Gas Station']\n",
    "        L_entries_compressed = pd.read_csv(s3.get_object(Bucket= bucket, Key= 'CNN Input (Sean)/L Entries.csv') ['Body'])\n",
    "        L_entries_compressed_as_array = np.zeros((len(L_entries_compressed), len(L_LINES)))\n",
    "        # Unpack the json strings to numpy\n",
    "        for line in L_LINES:\n",
    "            L_entries_compressed[line] = L_entries_compressed[line].apply(lambda array_string: np.array(json.loads(array_string)))\n",
    "        # L Entries is a pandas dataframe:\n",
    "        #  column is L line\n",
    "        #  row is day number\n",
    "        #  Cell is numpy array:\n",
    "        #    row 1 is x coordinate of rail station\n",
    "        #    row 2 is y coordinate of rail station\n",
    "        #    row 3 is number of entries for rail station\n",
    "        socioeconomic_frames = np.load(BytesIO(s3.get_object(Bucket=bucket, Key='CNN Input (Sean)/Socioeconomic Frames.npz')['Body'].read()))['socioeconomic_frame']\n",
    "    \n",
    "    static_channels = np.zeros((NUM_STATIC_CHANNELS, X_MAX_PIXELS, Y_MAX_PIXELS))\n",
    "    static_channels[STREET_CHANNEL] = street_frame\n",
    "    static_channels[WATERWAY_CHANNEL] = waterway_frame\n",
    "    static_channels[PARK_CHANNEL] = park_frame\n",
    "    static_channels[FOREST_CHANNEL] = forest_frame\n",
    "    static_channels[SCHOOL_CHANNEL] = school_frame\n",
    "    static_channels[LIBRARY_CHANNEL] = library_frame\n",
    "    static_channels[BUILDING_CHANNELS + 0] = uninhabitable_building_frame\n",
    "    static_channels[BUILDING_CHANNELS + 1] = building_frames['Sound']['Stories']\n",
    "    static_channels[BUILDING_CHANNELS + 2] = building_frames['Sound']['Area']\n",
    "    static_channels[BUILDING_CHANNELS + 3] = building_frames['Sound']['Units']\n",
    "    static_channels[BUILDING_CHANNELS + 4] = building_frames['Minor Repair']['Stories']\n",
    "    static_channels[BUILDING_CHANNELS + 5] = building_frames['Minor Repair']['Area']\n",
    "    static_channels[BUILDING_CHANNELS + 6] = building_frames['Minor Repair']['Units']\n",
    "    static_channels[BUILDING_CHANNELS + 7] = building_frames['Major Repair']['Stories']\n",
    "    static_channels[BUILDING_CHANNELS + 8] = building_frames['Major Repair']['Area']\n",
    "    static_channels[BUILDING_CHANNELS + 9] = building_frames['Major Repair']['Units']\n",
    "    static_channels[BUSINESS_CHANNELS + 0] = business_frames['Food Service']\n",
    "    static_channels[BUSINESS_CHANNELS + 1] = business_frames['Tobacco Sale']\n",
    "    static_channels[BUSINESS_CHANNELS + 2] = business_frames['Alcohol Consumption']\n",
    "    static_channels[BUSINESS_CHANNELS + 3] = business_frames['Package Store']\n",
    "    static_channels[BUSINESS_CHANNELS + 4] = business_frames['Gas Station']\n",
    "    static_channels[SOCIO_CHANNELS + 0] = socioeconomic_frames[0]\n",
    "    static_channels[SOCIO_CHANNELS + 1] = socioeconomic_frames[1]\n",
    "    static_channels[SOCIO_CHANNELS + 2] = socioeconomic_frames[2]\n",
    "    static_channels[SOCIO_CHANNELS + 3] = socioeconomic_frames[3]\n",
    "    static_channels[SOCIO_CHANNELS + 4] = socioeconomic_frames[4]\n",
    "    static_channels[SOCIO_CHANNELS + 5] = socioeconomic_frames[5]\n",
    "    static_channels[SOCIO_CHANNELS + 6] = socioeconomic_frames[6]\n",
    "\n",
    "    return static_channels, min_temp_lookup, max_temp_lookup, precipitation_lookup, life_expectancy_frame, L_entries_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data loaded.\n",
      "Output data loaded.\n",
      "Test and Dev samples generated.\n",
      "Mean and variance calculated.\n",
      "Input data normalized\n"
     ]
    }
   ],
   "source": [
    "static_channels, min_temp_lookup, max_temp_lookup, precipitation_lookup, life_expectancy_frame, L_entries_compressed = create_static_channels()\n",
    "# Generate global normalized data arrays\n",
    "normalized_zeros = np.zeros(NUM_INPUT_CHANNELS)\n",
    "# Normalize the static channels\n",
    "normalized_static_channels = static_channels\n",
    "\n",
    "# Normalize the weather data\n",
    "normalized_min_temp_lookup = min_temp_lookup\n",
    "normalized_max_temp_lookup = max_temp_lookup\n",
    "normalized_precipitation_lookup = precipitation_lookup\n",
    "# Normalize life expectancy\n",
    "normalized_life_expectancy_frame = life_expectancy_frame\n",
    "# Normalize L Entries\n",
    "normalized_L_entries_compressed = L_entries_compressed\n",
    "# Calculate normalized date and time 'ones'\n",
    "# normalized_months = np.ones(12)\n",
    "# normalized_days = np.ones(31)\n",
    "# normalized_time_slots = np.ones(NUM_TIME_SLOTS)\n",
    "INPUT_DATA_NORMALIZED = False\n",
    "print('Input data loaded.')\n",
    "\n",
    "#####################################\n",
    "# IMPORT PROCESSED (NONSTATIC) DATA #\n",
    "#####################################\n",
    "\n",
    "crime_frames = None\n",
    "if not USING_EC2:\n",
    "    crime_frames = np.load(dataset_location + 'Crimes.npz')['crime_frame']\n",
    "else:\n",
    "    crime_frames = np.load(BytesIO(s3.get_object(Bucket=bucket, Key='CNN Input (Sean)/Crimes.npz')['Body'].read()))['crime_frame']\n",
    "# Make it easy to convert from day_index to year, month, day\n",
    "year_fast_lookup = np.vectorize(lambda day_index: (FIRST_DATE+dtm.timedelta(days=int(day_index))).year)(np.arange(NUM_DAYS))\n",
    "month_fast_lookup = np.vectorize(lambda day_index: (FIRST_DATE+dtm.timedelta(days=int(day_index))).month)(np.arange(NUM_DAYS))-1\n",
    "day_fast_lookup = np.vectorize(lambda day_index: (FIRST_DATE+dtm.timedelta(days=int(day_index))).day)(np.arange(NUM_DAYS))-1\n",
    "# Make it easy to randomly choose crimes\n",
    "crime_indices = np.argwhere(crime_frames != -1)\n",
    "print('Output data loaded.')\n",
    "\n",
    "# Create the Test and Dev sets\n",
    "test = sample_index_and_location(test_set_size)\n",
    "dev = sample_index_and_location(dev_set_size, {'Test':test, 'Dev':EMPTY_SAMPLE})\n",
    "print('Test and Dev samples generated.')\n",
    "# Ensure the data is not normalized\n",
    "normalize_input_data(np.zeros(NUM_INPUT_CHANNELS), np.ones(NUM_INPUT_CHANNELS))\n",
    "layer_means, layer_variances = calculate_mean_and_variance(test, dev, sample_size=num_samples_for_normalization)\n",
    "print('Mean and variance calculated.')\n",
    "normalize_input_data(layer_means, layer_variances)\n",
    "print('Input data normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved model.  Using default parameter initialization.\n",
      "Beginning Training\n",
      "0 Epochs\n",
      "\tCost: 644687435.000000\n",
      "\tTrain Accuracy: 0.013000\n",
      "\tDev Accuracy: 0.015000\n",
      "1 Epochs\n",
      "\tCost: 466936791.200000\n",
      "\tTrain Accuracy: 0.015000\n",
      "\tDev Accuracy: 0.021000\n",
      "2 Epochs\n",
      "\tCost: 233709904.800000\n",
      "\tTrain Accuracy: 0.023000\n",
      "\tDev Accuracy: 0.021000\n",
      "3 Epochs\n",
      "\tCost: 143066731.200000\n",
      "\tTrain Accuracy: 0.024000\n",
      "\tDev Accuracy: 0.017000\n",
      "4 Epochs\n",
      "\tCost: 80832270.750000\n",
      "\tTrain Accuracy: 0.013000\n",
      "\tDev Accuracy: 0.016000\n",
      "5 Epochs\n",
      "\tCost: 90275900.600000\n",
      "\tTrain Accuracy: 0.015000\n",
      "\tDev Accuracy: 0.013000\n",
      "6 Epochs\n",
      "\tCost: 39016619.600000\n",
      "\tTrain Accuracy: 0.023000\n",
      "\tDev Accuracy: 0.013000\n",
      "7 Epochs\n",
      "\tCost: 33091026.600000\n",
      "\tTrain Accuracy: 0.026000\n",
      "\tDev Accuracy: 0.013000\n",
      "8 Epochs\n",
      "\tCost: 44206229.525000\n",
      "\tTrain Accuracy: 0.017000\n",
      "\tDev Accuracy: 0.012000\n",
      "9 Epochs\n",
      "\tCost: 35173804.512500\n",
      "\tTrain Accuracy: 0.022000\n",
      "\tDev Accuracy: 0.018000\n",
      "10 Epochs\n",
      "\tCost: 51600966.175000\n",
      "\tTrain Accuracy: 0.021000\n",
      "\tDev Accuracy: 0.023000\n"
     ]
    }
   ],
   "source": [
    "execute_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
