{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Milestone Code Legend\n",
    "This code base is just a portion of what we have written.\n",
    "\n",
    "1. It includes Data Preprocessing/Data Conditoning of the data sources. \n",
    "2. An initial SVM model\n",
    "3. An inital Keras model\n",
    "4. A Tenor flow model used to beat the baseline\n",
    "4. Code/script used to visualize the CNN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#library imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pathlib as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sys.prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#useful functions\n",
    "def printNulls(df):\n",
    "    null_columns = df.columns[df.isnull().any()]\n",
    "    return df[null_columns].isnull().sum() \n",
    "\n",
    "\n",
    "def writeDFToFile(dfs, path_): #dfs is an array of dataframes and their sheet names , path needs to have\n",
    "    time_ = str(datetime.datetime.now())\n",
    "    current_date_time = time_[0:time_.index(\".\")]\n",
    "    current_date_time = current_date_time.replace(\":\", \"-\")\n",
    "    task4_fileoutput = path_+current_date_time+\".xlsx\"\n",
    "\n",
    "    writer = pd.ExcelWriter(task4_fileoutput)\n",
    "    \n",
    "    for df_tuple in dfs:  \n",
    "        df = df_tuple[0]\n",
    "        sheetName = df_tuple[1]\n",
    "        df.to_excel(writer, sheetName)\n",
    "    print(\"file written to :       \" + task4_fileoutput)\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing/Data Conditioning \n",
    "(3 datasources so far: crime data, liquor store, socioeconomic salary data)\n",
    "Please refer to milestone report for the links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_=\"\"\"\n",
    "Start with analysis, get the crime data \n",
    "\"\"\"\n",
    "\n",
    "print(\"started\")\n",
    "#change location here \n",
    "path = r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearningData\\chicago energy usage\\Crimes_-_2001_to_present.csv'\n",
    "path1 = path.replace('\\\\', r'//')\n",
    "crime_rate = pd.read_csv(path1, sep=',', engine='python')\n",
    "print(\"done\")\n",
    "crime_rate = crime_rate.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_=\"\"\"\n",
    "After looking at the dataset, we see that there are Precincts we can join on , there are Police Districts we can join on and Community area as well \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#change the Primary Type - the thing we want to predict - to discrete category numbers \n",
    "crime_rate['categoryType'] =  pd.Categorical(crime_rate['Primary Type'])\n",
    "crime_rate['categoryCode'] = crime_rate['categoryType'].cat.codes            # df.cc.astype('category').cat.codes  https://stackoverflow.com/questions/38088652/pandas-convert-categories-to-numbers\n",
    "\n",
    "crime_rate['time_'] = pd.to_datetime(crime_rate['Date'], format =\"%m/%d/%Y %I:%M:%S %p\")\n",
    "crime_rate['hr'] = crime_rate.time_.dt.hour\n",
    "crime_rate['min'] = crime_rate.time_.dt.minute\n",
    "\n",
    "bk2= crime_rate.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_=\"\"\"\n",
    "Join in the socio economic data -- columns to join on are \"Community Area\" and \"Community Area Number\"\n",
    "\"\"\"\n",
    "\n",
    "path = r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearningData\\chicago energy usage\\crime still need to explore\\Census_Data_-_Selected_socioeconomic_indicators_in_Chicago__2008___2012.csv'\n",
    "path2 = path.replace('\\\\', r'//')\n",
    "socioeconomic_data = pd.read_csv(path2, sep=',', engine='python')\n",
    "socioeconomic_data = socioeconomic_data.dropna()  # there are currently 77 community areas: https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2 \n",
    "scbk = socioeconomic_data.copy()\n",
    "\n",
    "#assert that the community area columns have the same datatypes otherwise we get null values \n",
    "#use: socioeconomic_data.dtypes \n",
    "assert(socioeconomic_data['Community Area Number'].dtype == crime_rate['Community Area'].dtype)\n",
    "#rename one of the df's column to the other one's, see below -> so set Community Area Number to just Community Area so the columns match \n",
    "socioeconomic_data.rename(columns = {'Community Area Number':'Community Area'}, inplace = True )\n",
    "#do the join and save\n",
    "crime_socio = pd.merge(left = socioeconomic_data, right = crime_rate, on = 'Community Area', how = 'right')\n",
    "\n",
    "printNulls(crime_socio) # shows you which columns have null values only 67 values, just delete them \n",
    "crime_socio = crime_socio.dropna()\n",
    "cs_bk = crime_socio.copy() # back up the data \n",
    "\n",
    "#done with joining the socio economic data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_=\"\"\"\n",
    "Join in the Liquor Store information -- columns to join on are 'POLICE DISTRICT'   and 'District'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "path = r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearningData\\chicago energy usage\\crime still need to explore\\Business_Licenses_-_Current_Liquor_and_Public_Places_of_Amusement_Licenses.csv'\n",
    "path3 = path.replace('\\\\', r'//')\n",
    "liquor_store_data = pd.read_csv(path3, sep=',', engine='python')\n",
    "if('﻿ID' in liquor_store_data.columns.tolist()):\n",
    "    liquor_store_data.rename(columns ={'﻿ID':'ID'}, inplace= True)\n",
    "\n",
    "lsdbk = liquor_store_data.copy() \n",
    "\n",
    "# data exploration \n",
    "liquor_store_data.columns\n",
    "\n",
    "#for now just join on the Police District and Distict , count the number of liqour stores in each distrcit\n",
    "#NOTE:  that this is not the best thing to do -> ideally for each crime find the store that was closest to it -> this would be much more informational \n",
    "# problem is that there are 6644 unique stores : len(liquor_store_data['﻿ID'].unique().tolist()) and we have 6 million crimes -> that is computationally untractable. \n",
    "#might be able to find better ways of doing it though -> maybe make a dictionary for faster look ups \n",
    "\n",
    "#turns out that the lenght of liquor_store_data is the number of unique Id's <- this is great , the below works (no need to drop duplicates) \n",
    "liquor_store_data = liquor_store_data[['POLICE DISTRICT', 'ID']].copy()\n",
    "liquor_store_data = liquor_store_data.dropna()  \n",
    "len(liquor_store_data) # this is now less : 6602 rows , so had some nulls\n",
    "\n",
    "stores_per_police_district= liquor_store_data.groupby(['POLICE DISTRICT']).count().reset_index()\n",
    "#make a base measure table\n",
    "# found at : most current list is https://data.cityofchicago.org/Public-Safety/Boundaries-Police-Districts-current-/fthy-xz3r ,\n",
    "# turns out other police districts might have existed between 2001 to 2017 , therefore we can join and makea base measure table \n",
    "\n",
    "#stores per police has 22 different police districts\n",
    "# crime_socio has 24 different police districts -> joining will cause problems -> so we can just remove the na values for now KLUDGE\n",
    "stores_per_police_district.rename(columns = {'POLICE DISTRICT':'District', 'ID':'LiquorStoreCount_District'} , inplace = True )\n",
    "\n",
    "#assert same datatypes of integer/float\n",
    "assert(stores_per_police_district.District.dtype == crime_socio.District.dtype)\n",
    "\n",
    "#join in \n",
    "crime_socio_lq = pd.merge(left = crime_socio, right = stores_per_police_district, on = 'District', how ='left')\n",
    "prevLen = len(crime_socio_lq)\n",
    "crime_socio_lq = crime_socio_lq.dropna()  # KLUDGE check what the value is of dropped columns\n",
    "endLen = len(crime_socio_lq)\n",
    "cr_so_lq_bk = crime_socio_lq.copy() # backup data \n",
    "\n",
    "#done joining in on the liquor store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_=\"\"\"\n",
    "Now we just store the variables we want and write to file  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#the first row below is for the crime data\n",
    "#the second row are the socio economic rows\n",
    "#the third are the liquor store counts\n",
    "result = crime_socio_lq[['hr', 'min', 'Latitude', 'Longitude', 'categoryCode', 'Community Area', 'District',\n",
    " 'PERCENT AGED UNDER 18 OR OVER 64', 'PERCENT OF HOUSING CROWDED', 'PER CAPITA INCOME ',  'PERCENT AGED 16+ UNEMPLOYED', 'HARDSHIP INDEX', 'PERCENT HOUSEHOLDS BELOW POVERTY', 'PERCENT AGED 25+ WITHOUT HIGH SCHOOL DIPLOMA',\n",
    " 'LiquorStoreCount_District']].copy()\n",
    "\n",
    "\n",
    "#write the data frame to file \n",
    "path = r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearningData\\chicago energy usage\\crime data processed\\crime_socio_lqr_'\n",
    "pathLast = path.replace('\\\\', r'//')\n",
    "writeDFToFile(dfs = [(result, 'data')], path_ = pathLast)  # this writes a .xlsx Excel file dfs is a list of tuples. Each tuple, the first value is the df, and the second value is the Excel sheet you put it in \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "An initial SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in Excel file\n",
    "# path = r'C:/Users/User/Documents/CS230 Project/chicago crime/crime_ocurrences2018-10-24 10-41-33.xlsx'\n",
    "path = r'C:/Users/User/Documents/CS230 Project/chicago crime/crime_socio_lqr_2018-10-25 22-35-49.xlsx'\n",
    "df = pd.read_excel(open(path,'rb'), sheetname='data')\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "cols.remove('categoryCode')\n",
    "cols.remove('LiquorStoreCount_District')\n",
    "# cols.remove('PER CAPITA INCOME ')\n",
    "\n",
    "\n",
    "#normalize any variables\n",
    "# df['LiquorStoreCount_District'] = df['LiquorStoreCount_District']/df['LiquorStoreCount_District'].max()*100.0\n",
    "df['PER CAPITA INCOME '] = df['PER CAPITA INCOME ']/df['PER CAPITA INCOME '].max()*100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## splitting up the dataset for train /dev\n",
    "classes = 35   # 9 is missing so maybe 34?\n",
    "\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"done randomizing\")\n",
    "\n",
    "#just dev and train set\n",
    "test = df[:30000].copy()\n",
    "Y_test = test['categoryCode'].copy().values\n",
    "Y_test = Y_test.reshape(len(Y_test),1)\n",
    "X_test = test[cols].copy().values\n",
    "train = df[30000:].copy()\n",
    "Y_train = train['categoryCode'].copy().values\n",
    "Y_train = Y_train.reshape(len(Y_train),1)\n",
    "\n",
    "X_train = train[cols].copy().values\n",
    "print(\"done splitting the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "Y_test_ = np.array(Y_test.flatten().tolist())\n",
    "Y_train_ = np.array(Y_train.flatten().tolist())\n",
    "\n",
    "\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(X_train[:30000], Y_train_[:30000])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate accuracy SVM\n",
    "\n",
    "y_pred = clf.predict(X_test[:2000])\n",
    "print(y_pred[:10])\n",
    "Y_train_for_compare = Y_test_[:2000]\n",
    "print(Y_train_for_compare)\n",
    "\n",
    "\n",
    "# make a pandas dataframe to see the results \n",
    "truth = Y_train_for_compare.tolist()\n",
    "pred = y_pred.tolist()\n",
    "df_res = pd.DataFrame({'truth':truth, 'pred':pred})\n",
    "df_res.head()\n",
    "df_res['res']=0\n",
    "assert(df_res.pred.dtype == df_res.truth.dtype)\n",
    "df_res.loc[df_res.pred ==df_res.truth, 'res' ] = 1\n",
    "df_res.head()\n",
    "\n",
    "acc = df_res.res.sum()/(len(df_res) +.0)\n",
    "print(\"Accuracy of linear model: \", acc) # 30,000 train, 1000 test\n",
    "# print(type(Y_train_for_compare.tolist()))\n",
    "# y_pred\n",
    "\n",
    "\n",
    "df_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial Keras model (not mentioned in milestone report, but something we worked on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing the data\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes=classes)\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes=classes)\n",
    "\n",
    "x_train = X_train\n",
    "y_train = Y_train\n",
    "\n",
    "x_test = X_test\n",
    "y_test = Y_test\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "from copy import deepcopy\n",
    "a = deepcopy(Y_test)[:20]\n",
    "a\n",
    "b = keras.utils.to_categorical(a, num_classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(28, input_dim=13,  activation='relu'))\n",
    "model.add(Dense(28, activation=tf.nn.relu))\n",
    "model.add(Dense(20, activation=tf.nn.relu))\n",
    "model.add(Dense(20, activation=tf.nn.relu))\n",
    "model.add(Dense(classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, epochs=20, batch_size = 20000)\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test accuracy:', test_acc)\n",
    "print('Test Loss ', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train,\n",
    "          Y_train,\n",
    "          X_test,\n",
    "          Y_test,\n",
    "          learning_rate = 0.0001,\n",
    "          num_epochs = 1500,\n",
    "          minibatch_size = 64,\n",
    "          print_cost = True):\n",
    "    ops.reset_default_graph()\n",
    "    (n_x, m) = X_train.shape\n",
    "    n_y = Y_train.shape[0]\n",
    "    costs = []\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(n_x,None), name='X')\n",
    "    Y = tf.placeholder(tf.float32, shape=(1,None), name='X')\n",
    "    \n",
    "    W1 = tf.get_variable('W1', (10,n_x), initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b1 = tf.get_variable('b1', (10,1), initializer=tf.zeros_initializer())\n",
    "    W2 = tf.get_variable('W2', (1,10), initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b2 = tf.get_variable('b2', (1,1), initializer=tf.zeros_initializer())\n",
    "    W3 = tf.get_variable('W3', (1,10), initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b3 = tf.get_variable('b3', (1,1), initializer=tf.zeros_initializer())\n",
    "    W4 = tf.get_variable('W4', (10,10), initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b4 = tf.get_variable('b4', (10,1), initializer=tf.zeros_initializer())\n",
    "    W5 = tf.get_variable('W5', (10,10), initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b5 = tf.get_variable('b5', (10,1), initializer=tf.zeros_initializer())\n",
    "    W6 = tf.get_variable('W6', (1,10), initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b6 = tf.get_variable('b6', (1,1), initializer=tf.zeros_initializer())\n",
    "\n",
    "    parameters = {'W1':W1,\n",
    "                 'b1':b1,\n",
    "                 'W2':W2,\n",
    "                 'b2':b2,\n",
    "                 'W3':W3,\n",
    "                 'b3':b3,\n",
    "                 'W4':W4,\n",
    "                 'b4':b4,\n",
    "                 'W5':W5,\n",
    "                 'b5':b5,\n",
    "                 'W6':W6,\n",
    "                 'b6':b6}\n",
    "    \n",
    "    Z1 = W1@X+b1\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = W2@A1+b2\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "#     Z3 = W3@A2+b3\n",
    "#     A3 = tf.nn.relu(Z3)\n",
    "#     Z4 = W4@A3+b4\n",
    "#     A4 = tf.nn.relu(Z4)\n",
    "#     Z5 = W5@A4+b5\n",
    "#     A5 = tf.nn.relu(Z5)\n",
    "#     Z6 = W6@A5+b6\n",
    "#     A6 = tf.nn.relu(Z6)\n",
    "\n",
    "    cost = tf.sqrt(tf.reduce_mean((A2-Y)**2))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    seed = 3\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size)\n",
    "            if num_minibatches < 1: num_minibatches=1\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.abs(1-A2/Y) < 0.2\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        #print(sess.run(W1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model(X_train, Y_train, X_test, Y_test, learning_rate=0.08, minibatch_size=10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Script For CNN data\n",
    "The result of this script gets copied and pasted into an html file. Stored in our repository in\n",
    "Ali/cnn_maps/. The file already calls on the Map API using a key of sorts and then with the Geo JSON generated from this code below, has the visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helps to generate code by using the tmplate\n",
    "\n",
    "template1 = \"\"\"                        {\n",
    "                            \"type\": \"Feature\",\n",
    "                            \"geometry\": {\n",
    "                                \"type\": \"MultiPoint\",\n",
    "                                \"coordinates\": \n",
    "           \"\"\"\n",
    "template2 = \"\"\"                     \n",
    "                                }\n",
    "                                ,\n",
    "                            \"properties\": {\n",
    "                                \"name\": \\\"\"\"\"\n",
    "template3 = \"\"\"\"\n",
    "                            }\n",
    "                        },\n",
    "\"\"\"\n",
    "\n",
    "str(list2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#add the GEO JSON data to be copied and pasted to HTML\n",
    "\n",
    "names = []\n",
    "for index in range(0, 34):\n",
    "    df_cat6 = df.loc[df['categoryCode'] == index]\n",
    "    lat_ = df_cat6['Latitude'].tolist()\n",
    "    long_ = df_cat6['Longitude'].tolist()\n",
    "    list_indexes = np.arange(len(long_)).tolist()\n",
    "    np.random.shuffle(list_indexes)\n",
    "    if(len(list_indexes) < 300):\n",
    "        list1 = [[long_[x], lat_[x]] for x in list_indexes]\n",
    "        print(template1, str(list1), template2, 'cat'+str(index),  template3)\n",
    "    else:\n",
    "        list1 = [[long_[x], lat_[x]] for x in list_indexes if x <300]\n",
    "        print(template1, str(list1), template2, 'cat'+str(index),  template3)\n",
    "    names.append(\" \"+'cat'+str(index)+\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in names:\n",
    "    print(\"'\"+x+\"', '#',\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
