{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Statements \n",
    "####################\n",
    "import os \n",
    "import sys\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pathlib as pl\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image, ImageDraw\n",
    "import pathlib\n",
    "import datetime\n",
    "import itertools as ite \n",
    "import math\n",
    "import calendar\n",
    "\n",
    "import shapefile  #conda install -c conda-forge pyshp    # (version should be 2.0)\n",
    "from shapely.geometry import Point   #conda install -c conda-forge shapely\n",
    "from shapely.geometry import shape\n",
    "\n",
    "\n",
    "NUM_TIME_BINS_PER_DAY = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_=\"\"\"\n",
    "\n",
    "Define any useful functions \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def cleanURL(url):\n",
    "    p = pathlib.Path(url)\n",
    "    path = str(p.as_posix()) \n",
    "    return path \n",
    "\n",
    "\n",
    "def getDF(loc, sheetname):\n",
    "    dataframe = pd.read_excel(loc, sheetname)\n",
    "    #https://stackoverflow.com/questions/40950310/strip-trim-all-strings-of-a-dataframe\n",
    "    dataframe = dataframe.applymap(lambda x: x.strip() if type(x) is str else x)\n",
    "    return dataframe\n",
    "\n",
    "def printNulls(df):\n",
    "    null_columns = df.columns[df.isnull().any()]\n",
    "    return df[null_columns].isnull().sum() \n",
    "\n",
    "\n",
    "def writeDFToFile(dfs, path_): #dfs is an array of dataframes and their sheet names , path needs to have\n",
    "    time_ = str(datetime.datetime.now())\n",
    "    current_date_time = time_[0:time_.index(\".\")]\n",
    "    current_date_time = current_date_time.replace(\":\", \"-\")\n",
    "    task4_fileoutput = path_+current_date_time+\".xlsx\"\n",
    "\n",
    "    writer = pd.ExcelWriter(task4_fileoutput)\n",
    "    \n",
    "    for df_tuple in dfs:  \n",
    "        df = df_tuple[0]\n",
    "        sheetName = df_tuple[1]\n",
    "        df.to_excel(writer, sheetName)\n",
    "    print(\"file written to :       \" + task4_fileoutput)\n",
    "    writer.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sf.shapes()[0].__geo_interface__\n",
    "# feature = sf.shapeRecords()[0]\n",
    "# feature.record\n",
    "# feature.shape.shapeTypeName\n",
    "\n",
    "##check if all the shapes are valid polygons in a shapefile \n",
    "def checkIfShapeFileFilledWithPolygons(sfile):\n",
    "    for shape_ in sfile.shapes():\n",
    "        assert(shape_.shapeType == 5) # the shape.shapeTypeName is 'POLYGON'\n",
    "\n",
    "\n",
    "#https://gis.stackexchange.com/questions/250172/finding-out-if-coordinate-is-within-shapefile-shp-using-pyshp\n",
    "def return_Category_A_Point_Belongs_In(colIndex, sfile, point):  # (longitude,latitude) wise \n",
    "    features = sf.shapeRecords()\n",
    "    record_ = None\n",
    "    shape_boundary = None\n",
    "    for i, feature in enumerate(features):\n",
    "        record_ = feature.record\n",
    "        shape_boundary = feature.shape\n",
    "        if(Point(point).within(shape(shape_boundary))):\n",
    "            return record_[colIndex], shape_boundary\n",
    "    #if none found, it equals none \n",
    "    return None, None\n",
    "\n",
    "def getShapeThatCorrespondsToCategory(colIndex, category, sfile):\n",
    "    records = sfile.records()\n",
    "    for i, record in enumerate(records):\n",
    "#         print(i)\n",
    "        if(record[colIndex] == category):\n",
    "            return sfile.shapes()[i]\n",
    "        \n",
    "    #if none found, it equals none \n",
    "    return None\n",
    "\n",
    "\n",
    "#optimized for 30x speed up over other functions \n",
    "#list_of_categories must be list of ints\n",
    "#colIndex must be an integer\n",
    "def maskGISPolygonsData(colIndex, list_of_categories, iHeight, iWidth,sf ): # sf is shapes file \n",
    "    iHeight = int(iHeight)\n",
    "    iWidth = int(iWidth)\n",
    "\n",
    "    records = sf.records()\n",
    "    actualCategoriesFoundInShapeFile = []\n",
    "    for record in records:\n",
    "        category = int(record[colIndex])\n",
    "        actualCategoriesFoundInShapeFile.append(category)\n",
    "\n",
    "    shapesOfShapeFile = sf.shapes()\n",
    "    # now that we have the categories the data provides, we loop through the categories the data has\n",
    "    # if there are categories that the data has that shape file does not => leave None in that list \n",
    "    shapes= []\n",
    "    #shapes = sf.shapes()\n",
    "    list_ = list_of_categories\n",
    "    lookup_ = {}\n",
    "\n",
    "    list_ = list_of_categories\n",
    "    locationInShapeFile = None \n",
    "    #look up dictionary \n",
    "    for i, ele in enumerate(list_):\n",
    "        if(ele in actualCategoriesFoundInShapeFile):\n",
    "            locationInShapeFile = actualCategoriesFoundInShapeFile.index(ele)\n",
    "            shapes.append(shapesOfShapeFile[locationInShapeFile])\n",
    "        else: \n",
    "            shapes.append(None) \n",
    "        lookup_[i+1] = ele #we are saying that this category resides in i+1 in the Masks table  \n",
    "\n",
    "    assert(len(shapes) == len(lookup_))\n",
    "    #set the masks\n",
    "    MASKS= np.zeros((len(lookup_)+1, iHeight, iWidth)) # we add a category layer and hence we added the +1 -> this allows us to have a -1 category \n",
    "\n",
    "    ## look up dictionary for points \n",
    "    pointsLU = {} # points look up \n",
    "\n",
    "    for row in range(iHeight):\n",
    "        for col in range(iWidth):\n",
    "\n",
    "            colMid = col+.5 # make the latitude and longitude be in the middle of the pixel\n",
    "            rowMid = iHeight - row - 1 +.5 # double check for this to work \n",
    "            #rowMid = row +.5\n",
    "            \n",
    "            longVal = (colMid-0.)*(1./longMultiplier)+longMin2\n",
    "            latVal = (rowMid -0.)*(1./latMultiplier)+latMin2\n",
    "            #point_ = (longVal,latVal)\n",
    "\n",
    "            #plug in latVal and longVal\n",
    "            pointsLU[(row, col)] = Point((longVal, latVal))\n",
    "\n",
    "\n",
    "    #now for each category, apply the row col to get the mask \n",
    "\n",
    "    for ishape, shape_ in enumerate(shapes):\n",
    "        if(type(shape_) == type(None)): #we didn't find a shape for the element at this index\n",
    "            print(\"found no shape\")\n",
    "            continue # no shape -> this category cannot compete in the code \n",
    "        boundary = shape(shape_)\n",
    "        for row in range(iHeight):\n",
    "            for col in range(iWidth):\n",
    "                if(len(np.where(MASKS[:,row,col] ==1)[0]) == 1):\n",
    "                    continue\n",
    "            \n",
    "                #now check if point exists within boundary \n",
    "                if(pointsLU[(row, col)].within(boundary)):  \n",
    "                    MASKS[ishape+1, row, col] = 1.   # shift by +1 because at 0 we will have -1 layer\n",
    "    \n",
    "    for row in range(iHeight):\n",
    "        for col in range(iWidth):\n",
    "            if(len(np.where(MASKS[:,row,col] ==1)[0]) == 0): # if nothing found for this row, col we put it in the null category \n",
    "                MASKS[0, row, col] = 1.\n",
    "        \n",
    "    \n",
    "    return lookup_ , MASKS \n",
    "\n",
    "def convertImageToCategoryMask(colIndex, categoryVal, sfile ,iHeight , iWidth ,latMultiplier, longMultiplier, latMin2,longMin2 ):\n",
    "    mask = np.zeros(shape=(iHeight, iWidth))\n",
    "    shape_that_points_should_be_in = getShapeThatCorrespondsToCategory(colIndex, categoryVal, sfile)   \n",
    "    \n",
    "    #if None is returned, the shapefile didn't have that category and therefore this category has no mask, no point belongs\n",
    "    if(type(shape_that_points_should_be_in) == type(None)):\n",
    "        return mask # KLUDGE: should we have a mask of -1's since this category is not available?  or -1 when point belongs to no category\n",
    "    \n",
    "    boundary = shape(shape_that_points_should_be_in)\n",
    "    \n",
    "    for row in range(iHeight):\n",
    "        for col in range(iWidth):\n",
    "            colMid = col+.5 # make the latitude and longitude be in the middle of the pixel\n",
    "            #rowMid = iHeight - row - 1 +.5 # double check for this to work \n",
    "            rowMid = row +.5\n",
    "            \n",
    "            longVal = (colMid-0.)*(1./longMultiplier)+longMin2\n",
    "            latVal = (rowMid -0.)*(1./latMultiplier)+latMin2\n",
    "            point_ = (longVal,latVal)\n",
    "            \n",
    "            if(Point(point_).within(boundary)):\n",
    "                mask[row][col]= 1 \n",
    "            \n",
    "    return mask \n",
    "\n",
    "def convertImageToCategoryMask2(colIndex,  sfile ,iHeight  , iWidth ,latMultiplier, longMultiplier, latMin2,longMin2 ):\n",
    "    iHeight = int(iHeight)\n",
    "    iWidth = int(iWidth)\n",
    "    mask = np.zeros(shape=(iHeight, iWidth))\n",
    "#     shape_that_points_should_be_in = getShapeThatCorrespondsToCategory(colIndex, categoryVal, sfile)   \n",
    "    \n",
    "#     #if None is returned, the shapefile didn't have that category and therefore this category has no mask, no point belongs\n",
    "#     if(type(shape_that_points_should_be_in) == type(None)):\n",
    "#         return mask # KLUDGE: should we have a mask of -1's since this category is not available?  or -1 when point belongs to no category\n",
    "    \n",
    "#     boundary = shape(shape_that_points_should_be_in)\n",
    "    \n",
    "    for row in range(iHeight):\n",
    "        for col in range(iWidth):\n",
    "            colMid = col+.5 # make the latitude and longitude be in the middle of the pixel\n",
    "            #rowMid = iHeight - row - 1 +.5 # double check for this to work \n",
    "            rowMid = row +.5\n",
    "            \n",
    "            longVal = (colMid-0.)*(1./longMultiplier)+longMin2\n",
    "            latVal = (rowMid -0.)*(1./latMultiplier)+latMin2\n",
    "            point_ = (longVal,latVal)\n",
    "            \n",
    "            r,s = return_Category_A_Point_Belongs_In(colIndex, sfile, point_)\n",
    "            \n",
    "            if(type(r) ==type(None)):\n",
    "                r = -1\n",
    "                \n",
    "            r = int(r)\n",
    "            mask[row][col]= r                \n",
    "            \n",
    "    return mask \n",
    "\n",
    "# # Test 1\n",
    "# r,s = return_Category_A_Point_Belongs_In(2, sf, point_) # for community areas -> the 2nd column gives the comm area\n",
    "# print(r)\n",
    "# print(s)\n",
    "# ############works!!!  returns category 25 which is the right value for community area for that point!!!!! \n",
    "\n",
    "\n",
    "# # Test 2\n",
    "# sampleRecord = sf.records()[14]\n",
    "# realShape = sf.shapes()[14]\n",
    "# colWanted = sampleRecord[2] # for Community area -> look at column 2 \n",
    "# shape_ = getShapeThatCorrespondsToCategory(2, colWanted, sf)\n",
    "\n",
    "# # shape_.equals(realShape)\n",
    "# # print(realShape.__geo_interface__['coordinates'])\n",
    "# # print(shape_.__geo_interface__['coordinates'])\n",
    "# a =shape_.__geo_interface__['coordinates']\n",
    "# b =realShape.__geo_interface__['coordinates']\n",
    "# print(a ==b ) #should be true\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:  Load in the final_dataframe dataset (the crimes that have been filtered/preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3565254240000044\n",
      "0.24557258000000104\n"
     ]
    }
   ],
   "source": [
    "# final_dataframe.to_csv(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\rev2_ 11 30 2018\\data generated checkpoint/final_dataframe_after_removing_extreme_vals.csv') , sep = ',' )\n",
    "# final_dataframe = pd.read_csv(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\rev2_ 11 30 2018\\data generated checkpoint/final_dataframe_after_removing_extreme_vals.csv'))\n",
    "final_dataframe = pd.read_csv(cleanURL(r'C:\\Users\\User\\Documents\\CS230 Project\\new_github\\final_dataframe_after_removing_extreme_vals.csv'))\n",
    "final_dataframe['Date'] = pd.to_datetime(final_dataframe['Date'], format ='%Y-%m-%d %H:%M:%S')  #python changes the format when it starts up\n",
    "\n",
    "\n",
    "#calculate the latitude and longitude mins and maxes\n",
    "latMax2 = final_dataframe.Latitude.max()\n",
    "latMin2 = final_dataframe.Latitude.min()\n",
    "longMax2  = final_dataframe.Longitude.max()\n",
    "longMin2  = final_dataframe.Longitude.min()\n",
    "print(latMax2 - latMin2)\n",
    "print(longMax2 - longMin2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Convert latitude and longitude to pixel values, perform checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# Step 2. Generate Image Size and Scaling Multipliers to convert to image format. Have a 256 by 256 datasize. \n",
    "############\n",
    "\n",
    "iHeight = 256.\n",
    "iWidth = 256.\n",
    "\n",
    "# pixel/degree ratios\n",
    "latMultiplier = (iHeight- 0.)/(latMax2 - latMin2)  \n",
    "longMultiplier = (iWidth- 0.)/(longMax2 - longMin2)\n",
    "\n",
    "#convert the latitude and longitude values to the pixel values\n",
    "final_dataframe['latPixel'] = (final_dataframe.Latitude - latMin2)*latMultiplier\n",
    "final_dataframe['longPixel'] =(final_dataframe.Longitude - longMin2)*longMultiplier\n",
    "\n",
    "#pixel values are floats and not integers. type cast\n",
    "final_dataframe['latPixel'] = final_dataframe['latPixel'].astype(np.int64)\n",
    "final_dataframe['longPixel'] = final_dataframe['longPixel'].astype(np.int64)\n",
    "\n",
    "final_dataframe['latPixel'] = iHeight - final_dataframe['latPixel'] - 1  # this is important because matrix row numbers\n",
    "\n",
    "#clip any values greater the iHeight and iWeight\n",
    "final_dataframe['latPixel'] = final_dataframe['latPixel'].astype(np.int64)\n",
    "final_dataframe['longPixel'] = final_dataframe['longPixel'].astype(np.int64)\n",
    "final_dataframe.loc[(final_dataframe['latPixel'] >= int(iHeight)), 'latPixel'] = int(iHeight) - 1\n",
    "final_dataframe.loc[(final_dataframe['longPixel'] >= int(iWidth)), 'longPixel'] = int(iWidth) - 1\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Get ready for generating the different types of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155928\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# Step 3. Get ready for the Data Generation Loop \n",
    "###########\n",
    "\n",
    "\n",
    "#initialize values for the for loop \n",
    "mindate = final_dataframe.Date.min().date()\n",
    "maxdate = final_dataframe.Date.max().date()\n",
    "\n",
    "delta = maxdate - mindate\n",
    "\n",
    "#https://stackoverflow.com/questions/7274267/print-all-day-dates-between-two-dates\n",
    "datesInDays = [ mindate + datetime.timedelta(index) for index in range(delta.days+1)] \n",
    "timeOfDays = [i for i in range(NUM_TIME_BINS_PER_DAY)]\n",
    "\n",
    "\n",
    "#we want the cartesian product of days and the time of days -> this is how many images we are making\n",
    "dates_and_timeOfDays_iterator = ite.product(datesInDays, timeOfDays)\n",
    "dates_and_timeOfDays = [[z[0], z[1]] for z in dates_and_timeOfDays_iterator]  # has the day and the time of day\n",
    "\n",
    "#check we got all possible combinations \n",
    "assert(len(dates_and_timeOfDays) == NUM_TIME_BINS_PER_DAY * len(datesInDays))\n",
    "\n",
    "print(len(dates_and_timeOfDays))\n",
    "#https://stackoverflow.com/questions/13635032/what-is-the-inverse-function-of-zip-in-python\n",
    "#allDates, allTimesOfDay  = zip(*dates_and_timeOfDays) # we unzip the cartesian product so that we can now loop through everything\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Perform Additional Checks, etc. before the different data are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(final_dataframe['TIME_OF_DAY'].dtype == np.dtype('int32') or final_dataframe['TIME_OF_DAY'].dtype == np.dtype('int64'))\n",
    "assert(final_dataframe['DAY'].dtype == np.dtype('int64'))\n",
    "assert(final_dataframe['MONTH'].dtype == np.dtype('int64'))\n",
    "assert(final_dataframe['YEAR'].dtype == np.dtype('int64'))\n",
    "\n",
    "\n",
    "########################################\n",
    "#######################\n",
    "bk_day_timeOfDay = copy.deepcopy(dates_and_timeOfDays)   # [datetime.date(2001, 1, 1), 0] to [datetime.date(2018, 10, 15), 23]\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "#https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_dataframe['categoryType'] =  pd.Categorical(final_dataframe['Primary Type'])\n",
    "final_dataframe['categoryCode'] = final_dataframe['categoryType'].cat.codes            # df.cc.astype('category').cat.codes  https://stackoverflow.com/questions/38088652/pandas-convert-categories-to-numbers\n",
    "# For faster filtering, remove some columns\n",
    "all_cols = final_dataframe.columns.tolist()\n",
    "all_cols.remove('ID')\n",
    "all_cols.remove('Unnamed: 0')\n",
    "all_cols.remove('Unnamed: 0.1')\n",
    "all_cols.remove('Latitude') # have the pixelated values already\n",
    "all_cols.remove('Longitude')\n",
    "fDF = final_dataframe[all_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the masks that were generated in other python notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load all the masks \n",
    "COMM_AREA_MASK = np.load(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\data_masks\\commArea.npy'))\n",
    "COMM_AREA_MASK_LOOKUP = pd.read_csv(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\data_masks\\commAreaL.csv'))\n",
    "\n",
    "BEATS_AREA_MASK = np.load(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\data_masks\\beatArea.npy'))\n",
    "BEATS_AREA_MASK_LOOKUP = pd.read_csv(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\data_masks\\beatAreaL.csv'))\n",
    "\n",
    "DISTRICT_AREA_MASK = np.load(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\data_masks\\districtArea.npy'))\n",
    "DISTRICT_AREA_MASK_LOOKUP = pd.read_csv(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\data_masks\\districtAreaL.csv'))\n",
    "\n",
    "WARD_AREA_MASK = np.load(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\data_masks\\wardArea.npy'))\n",
    "WARD_AREA_MASK_LOOKUP = pd.read_csv(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\data_masks\\wardAreaL.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data starting with Time Related Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The time index \n",
    "#### dates [month, day, year, time of day]\n",
    "dates_data = np.zeros(shape = (len(dates_and_timeOfDays), 4))\n",
    "for x_ in range(len(dates_data)):\n",
    "    dates_data_ele = np.array([dates_and_timeOfDays[x_][0].month, dates_and_timeOfDays[x_][0].day, dates_and_timeOfDays[x_][0].year, dates_and_timeOfDays[x_][1] ])\n",
    "    #dates_data_ele = np.array([int(dates_and_timeOfDays[x_][0].month), int(dates_and_timeOfDays[x_][0].day), int(dates_and_timeOfDays[x_][0].year), int(dates_and_timeOfDays[x_][1]) ])\n",
    "    dates_data[x_] = dates_data_ele\n",
    "loc = r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\rev2_ 11 30 2018\\outputs_data 12 1 2018\\dates_data.npy'\n",
    "np.save(cleanURL(loc), dates_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Outputs (crime categories on 256 by 256 grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Outputs\n",
    "BATCHSIZE = 3000\n",
    "listOfChunks = [x_ for x_ in chunks(bk_day_timeOfDay, BATCHSIZE)]\n",
    "for _i_, chunk in enumerate(listOfChunks):\n",
    "    dates_and_timeOfDays = chunk \n",
    "    _i_ = _i_+1\n",
    "    print(\"Batch number : \", _i_, \" Index in listOfchunks: \", (_i_-1))\n",
    "    fileNameTosaveNPY = cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\rev2_ 11 30 2018\\outputs_data 12 1 2018\\y__c' + str(_i_) + '_.npy')\n",
    "\n",
    "    images_y = None  \n",
    "    images_y = np.zeros(shape = (len(dates_and_timeOfDays), int(iHeight),int(iWidth)))\n",
    "\n",
    "    dt = None\n",
    "    tofD = None \n",
    "    zero_img = np.zeros(shape = (int(iHeight),int(iWidth)))\n",
    "    row = None \n",
    "    col = None \n",
    "    \n",
    "\n",
    "    print(\"started \", _i_,  datetime.datetime.now())\n",
    "    for i, _ in enumerate(dates_and_timeOfDays):\n",
    "        if(i%1000 == 0):\n",
    "            print(i)\n",
    "            print(\"In the middle\", datetime.datetime.now())\n",
    "        dt = _[0]\n",
    "        tofD = _[1]\n",
    "        df = fDF.loc[(tofD == fDF['TIME_OF_DAY']) & (dt.day == fDF['DAY']) & (dt.month == fDF['MONTH']) & (dt.year == fDF['YEAR'])]\n",
    "\n",
    "        img = np.zeros((zero_img.shape[0], zero_img.shape[1]))\n",
    "        #the lat/long base layer\n",
    "        #img[0] = np.array(zero_img)\n",
    "        if(len(df) != 0):\n",
    "            row = df.latPixel.values\n",
    "            col = df.longPixel.values\n",
    "            img[row,col] = df.categoryCode.values #works like a charm\n",
    "            #print('herew')\n",
    "            #to check:  a = np.where(img ==1), a[0] , a[1]\n",
    "        images_y[i] = img \n",
    "\n",
    "    np.save(fileNameTosaveNPY, images_y)\n",
    "    \n",
    "    print(\"finished \", _i_,  datetime.datetime.now())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Socioeconomic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data ready, make assertions, initialize the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Socio Economic data \n",
    "socioeconomic_dataframe= pd.read_csv(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\rev2_ 11 30 2018\\data generated checkpoint/socio_economic.csv'))\n",
    "\n",
    "assert(socioeconomic_dataframe['Community Area'].dtype == np.dtype(np.int64))\n",
    "assert(COMM_AREA_MASK_LOOKUP['value'].dtype == np.dtype(np.int64))\n",
    "assert(COMM_AREA_MASK_LOOKUP['key'].dtype == np.dtype(np.int64))\n",
    "assert(len(socioeconomic_dataframe) == 77)#that is the number of categories we have \n",
    "\n",
    "\n",
    "#print(set(COMM_AREA_MASK_LOOKUP['value'].tolist())  ^ set(socioeconomic_dataframe['Community Area'].tolist()))\n",
    "#assert( set(COMM_AREA_MASK_LOOKUP['value'].tolist()) > set(socioeconomic_dataframe['Community Area'].tolist()))\n",
    "\n",
    "#7 images, each one will have len(COMM_AREA_MASK) channels/masks and each channel/mask is 256,256\n",
    "socio_economic_image = np.zeros(shape=(7, len(COMM_AREA_MASK), int(iHeight), int(iWidth))) #doing the other way, where I loop through what is in th socioeconomic_dataframe first is going to give same result since I put in 0 for a class that does not exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#layer 0 'PERCENT OF HOUSING CROWDED'\n",
    "#layer 1 'PERCENT HOUSEHOLDS BELOW POVERTY'\n",
    "#layer 2 'PERCENT AGED 16+ UNEMPLOYED'\n",
    "#layer 3 'PERCENT AGED 25+ WITHOUT HIGH SCHOOL DIPLOMA'\n",
    "#layer 4 'PERCENT AGED UNDER 18 OR OVER 64'\n",
    "#layer 5 'PER CAPITA INCOME '\n",
    "#layer 6 'HARDSHIP INDEX'           # seven layers in total   to normalize -> normalize on that layer \n",
    "\n",
    "for socioImgIndex in range(len(COMM_AREA_MASK)):\n",
    "    if(socioImgIndex == 0):\n",
    "        socio_economic_image[:, 0] = COMM_AREA_MASK[0]  # all 7 layers should have this \n",
    "        continue\n",
    "\n",
    "    #if the other one we need to do a look up \n",
    "   \n",
    "    realCommunityAreaNumber = COMM_AREA_MASK_LOOKUP.loc[(COMM_AREA_MASK_LOOKUP['key']  == socioImgIndex), 'value'].values[0]\n",
    "\n",
    "    try:\n",
    "        # do a look up in the socioeconomic_dataframe \n",
    "        socio_economic_image[0, socioImgIndex] = COMM_AREA_MASK[socioImgIndex]*(socioeconomic_dataframe.loc[(socioeconomic_dataframe['Community Area']  == realCommunityAreaNumber), 'PERCENT OF HOUSING CROWDED'].values[0])\n",
    "        socio_economic_image[1, socioImgIndex] = COMM_AREA_MASK[socioImgIndex]*(socioeconomic_dataframe.loc[(socioeconomic_dataframe['Community Area']  == realCommunityAreaNumber), 'PERCENT HOUSEHOLDS BELOW POVERTY'].values[0])\n",
    "        socio_economic_image[2, socioImgIndex] = COMM_AREA_MASK[socioImgIndex]*(socioeconomic_dataframe.loc[(socioeconomic_dataframe['Community Area']  == realCommunityAreaNumber), 'PERCENT AGED 16+ UNEMPLOYED'].values[0])\n",
    "        socio_economic_image[3, socioImgIndex] = COMM_AREA_MASK[socioImgIndex]*(socioeconomic_dataframe.loc[(socioeconomic_dataframe['Community Area']  == realCommunityAreaNumber), 'PERCENT AGED 25+ WITHOUT HIGH SCHOOL DIPLOMA'].values[0])\n",
    "        socio_economic_image[4, socioImgIndex] = COMM_AREA_MASK[socioImgIndex]*(socioeconomic_dataframe.loc[(socioeconomic_dataframe['Community Area']  == realCommunityAreaNumber), 'PERCENT AGED UNDER 18 OR OVER 64'].values[0])\n",
    "        socio_economic_image[5, socioImgIndex] = COMM_AREA_MASK[socioImgIndex]*(socioeconomic_dataframe.loc[(socioeconomic_dataframe['Community Area']  == realCommunityAreaNumber), 'PER CAPITA INCOME '].values[0])\n",
    "        socio_economic_image[6, socioImgIndex] = COMM_AREA_MASK[socioImgIndex]*(socioeconomic_dataframe.loc[(socioeconomic_dataframe['Community Area']  == realCommunityAreaNumber), 'HARDSHIP INDEX'].values[0])\n",
    "    except:\n",
    "        continue # if we don't find the community area in the dataframe, we don't have the data ,just keep zeros \n",
    "\n",
    "# done for loop, sum up all seven images\n",
    "socio_economic_image_result = np.zeros(shape=(7, int(iHeight), int(iWidth)))\n",
    "for ind in range(len(socio_economic_image_result)):\n",
    "    socio_economic_image_result[ind] = np.sum(socio_economic_image[ind], axis = 0)\n",
    "\n",
    "\n",
    "fileNameTosaveNPY_socio = cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\rev2_ 11 30 2018\\outputs_data 12 1 2018\\x__socio_.npy')\n",
    "np.save(fileNameTosaveNPY_socio, socio_economic_image_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Businesses Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data. Initialize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "businessesLoc = cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\rev2_ 11 30 2018\\Businesses.xlsx')\n",
    "businesses = getDF(loc = businessesLoc, sheetname = 'Sheet1')\n",
    "printNulls(businesses)  # make sure there are no na values \n",
    "\n",
    "businessesImg = np.zeros(shape=(5, int(iHeight), int(iWidth)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#below we are placing the columns in the correct order \n",
    "businesses = businesses[['Start Date', 'End Date', 'Location', 'Food Service', 'Tobacco Sale', 'Alcohol Consumption', 'Package Store', 'Gas Station']].copy()\n",
    "businesses['Location']=  businesses['Location'].str[1:-1]\n",
    "businesses[['lat', 'long']] = businesses['Location'].str.split(', ', expand = True)\n",
    "businesses['long'] = businesses['long'].astype(np.float64)\n",
    "businesses['lat'] = businesses['lat'].astype(np.float64)\n",
    "\n",
    "## filter out anything that is outside latmin2, latmax2, longmin2, longmax2\n",
    "businesses = businesses.loc[(businesses['lat'] < latMax2) & (businesses['lat'] > latMin2)].copy()\n",
    "businesses = businesses.loc[(businesses['long'] < longMax2) & (businesses['long'] > longMin2)].copy()\n",
    "\n",
    "\n",
    "#convert the latitude and longitude values to the pixel values\n",
    "businesses['latPixel'] = (businesses.lat - latMin2)*latMultiplier\n",
    "businesses['longPixel'] =(businesses.long - longMin2)*longMultiplier\n",
    "\n",
    "#pixel values are floats and not integers. type cast\n",
    "businesses['latPixel'] = businesses['latPixel'].astype(np.int64)\n",
    "businesses['longPixel'] = businesses['longPixel'].astype(np.int64)\n",
    "\n",
    "businesses['latPixel'] = iHeight - businesses['latPixel'] - 1  # this is important because matrix row numbers\n",
    "\n",
    "#clip any values greater the iHeight and iWeight\n",
    "businesses['latPixel'] = businesses['latPixel'].astype(np.int64)\n",
    "businesses['longPixel'] = businesses['longPixel'].astype(np.int64)\n",
    "businesses.loc[(businesses['latPixel'] >= int(iHeight)), 'latPixel'] = int(iHeight) - 1\n",
    "businesses.loc[(businesses['longPixel'] >= int(iWidth)), 'longPixel'] = int(iWidth) - 1\n",
    "businesses.loc[(businesses['latPixel'] <= 0), 'latPixel'] = 0\n",
    "businesses.loc[(businesses['longPixel'] <= 0), 'longPixel'] = 0\n",
    "\n",
    "\n",
    "#check how many were set to 255 \n",
    "len(np.where(businesses.longPixel == 255)[0])\n",
    "len(np.where(businesses.latPixel == 255)[0])\n",
    "len(np.where(businesses.longPixel == 0)[0])\n",
    "len(np.where(businesses.latPixel == 0)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#layer 0  'Food Service'\n",
    "#layer 1  'Tobacco Sale'\n",
    "#layer 2   'Alcohol Consumption'\n",
    "#layer 3   'Package Store'\n",
    "#layer 4   'Gas Station'\n",
    "businesses = businesses[['Start Date', 'End Date', 'Location', 'Food Service', 'Tobacco Sale', 'Alcohol Consumption', 'Package Store', 'Gas Station', 'lat', 'long', 'latPixel', 'longPixel']].copy()\n",
    "cols = businesses.columns.tolist()\n",
    "latPixelIndex = cols.index('latPixel')\n",
    "longPixelIndex = cols.index('longPixel')\n",
    "\n",
    "fsPixelIndex = cols.index('Food Service')\n",
    "tsPixelIndex = cols.index('Tobacco Sale')\n",
    "acPixelIndex = cols.index('Alcohol Consumption')\n",
    "psPixelIndex = cols.index('Package Store')\n",
    "gsPixelIndex = cols.index('Gas Station')\n",
    "\n",
    "#layer 0  'Food Service'\n",
    "#layer 1  'Tobacco Sale'\n",
    "#layer 2   'Alcohol Consumption'\n",
    "#layer 3   'Package Store'\n",
    "#layer 4   'Gas Station'\n",
    "\n",
    "#assert(businesses['Location'].dtype ==np.dtype('o'))\n",
    "for rowIndex in range(len(businesses)):\n",
    "    #convert the point\n",
    "    pointLat = businesses.iat[rowIndex, latPixelIndex]\n",
    "    pointLong =  businesses.iat[rowIndex, longPixelIndex]\n",
    "    \n",
    "    if( businesses.iat[rowIndex, fsPixelIndex] == True):\n",
    "        businessesImg[0, pointLat , pointLong] = 1\n",
    "        continue\n",
    "\n",
    "    if( businesses.iat[rowIndex, tsPixelIndex] == True):\n",
    "        businessesImg[1, pointLat , pointLong] = 1\n",
    "        continue\n",
    "\n",
    "    if( businesses.iat[rowIndex, acPixelIndex] == True):\n",
    "        businessesImg[2, pointLat , pointLong] = 1\n",
    "        continue\n",
    "\n",
    "    if( businesses.iat[rowIndex, psPixelIndex] == True):\n",
    "        businessesImg[3, pointLat , pointLong] = 1\n",
    "        continue\n",
    "\n",
    "    if( businesses.iat[rowIndex, gsPixelIndex] == True):\n",
    "        businessesImg[4, pointLat , pointLong] = 1\n",
    "        continue\n",
    "\n",
    "fileNameTosaveNPY_businesses = cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\rev2_ 11 30 2018\\outputs_data 12 1 2018\\x__businesses_.npy')\n",
    "np.save(fileNameTosaveNPY_businesses, businessesImg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Buildings Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up dataframe, perform checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Buildings \n",
    "buildingsLoc = cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\BuildingsParsed.csv')\n",
    "buildings= pd.read_csv(buildingsLoc)\n",
    "assert(buildings.lat.dtype == np.dtype('float64'))\n",
    "assert(buildings.long.dtype == np.dtype('float64'))\n",
    "\n",
    "#buildings = getDF(loc = businessesLoc, sheetname = 'Sheet1')\n",
    "printNulls(buildings)  # make sure there are no na values \n",
    "\n",
    "buildings = buildings.loc[(buildings['lat'] < latMax2) & (buildings['lat'] > latMin2)].copy()\n",
    "buildings = buildings.loc[(buildings['long'] < longMax2) & (buildings['long'] > longMin2)].copy()\n",
    "\n",
    "\n",
    "#convert the latitude and longitude values to the pixel values\n",
    "buildings['latPixel'] = (buildings.lat - latMin2)*latMultiplier\n",
    "buildings['longPixel'] =(buildings.long - longMin2)*longMultiplier\n",
    "\n",
    "#pixel values are floats and not integers. type cast\n",
    "buildings['latPixel'] = buildings['latPixel'].astype(np.int64)\n",
    "buildings['longPixel'] = buildings['longPixel'].astype(np.int64)\n",
    "\n",
    "buildings['latPixel'] = iHeight - buildings['latPixel'] - 1  # this is important because matrix row numbers\n",
    "\n",
    "#clip any values greater the iHeight and iWeight\n",
    "buildings['latPixel'] = buildings['latPixel'].astype(np.int64)\n",
    "buildings['longPixel'] = buildings['longPixel'].astype(np.int64)\n",
    "buildings.loc[(buildings['latPixel'] >= int(iHeight)), 'latPixel'] = int(iHeight) - 1\n",
    "buildings.loc[(buildings['longPixel'] >= int(iWidth)), 'longPixel'] = int(iWidth) - 1\n",
    "buildings.loc[(buildings['latPixel'] <= 0), 'latPixel'] = 0\n",
    "buildings.loc[(buildings['longPixel'] <= 0), 'longPixel'] = 0\n",
    "\n",
    "\n",
    "#check how many were set to 255 \n",
    "len(np.where(buildings.longPixel == 255)[0])  #9089\n",
    "len(np.where(buildings.latPixel == 255)[0])  # 4480 rows, have to make sure if we want this \n",
    "len(np.where(buildings.longPixel == 0)[0])  #9089\n",
    "len(np.where(buildings.latPixel == 0)[0])  #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform additional checks, make the \"uninhabitable\" columns the same, Initialize the data image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove extra column\n",
    "buildings.loc[(buildings['Condition'] == 'UNNHABITABLE'), 'Condition'] = 'UNINHABITABLE' \n",
    "\n",
    "assert(buildings['Stories'].dtype ==np.dtype('int64'))\n",
    "assert(buildings['Units'].dtype ==np.dtype('int64'))\n",
    "assert(buildings['Square Footage'].dtype ==np.dtype('int64'))\n",
    "\n",
    "#intialize the data \n",
    "buildingsImg = np.zeros(shape=(10, int(iHeight), int(iWidth)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#layer 0  'UNINHABITABLE' or 'UNNHABITABLE'\n",
    "#layer 1  'SOUND' , Stories\n",
    "#layer 2  'SOUND' , units\n",
    "#layer 3  'SOUND' , sq footage\n",
    "#layer 4  'NEEDS MINOR REPAIR' , Stories\n",
    "#layer 5  'NEEDS MINOR REPAIR' , units\n",
    "#layer 6  'NEEDS MINOR REPAIR' , sq footage\n",
    "#layer 7  'NEEDS MAJOR REPAIR' , Stories\n",
    "#layer 8  'NEEDS MAJOR REPAIR' , units\n",
    "#layer 9  'NEEDS MAJOR REPAIR' , sq footage\n",
    "\n",
    "\n",
    "#layer 0\n",
    "l0 = buildings.loc[(buildings['Condition'] == 'UNINHABITABLE')].copy()\n",
    "rowVals = l0.latPixel.values\n",
    "colVals = l0.longPixel.values\n",
    "buildingsImg[0][rowVals, colVals] = 1\n",
    "\n",
    "#layers 1 -3\n",
    "l1 = buildings.loc[(buildings['Condition'] == 'SOUND')].copy()\n",
    "rowVals = l1.latPixel.values\n",
    "colVals = l1.longPixel.values\n",
    "buildingsImg[1][rowVals, colVals] = l1.Stories.values\n",
    "buildingsImg[2][rowVals, colVals] = l1.Units.values\n",
    "buildingsImg[3][rowVals, colVals] = l1['Square Footage'].values\n",
    "\n",
    "#layers 4 -6\n",
    "l2 = buildings.loc[(buildings['Condition'] == 'NEEDS MINOR REPAIR')].copy()\n",
    "rowVals = l2.latPixel.values\n",
    "colVals = l2.longPixel.values\n",
    "buildingsImg[4][rowVals, colVals] = l2.Stories.values\n",
    "buildingsImg[5][rowVals, colVals] = l2.Units.values\n",
    "buildingsImg[6][rowVals, colVals] = l2['Square Footage'].values\n",
    "\n",
    "#layers 7 - 9\n",
    "l3 = buildings.loc[(buildings['Condition'] == 'NEEDS MAJOR REPAIR')].copy()\n",
    "rowVals = l3.latPixel.values\n",
    "colVals = l3.longPixel.values\n",
    "buildingsImg[7][rowVals, colVals] = l3.Stories.values\n",
    "buildingsImg[8][rowVals, colVals] = l3.Units.values\n",
    "buildingsImg[9][rowVals, colVals] = l3['Square Footage'].values\n",
    "\n",
    "\n",
    "fileNameTosaveNPY_buildings= cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\rev2_ 11 30 2018\\outputs_data 12 1 2018\\x__buildings_.npy')\n",
    "np.save(fileNameTosaveNPY_buildings, buildingsImg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate L Entries Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest data, clean and perform checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# L- entries \n",
    "LentriesLoc = cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\LEntriesParsed.csv')\n",
    "Lentries= pd.read_csv(LentriesLoc)\n",
    "assert(Lentries.lat.dtype == np.dtype('float64'))\n",
    "assert(Lentries.long.dtype == np.dtype('float64'))\n",
    "\n",
    "#the preprocessing required Kludge: probably make this a function \n",
    "\n",
    "printNulls(Lentries)  # make sure there are no na values \n",
    "\n",
    "#filter out values that don't belong to our rectangles \n",
    "Lentries = Lentries.loc[(Lentries['lat'] < latMax2) & (Lentries['lat'] > latMin2)].copy()\n",
    "Lentries = Lentries.loc[(Lentries['long'] < longMax2) & (Lentries['long'] > longMin2)].copy()\n",
    "\n",
    "\n",
    "#convert the latitude and longitude values to the pixel values\n",
    "Lentries['latPixel'] = (Lentries.lat - latMin2)*latMultiplier\n",
    "Lentries['longPixel'] =(Lentries.long - longMin2)*longMultiplier\n",
    "\n",
    "#pixel values are floats and not integers. type cast\n",
    "Lentries['latPixel'] = Lentries['latPixel'].astype(np.int64)\n",
    "Lentries['longPixel'] = Lentries['longPixel'].astype(np.int64)\n",
    "\n",
    "Lentries['latPixel'] = iHeight - Lentries['latPixel'] - 1  # this is important because matrix row numbers\n",
    "\n",
    "#clip any values greater the iHeight and iWeight\n",
    "Lentries['latPixel'] = Lentries['latPixel'].astype(np.int64)\n",
    "Lentries['longPixel'] = Lentries['longPixel'].astype(np.int64)\n",
    "Lentries.loc[(Lentries['latPixel'] >= int(iHeight)), 'latPixel'] = int(iHeight) - 1\n",
    "Lentries.loc[(Lentries['longPixel'] >= int(iWidth)), 'longPixel'] = int(iWidth) - 1\n",
    "Lentries.loc[(Lentries['latPixel'] <= 0), 'latPixel'] = 0\n",
    "Lentries.loc[(Lentries['longPixel'] <= 0), 'longPixel'] = 0\n",
    "\n",
    "\n",
    "#check how many were set to 255 \n",
    "len(np.where(Lentries.longPixel == 255)[0])  #9089\n",
    "len(np.where(Lentries.latPixel == 255)[0])  # 4480 rows, have to make sure if we want this \n",
    "len(np.where(Lentries.longPixel == 0)[0])  #9089\n",
    "len(np.where(Lentries.latPixel == 0)[0])  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Date column to right format, get the first and the last days of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#chaange the date times \n",
    "Lentries['Date'] = pd.to_datetime(Lentries['Date'], format ='%m/%d/%Y') \n",
    "\n",
    "\n",
    "#get the first and the last day from below\n",
    "firstDay = bk_day_timeOfDay[0][0]\n",
    "lastDay = bk_day_timeOfDay[-1][0]\n",
    "delta = lastDay - firstDay\n",
    "daysdelta = delta.days\n",
    "#lastDay-Lentries.Date.max().date() --> difference of 107 days => we could shave that off if we'd like, I am going to keep empty images \n",
    "\n",
    "#initialize the data image\n",
    "LEntriesImg = np.zeros(shape = (daysdelta, 8, int(iHeight), int(iWidth)) , dtype=numpy.int16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loop to get LEntries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#layer 0 - Green Line\n",
    "#layer 1 - Red Line\n",
    "#layer 2 - Brown\n",
    "#layer 3 - Purple\n",
    "#layer 4 - Yellow\n",
    "#layer 5 - Blue\n",
    "#layer 6 - Orange\n",
    "#layer 7 - pink \n",
    "intermediateDf = None \n",
    "\n",
    "\n",
    "for day in range(daysdelta):\n",
    "    if(day%1000==0):\n",
    "        print(day)\n",
    "    date_ = firstDay+datetime.timedelta(day)\n",
    "    \n",
    "    intermediateDf = Lentries.loc[(Lentries['Date'].dt.day == date_.day) & (Lentries['Date'].dt.month ==date_.month)  &(Lentries['Date'].dt.year ==date_.year) ].copy()\n",
    "    green = intermediateDf.loc[intermediateDf['Green Line']==True]\n",
    "    red = intermediateDf.loc[intermediateDf['Red Line']==True]\n",
    "    brown= intermediateDf.loc[intermediateDf['Brown Line']==True]\n",
    "    purple= intermediateDf.loc[intermediateDf['Purple Line']==True]\n",
    "    yellow= intermediateDf.loc[intermediateDf['Yellow Line']==True]\n",
    "    blue= intermediateDf.loc[intermediateDf['Blue Line']==True]\n",
    "    orange= intermediateDf.loc[intermediateDf['Orange Line']==True]\n",
    "    pink= intermediateDf.loc[intermediateDf['Pink Line']==True]\n",
    "    \n",
    "    #filter on the day and then filter on the \n",
    "    LEntriesImg[day,0, green.latPixel.values , green.longPixel.values] = green.Entries.values\n",
    "    LEntriesImg[day,1, red.latPixel.values , red.longPixel.values] = red.Entries.values  \n",
    "    LEntriesImg[day,2, brown.latPixel.values , brown.longPixel.values] = brown.Entries.values  \n",
    "    LEntriesImg[day,3, purple.latPixel.values , purple.longPixel.values] = purple.Entries.values  \n",
    "    LEntriesImg[day,4, yellow.latPixel.values , yellow.longPixel.values] = yellow.Entries.values  \n",
    "    LEntriesImg[day,5, blue.latPixel.values , blue.longPixel.values] = blue.Entries.values  \n",
    "    LEntriesImg[day,6, orange.latPixel.values , orange.longPixel.values] = orange.Entries.values  \n",
    "    LEntriesImg[day,7, pink.latPixel.values , pink.longPixel.values] = pink.Entries.values  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "fileNameTosaveNPY_L_entries= cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch\\rev2_ 11 30 2018\\outputs_data 12 1 2018\\x__Lentries_2.npy')\n",
    "np.save(fileNameTosaveNPY_L_entries, LEntriesImg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Waterways Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shapefile  #conda install -c conda-forge pyshp    # (version should be 2.0)\n",
    "from shapely.geometry import Point   #conda install -c conda-forge shapely\n",
    "from shapely.geometry import shape\n",
    "import shapely.wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'Outline']\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Outline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MULTIPOLYGON (((-87.69979372946463 41.84283213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MULTIPOLYGON (((-87.67565819832085 41.84189684...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MULTIPOLYGON (((-87.67531615296008 41.84125308...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MULTIPOLYGON (((-87.67216765378741 41.84140721...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MULTIPOLYGON (((-87.66433144398182 41.84009604...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            Outline\n",
       "0           0  MULTIPOLYGON (((-87.69979372946463 41.84283213...\n",
       "1           1  MULTIPOLYGON (((-87.67565819832085 41.84189684...\n",
       "2           2  MULTIPOLYGON (((-87.67531615296008 41.84125308...\n",
       "3           3  MULTIPOLYGON (((-87.67216765378741 41.84140721...\n",
       "4           4  MULTIPOLYGON (((-87.66433144398182 41.84009604..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waterway = pd.read_csv(cleanURL(r'C:\\Users\\User\\Documents\\CS230 Project\\new_github\\WaterwaysCSVlast.csv'))\n",
    "polygonIndex = waterway.columns.tolist().index('Outline')\n",
    "print(waterway.columns.tolist())\n",
    "print(polygonIndex)\n",
    "\n",
    "waterway.head()\n",
    "# waterway.loc[waterway.index <43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printNulls(waterway)\n",
    "# waterway.iat[47,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "point_ =  waterway.iat[0,1]\n",
    "# print(point_)\n",
    "P = shapely.wkt.loads(point_)\n",
    "# print(type(P))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more than one polygon found :  290\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "badIndices=[]\n",
    "for rIndex in range(len(waterway)):\n",
    "    stringMultipolygon = waterway.iat[rIndex,polygonIndex]\n",
    "    multiPolygon = None \n",
    "    try:\n",
    "        multiPolygon = shapely.wkt.loads(stringMultipolygon)\n",
    "    except:\n",
    "        print('polygon not parsed correctly', rIndex)\n",
    "        badIndices.append(rIndex)\n",
    "        continue\n",
    "    polygons = list(multiPolygon)\n",
    "    try:\n",
    "        assert(len(polygons)==1)\n",
    "    except:\n",
    "        badIndices.append(rIndex)\n",
    "        print('more than one polygon found : ', rIndex)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[290]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove any bad indices\n",
    "waterwayCleaned = waterway.loc[~waterway.index.isin(badIndices)].copy()\n",
    "waterwayCleaned.head()\n",
    "len(waterwayCleaned)\n",
    "\n",
    "waterPolygons = []\n",
    "for rIndex in range(len(waterwayCleaned)):\n",
    "    stringMultipolygon = waterwayCleaned.iat[rIndex,polygonIndex]\n",
    "    multiPolygon = shapely.wkt.loads(stringMultipolygon)\n",
    "    polygons = list(multiPolygon)\n",
    "    waterPolygon = polygons[0]\n",
    "    waterPolygons.append(waterPolygon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P.within(shape(waterPolygons[0]))\n",
    "iHeight = 256\n",
    "iWidth = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "waterwayMask = np.zeros(shape=(int(iHeight), int(iWidth)))\n",
    "\n",
    "\n",
    "for row in range(int(iHeight)):\n",
    "    for col in range (int(iWidth)):\n",
    "\n",
    "        colMid = col+.5 # make the latitude and longitude be in the middle of the pixel\n",
    "        rowMid = iHeight - row - 1 +.5 # double check for this to work \n",
    "        #rowMid = row +.5\n",
    "\n",
    "        longVal = (colMid-0.)*(1./longMultiplier)+longMin2\n",
    "        latVal = (rowMid -0.)*(1./latMultiplier)+latMin2\n",
    "        point_ = Point((longVal, latVal))\n",
    "\n",
    "        inWaterArea = False\n",
    "        #plug in latVal and longVal\n",
    "        for wPolygon in waterPolygons:\n",
    "            if(point_.within(shape(wPolygon))):  # Point(point).within(shape(shape_boundary))\n",
    "                inWaterArea = True\n",
    "                break\n",
    "                \n",
    "        if(inWaterArea):\n",
    "            waterwayMask[row, col] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "waterway = np.load(cleanURL(r'C:\\Users\\User\\Documents\\CS230 Project\\new_github\\waterway.npy'))\n",
    "\n",
    "\n",
    "\n",
    "waterPolygons = []\n",
    "for rIndex in range(len(waterwayCleaned)):\n",
    "    stringMultipolygon = waterwayCleaned.iat[rIndex,polygonIndex]\n",
    "    multiPolygon = shapely.wkt.loads(stringMultipolygon)\n",
    "    polygons = list(multiPolygon)\n",
    "    waterPolygon = polygons[0]\n",
    "    waterPolygons.append(waterPolygon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(cleanURL(r'C:\\Users\\User\\Documents\\CS230 Project\\new_github\\waterway_12_6_2018.npy') , waterwayMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
