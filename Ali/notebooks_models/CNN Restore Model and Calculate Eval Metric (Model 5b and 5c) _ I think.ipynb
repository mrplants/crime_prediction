{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/muhammadayub/Desktop/CS230/Notebooks/re'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleanURL(url):\n",
    "    p = pathlib.Path(url)\n",
    "    path = str(p.as_posix()) \n",
    "    return path \n",
    "\n",
    "\n",
    "def getDF(loc, sheetname):\n",
    "    dataframe = pd.read_excel(loc, sheetname)\n",
    "    #https://stackoverflow.com/questions/40950310/strip-trim-all-strings-of-a-dataframe\n",
    "    dataframe = dataframe.applymap(lambda x: x.strip() if type(x) is str else x)\n",
    "    return dataframe\n",
    "\n",
    "def printNulls(df):\n",
    "    null_columns = df.columns[df.isnull().any()]\n",
    "    return df[null_columns].isnull().sum() \n",
    "\n",
    "\n",
    "def writeDFToFile(dfs, path_): #dfs is an array of dataframes and their sheet names , path needs to have\n",
    "    time_ = str(datetime.datetime.now())\n",
    "    current_date_time = time_[0:time_.index(\".\")]\n",
    "    current_date_time = current_date_time.replace(\":\", \"-\")\n",
    "    task4_fileoutput = path_+current_date_time+\".xlsx\"\n",
    "\n",
    "    writer = pd.ExcelWriter(task4_fileoutput)\n",
    "    \n",
    "    for df_tuple in dfs:  \n",
    "        df = df_tuple[0]\n",
    "        sheetName = df_tuple[1]\n",
    "        df.to_excel(writer, sheetName)\n",
    "    print(\"file written to :       \" + task4_fileoutput)\n",
    "    writer.save()\n",
    "    \n",
    "def plotImg(img):\n",
    "    if(type(img) ==type(None)):\n",
    "        img =outputsDataReal[10000]\n",
    "    arr = []\n",
    "    for a in img:\n",
    "        arr = [a] + arr\n",
    "    plt.pcolor( arr, cmap = 'gist_ncar' )\n",
    "\n",
    "    plt.show()    \n",
    "\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the Raw Data for the Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_AMT = 34  # model5/model3_12_9__541_160.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "devMiniBatches = np.load( '/home/muhammadayub/Desktop/CS230/models_saved/model5/devMiniBatches.npy')\n",
    "testMiniBatches = np.load( '/home/muhammadayub/Desktop/CS230/models_saved/model5/testMiniBatches.npy' )\n",
    "minibatches = np.load( '/home/muhammadayub/Desktop/CS230/models_saved/model5/minibatches.npy' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([308,  47, 204, 278, 176, 129, 274, 116, 181, 255, 369, 313, 128,\n",
       "       306, 107, 122,  65,  12,   6, 285, 296,  64, 370, 343, 294, 357,\n",
       "       185, 201, 289, 125, 305, 355, 371, 344, 105,  21, 214,  82, 177,\n",
       "        36, 253,  35,  27, 164, 172, 162, 366, 269,   9, 309, 209,  71,\n",
       "        91, 123, 338, 216, 353, 188, 243, 170, 151, 100,  83, 104, 297,\n",
       "       295, 298,   3,  74, 118, 250, 148, 228, 364, 149, 259, 276, 166,\n",
       "        22, 197, 268,  13, 272, 147,  69, 121, 334,  49,  59, 333,  88,\n",
       "       349, 271, 182, 175, 282,  68, 284,  70, 341,  38, 328, 155, 270,\n",
       "       225,  55, 131, 288, 332, 198, 300, 220,  31, 140,  52, 173, 254,\n",
       "        84,  85, 290, 167, 318, 319, 208, 108,  81, 190, 335, 154, 192,\n",
       "       351, 264,  32,  50,  75, 135, 193, 206, 286,  72,  93, 350, 307,\n",
       "        45,   2, 256, 356, 202,  86, 196, 130,   7,  58, 304, 127,  78,\n",
       "       302, 146, 327, 311, 329,  39,  87, 157,  28, 262, 159,  89,  19,\n",
       "       171, 115,  37, 342,  41, 330, 138, 372,  63, 287, 273, 189, 224,\n",
       "       143, 324, 136, 281, 280, 277, 279, 235, 221,  10, 260, 226, 265,\n",
       "       180, 120, 213, 103,  24, 326,  29, 367,   5, 283, 169, 203, 215,\n",
       "        95, 158, 275, 161, 360, 322, 331, 219, 267,  44,  33,  76,  99,\n",
       "        14, 184,  25, 145, 346, 231, 139, 240, 261, 316, 238, 258, 244,\n",
       "        80, 195,  43,  34,  18, 109, 229, 266, 352,  94, 156, 110,  51,\n",
       "       314, 174,  17,  66,   1, 359,  97, 199,  96, 212,  90, 257, 232,\n",
       "       363, 291, 230,  57, 323, 236, 241, 117, 106, 317, 339, 362, 163,\n",
       "       150, 325, 310, 119, 248, 234, 218, 222,  60,  46, 245,  54, 186,\n",
       "        23, 210, 153, 141,  73, 178, 301, 144,  79,  98, 142, 247, 187,\n",
       "       252, 337, 320, 191, 348, 336, 132,  56, 373, 205,  20, 312, 183,\n",
       "       211, 112,  48, 233, 113, 223, 165, 137, 321, 114, 249,  61, 354,\n",
       "         8,  16, 160, 251, 315, 227, 358, 303, 111,  67,  11])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doubleMinibatches = np.split(minibatches, int(len(minibatches)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222.69193840026855\n"
     ]
    }
   ],
   "source": [
    "base_filepath = \"/home/muhammadayub/Desktop/CS230/training_data/minibatches_real/\"\n",
    "\n",
    "a_ = time.time()\n",
    "\n",
    "inputData = []\n",
    "outputData = []\n",
    "\n",
    "for i_ in minibatches:\n",
    "    \n",
    "    inputPath = base_filepath+'input_'+str(i_)+\"_.npy\"\n",
    "    outputPath = base_filepath+'output_'+str(i_)+\"_.npy\"\n",
    "    \n",
    "    inputMinibatch = np.load(inputPath, mmap_mode = 'r')\n",
    "    outputMinibatch = np.load(outputPath, mmap_mode = 'r')\n",
    "    \n",
    "    oneHotVectors = np.zeros(shape=( len(outputMinibatch), CATEGORY_AMT ))\n",
    "    \n",
    "    \n",
    "    for x_ , img in enumerate(outputMinibatch):\n",
    "        cCount = Counter(img.flatten())\n",
    "        mC = cCount.most_common()\n",
    "        \n",
    "        if(len(mC) == 2):\n",
    "            if(mC[0][0] == -1):\n",
    "                oneHotVectors[x_][int(mC[1][0])] = 1\n",
    "            else:\n",
    "                print(\"oh no-> assumption doesn't hold true . Recode\")\n",
    "        elif(len(mC) == 3):\n",
    "            if(mC[0][0] == -1):\n",
    "                oneHotVectors[x_][int(mC[1][0])] = 1#.5  # we are multihot encoding to make the values the same \n",
    "                oneHotVectors[x_][int(mC[2][0])] = 1#.5\n",
    "            else:\n",
    "                print(\"oh no-> assumption doesn't hold true . Recode\")\n",
    "        else:#(len(mc) >= 4):\n",
    "            if(mC[0][0] == -1):\n",
    "                oneHotVectors[x_][int(mC[1][0])] = 1#.33\n",
    "                oneHotVectors[x_][int(mC[2][0])] = 1#.34\n",
    "                oneHotVectors[x_][int(mC[3][0])] = 1#.33\n",
    "            else:\n",
    "                print(\"oh no-> assumption doesn't hold true . Recode\")\n",
    "        \n",
    "            \n",
    "            \n",
    "    inputData.append(inputMinibatch)\n",
    "    outputData.append(oneHotVectors)\n",
    "    \n",
    "    \n",
    "b_ = time.time()\n",
    "   \n",
    "print(b_-a_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n",
      "336\n"
     ]
    }
   ],
   "source": [
    "print(len(inputData))\n",
    "print(len(outputData))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 34)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputData[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the model and learn from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_H0 -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    X = tf.placeholder(shape =[None, n_H0, n_W0, n_C0], dtype = np.float32, name=\"X\")\n",
    "    Y = tf.placeholder(shape  =[None, n_y], dtype = np.float32 , name=\"Y\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():    \n",
    "\n",
    "    W1 = tf.get_variable(\"W1\", [10, 10, 28, 3], initializer =tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W2 = tf.get_variable(\"W2\", [6, 6, 3, 2], initializer =tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W3 = tf.get_variable(\"W3\", [35,200] , initializer =tf.contrib.layers.xavier_initializer(seed = 0) )\n",
    "\n",
    "    parameters = {\"W1\": W1, \"W2\": W2 , 'W3': 'W3'}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, params):\n",
    "    W1 = params['W1']\n",
    "    W2 = params['W2']    \n",
    "    W3 = params['W3']\n",
    "    \n",
    "    #convolution \n",
    "    Z1 = tf.nn.conv2d(X,W1, strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    #bias added automatically # RELU\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    \n",
    "    #average pooling -> at this point all features/weights are important to us\n",
    "    P1 = tf.nn.avg_pool(A1, ksize = [1,3,3,1], strides = [1,1,1,1], padding = 'SAME')\n",
    "\n",
    "    # convolution \n",
    "    Z2 = tf.nn.conv2d(P1,W2, strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    #RELU\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    \n",
    "    #max pooling\n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1,3,3,1], strides = [1,1,1,1], padding = 'VALID')\n",
    "    \n",
    "    #flatten\n",
    "    P2 = tf.contrib.layers.flatten(P2)\n",
    "\n",
    "    #fully connected\n",
    "    Z3 = tf.contrib.layers.fully_connected(P2, CATEGORY_AMT, activation_fn = None) #1 for yes/no\n",
    "    #going to add the softmax directly\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22:04 - 22:08 Tuesday\n",
    "# 22:10 - 22:20\n",
    "#Restoring the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running  336  minibatches to get the evaluation metrics\n",
      "INFO:tensorflow:Restoring parameters from /home/muhammadayub/Desktop/CS230/models_saved/model5/model3_12_13__82_40.ckpt\n",
      "Model restored.\n",
      "Minibatch:  0   2018-12-13 16:30:00.313944\n",
      "Minibatch:  1   2018-12-13 16:30:00.613696\n",
      "Minibatch:  2   2018-12-13 16:30:00.890854\n",
      "Minibatch:  3   2018-12-13 16:30:01.088835\n",
      "Minibatch:  4   2018-12-13 16:30:01.290208\n",
      "Minibatch:  5   2018-12-13 16:30:01.507257\n",
      "Minibatch:  6   2018-12-13 16:30:01.734800\n",
      "Minibatch:  7   2018-12-13 16:30:01.927283\n",
      "Minibatch:  8   2018-12-13 16:30:02.121432\n",
      "Minibatch:  9   2018-12-13 16:30:02.315330\n",
      "Minibatch:  10   2018-12-13 16:30:02.576740\n",
      "Minibatch:  11   2018-12-13 16:30:03.065155\n",
      "Minibatch:  12   2018-12-13 16:30:03.361481\n",
      "Minibatch:  13   2018-12-13 16:30:03.667747\n",
      "Minibatch:  14   2018-12-13 16:30:03.953744\n",
      "Minibatch:  15   2018-12-13 16:30:05.796401\n",
      "Minibatch:  16   2018-12-13 16:30:06.065446\n",
      "Minibatch:  17   2018-12-13 16:30:06.347038\n",
      "Minibatch:  18   2018-12-13 16:30:09.469256\n",
      "Minibatch:  19   2018-12-13 16:30:09.687276\n",
      "Minibatch:  20   2018-12-13 16:30:10.147119\n",
      "Minibatch:  21   2018-12-13 16:30:10.542956\n",
      "Minibatch:  22   2018-12-13 16:30:10.817442\n",
      "Minibatch:  23   2018-12-13 16:30:11.089130\n",
      "Minibatch:  24   2018-12-13 16:30:11.367910\n",
      "Minibatch:  25   2018-12-13 16:30:11.657028\n",
      "Minibatch:  26   2018-12-13 16:30:11.909851\n",
      "Minibatch:  27   2018-12-13 16:30:14.621149\n",
      "Minibatch:  28   2018-12-13 16:30:14.819275\n",
      "Minibatch:  29   2018-12-13 16:30:15.024076\n",
      "Minibatch:  30   2018-12-13 16:30:17.397228\n",
      "Minibatch:  31   2018-12-13 16:30:17.708215\n",
      "Minibatch:  32   2018-12-13 16:30:20.263999\n",
      "Minibatch:  33   2018-12-13 16:30:20.502968\n",
      "Minibatch:  34   2018-12-13 16:30:20.920947\n",
      "Minibatch:  35   2018-12-13 16:30:22.320552\n",
      "Minibatch:  36   2018-12-13 16:30:22.622593\n",
      "Minibatch:  37   2018-12-13 16:30:22.889978\n",
      "Minibatch:  38   2018-12-13 16:30:23.188144\n",
      "Minibatch:  39   2018-12-13 16:30:23.461301\n",
      "Minibatch:  40   2018-12-13 16:30:25.715246\n",
      "Minibatch:  41   2018-12-13 16:30:26.446590\n",
      "Minibatch:  42   2018-12-13 16:30:29.466762\n",
      "Minibatch:  43   2018-12-13 16:30:30.623051\n",
      "Minibatch:  44   2018-12-13 16:30:30.901785\n",
      "Minibatch:  45   2018-12-13 16:30:31.199653\n",
      "Minibatch:  46   2018-12-13 16:30:31.479953\n",
      "Minibatch:  47   2018-12-13 16:30:33.271103\n",
      "Minibatch:  48   2018-12-13 16:30:35.856638\n",
      "Minibatch:  49   2018-12-13 16:30:36.313268\n",
      "Minibatch:  50   2018-12-13 16:30:36.607343\n",
      "Minibatch:  51   2018-12-13 16:30:36.891693\n",
      "Minibatch:  52   2018-12-13 16:30:37.850987\n",
      "Minibatch:  53   2018-12-13 16:30:38.137173\n",
      "Minibatch:  54   2018-12-13 16:30:38.422574\n",
      "Minibatch:  55   2018-12-13 16:30:38.704414\n",
      "Minibatch:  56   2018-12-13 16:30:38.995696\n",
      "Minibatch:  57   2018-12-13 16:30:39.223127\n",
      "Minibatch:  58   2018-12-13 16:30:39.418845\n",
      "Minibatch:  59   2018-12-13 16:30:39.621238\n",
      "Minibatch:  60   2018-12-13 16:30:39.839636\n",
      "Minibatch:  61   2018-12-13 16:30:40.041031\n",
      "Minibatch:  62   2018-12-13 16:30:43.196631\n",
      "Minibatch:  63   2018-12-13 16:30:43.516890\n",
      "Minibatch:  64   2018-12-13 16:30:45.356216\n",
      "Minibatch:  65   2018-12-13 16:30:48.565158\n",
      "Minibatch:  66   2018-12-13 16:30:48.895091\n",
      "Minibatch:  67   2018-12-13 16:30:49.176611\n",
      "Minibatch:  68   2018-12-13 16:30:49.459470\n",
      "Minibatch:  69   2018-12-13 16:30:52.425227\n",
      "Minibatch:  70   2018-12-13 16:30:55.391056\n",
      "Minibatch:  71   2018-12-13 16:30:55.813231\n",
      "Minibatch:  72   2018-12-13 16:30:56.079192\n",
      "Minibatch:  73   2018-12-13 16:30:57.633285\n",
      "Minibatch:  74   2018-12-13 16:31:00.000191\n",
      "Minibatch:  75   2018-12-13 16:31:01.853493\n",
      "Minibatch:  76   2018-12-13 16:31:02.144532\n",
      "Minibatch:  77   2018-12-13 16:31:04.099347\n",
      "Minibatch:  78   2018-12-13 16:31:04.385748\n",
      "Minibatch:  79   2018-12-13 16:31:04.655495\n",
      "Minibatch:  80   2018-12-13 16:31:04.852151\n",
      "Minibatch:  81   2018-12-13 16:31:05.053057\n",
      "Minibatch:  82   2018-12-13 16:31:05.265734\n",
      "Minibatch:  83   2018-12-13 16:31:05.480596\n",
      "Minibatch:  84   2018-12-13 16:31:08.749993\n",
      "Minibatch:  85   2018-12-13 16:31:10.647266\n",
      "Minibatch:  86   2018-12-13 16:31:11.106635\n",
      "Minibatch:  87   2018-12-13 16:31:11.422167\n",
      "Minibatch:  88   2018-12-13 16:31:11.730640\n",
      "Minibatch:  89   2018-12-13 16:31:12.029769\n",
      "Minibatch:  90   2018-12-13 16:31:12.295099\n",
      "Minibatch:  91   2018-12-13 16:31:12.565952\n",
      "Minibatch:  92   2018-12-13 16:31:13.005713\n",
      "Minibatch:  93   2018-12-13 16:31:13.340018\n",
      "Minibatch:  94   2018-12-13 16:31:13.641987\n",
      "Minibatch:  95   2018-12-13 16:31:15.686164\n",
      "Minibatch:  96   2018-12-13 16:31:15.959933\n",
      "Minibatch:  97   2018-12-13 16:31:16.248104\n",
      "Minibatch:  98   2018-12-13 16:31:16.556279\n",
      "Minibatch:  99   2018-12-13 16:31:16.867505\n",
      "Minibatch:  100   2018-12-13 16:31:17.105607\n",
      "Minibatch:  101   2018-12-13 16:31:17.316317\n",
      "Minibatch:  102   2018-12-13 16:31:17.545117\n",
      "Minibatch:  103   2018-12-13 16:31:17.857024\n",
      "Minibatch:  104   2018-12-13 16:31:18.292652\n",
      "Minibatch:  105   2018-12-13 16:31:18.551254\n",
      "Minibatch:  106   2018-12-13 16:31:18.835726\n",
      "Minibatch:  107   2018-12-13 16:31:22.155061\n",
      "Minibatch:  108   2018-12-13 16:31:22.499498\n",
      "Minibatch:  109   2018-12-13 16:31:22.782471\n",
      "Minibatch:  110   2018-12-13 16:31:23.066824\n",
      "Minibatch:  111   2018-12-13 16:31:23.352209\n",
      "Minibatch:  112   2018-12-13 16:31:26.600386\n",
      "Minibatch:  113   2018-12-13 16:31:28.476297\n",
      "Minibatch:  114   2018-12-13 16:31:28.739934\n",
      "Minibatch:  115   2018-12-13 16:31:28.937151\n",
      "Minibatch:  116   2018-12-13 16:31:29.137797\n",
      "Minibatch:  117   2018-12-13 16:31:29.346286\n",
      "Minibatch:  118   2018-12-13 16:31:31.721986\n",
      "Minibatch:  119   2018-12-13 16:31:34.661041\n",
      "Minibatch:  120   2018-12-13 16:31:35.029905\n",
      "Minibatch:  121   2018-12-13 16:31:35.322780\n",
      "Minibatch:  122   2018-12-13 16:31:35.616718\n",
      "Minibatch:  123   2018-12-13 16:31:35.900788\n",
      "Minibatch:  124   2018-12-13 16:31:36.180240\n",
      "Minibatch:  125   2018-12-13 16:31:36.378963\n",
      "Minibatch:  126   2018-12-13 16:31:36.586101\n",
      "Minibatch:  127   2018-12-13 16:31:38.787425\n",
      "Minibatch:  128   2018-12-13 16:31:39.067487\n",
      "Minibatch:  129   2018-12-13 16:31:41.929566\n",
      "Minibatch:  130   2018-12-13 16:31:44.410927\n",
      "Minibatch:  131   2018-12-13 16:31:46.637654\n",
      "Minibatch:  132   2018-12-13 16:31:48.865511\n",
      "Minibatch:  133   2018-12-13 16:31:49.068279\n",
      "Minibatch:  134   2018-12-13 16:31:49.318716\n",
      "Minibatch:  135   2018-12-13 16:31:49.763596\n",
      "Minibatch:  136   2018-12-13 16:31:51.858560\n",
      "Minibatch:  137   2018-12-13 16:31:52.060405\n",
      "Minibatch:  138   2018-12-13 16:31:52.269125\n",
      "Minibatch:  139   2018-12-13 16:31:54.434283\n",
      "Minibatch:  140   2018-12-13 16:31:54.716417\n",
      "Minibatch:  141   2018-12-13 16:31:54.917279\n",
      "Minibatch:  142   2018-12-13 16:31:55.113641\n",
      "Minibatch:  143   2018-12-13 16:31:55.318919\n",
      "Minibatch:  144   2018-12-13 16:31:55.539204\n",
      "Minibatch:  145   2018-12-13 16:31:55.729793\n",
      "Minibatch:  146   2018-12-13 16:31:55.927620\n",
      "Minibatch:  147   2018-12-13 16:31:56.149222\n",
      "Minibatch:  148   2018-12-13 16:31:56.566553\n",
      "Minibatch:  149   2018-12-13 16:31:56.963128\n",
      "Minibatch:  150   2018-12-13 16:31:59.856645\n",
      "Minibatch:  151   2018-12-13 16:32:00.192484\n",
      "Minibatch:  152   2018-12-13 16:32:00.484256\n",
      "Minibatch:  153   2018-12-13 16:32:02.620209\n",
      "Minibatch:  154   2018-12-13 16:32:02.916634\n",
      "Minibatch:  155   2018-12-13 16:32:03.212641\n",
      "Minibatch:  156   2018-12-13 16:32:03.884717\n",
      "Minibatch:  157   2018-12-13 16:32:04.307139\n",
      "Minibatch:  158   2018-12-13 16:32:07.435003\n",
      "Minibatch:  159   2018-12-13 16:32:09.151598\n",
      "Minibatch:  160   2018-12-13 16:32:10.880072\n",
      "Minibatch:  161   2018-12-13 16:32:11.205327\n",
      "Minibatch:  162   2018-12-13 16:32:11.528701\n",
      "Minibatch:  163   2018-12-13 16:32:11.758765\n",
      "Minibatch:  164   2018-12-13 16:32:11.976662\n",
      "Minibatch:  165   2018-12-13 16:32:13.440245\n",
      "Minibatch:  166   2018-12-13 16:32:14.062496\n",
      "Minibatch:  167   2018-12-13 16:32:15.962285\n",
      "Minibatch:  168   2018-12-13 16:32:16.173483\n",
      "Minibatch:  169   2018-12-13 16:32:16.384472\n",
      "Minibatch:  170   2018-12-13 16:32:19.102405\n",
      "Minibatch:  171   2018-12-13 16:32:19.403543\n",
      "Minibatch:  172   2018-12-13 16:32:22.112747\n",
      "Minibatch:  173   2018-12-13 16:32:22.361336\n",
      "Minibatch:  174   2018-12-13 16:32:24.867280\n",
      "Minibatch:  175   2018-12-13 16:32:25.100914\n",
      "Minibatch:  176   2018-12-13 16:32:25.582059\n",
      "Minibatch:  177   2018-12-13 16:32:26.932327\n",
      "Minibatch:  178   2018-12-13 16:32:27.229547\n",
      "Minibatch:  179   2018-12-13 16:32:27.509013\n",
      "Minibatch:  180   2018-12-13 16:32:27.830555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch:  181   2018-12-13 16:32:28.171876\n",
      "Minibatch:  182   2018-12-13 16:32:28.391816\n",
      "Minibatch:  183   2018-12-13 16:32:28.714607\n",
      "Minibatch:  184   2018-12-13 16:32:29.129558\n",
      "Minibatch:  185   2018-12-13 16:32:32.737227\n",
      "Minibatch:  186   2018-12-13 16:32:33.024854\n",
      "Minibatch:  187   2018-12-13 16:32:33.322375\n",
      "Minibatch:  188   2018-12-13 16:32:33.597943\n",
      "Minibatch:  189   2018-12-13 16:32:33.877353\n",
      "Minibatch:  190   2018-12-13 16:32:34.071695\n",
      "Minibatch:  191   2018-12-13 16:32:34.286852\n",
      "Minibatch:  192   2018-12-13 16:32:34.499659\n",
      "Minibatch:  193   2018-12-13 16:32:34.736796\n",
      "Minibatch:  194   2018-12-13 16:32:36.782306\n",
      "Minibatch:  195   2018-12-13 16:32:37.070824\n",
      "Minibatch:  196   2018-12-13 16:32:37.360839\n",
      "Minibatch:  197   2018-12-13 16:32:40.790565\n",
      "Minibatch:  198   2018-12-13 16:32:43.652291\n",
      "Minibatch:  199   2018-12-13 16:32:43.925433\n",
      "Minibatch:  200   2018-12-13 16:32:46.522250\n",
      "Minibatch:  201   2018-12-13 16:32:46.862665\n",
      "Minibatch:  202   2018-12-13 16:32:47.283289\n",
      "Minibatch:  203   2018-12-13 16:32:47.562346\n",
      "Minibatch:  204   2018-12-13 16:32:49.008406\n",
      "Minibatch:  205   2018-12-13 16:32:49.290842\n",
      "Minibatch:  206   2018-12-13 16:32:52.577437\n",
      "Minibatch:  207   2018-12-13 16:32:52.985485\n",
      "Minibatch:  208   2018-12-13 16:32:54.947084\n",
      "Minibatch:  209   2018-12-13 16:32:55.155098\n",
      "Minibatch:  210   2018-12-13 16:32:57.283804\n",
      "Minibatch:  211   2018-12-13 16:32:57.549497\n",
      "Minibatch:  212   2018-12-13 16:33:00.016469\n",
      "Minibatch:  213   2018-12-13 16:33:02.492800\n",
      "Minibatch:  214   2018-12-13 16:33:02.716784\n",
      "Minibatch:  215   2018-12-13 16:33:02.920599\n",
      "Minibatch:  216   2018-12-13 16:33:05.823672\n",
      "Minibatch:  217   2018-12-13 16:33:07.406844\n",
      "Minibatch:  218   2018-12-13 16:33:07.700575\n",
      "Minibatch:  219   2018-12-13 16:33:09.775740\n",
      "Minibatch:  220   2018-12-13 16:33:11.771599\n",
      "Minibatch:  221   2018-12-13 16:33:11.983228\n",
      "Minibatch:  222   2018-12-13 16:33:12.214056\n",
      "Minibatch:  223   2018-12-13 16:33:12.409807\n",
      "Minibatch:  224   2018-12-13 16:33:12.617237\n",
      "Minibatch:  225   2018-12-13 16:33:14.911498\n",
      "Minibatch:  226   2018-12-13 16:33:15.188011\n",
      "Minibatch:  227   2018-12-13 16:33:15.473142\n",
      "Minibatch:  228   2018-12-13 16:33:15.758708\n",
      "Minibatch:  229   2018-12-13 16:33:16.057289\n",
      "Minibatch:  230   2018-12-13 16:33:16.321075\n",
      "Minibatch:  231   2018-12-13 16:33:16.517748\n",
      "Minibatch:  232   2018-12-13 16:33:16.746126\n",
      "Minibatch:  233   2018-12-13 16:33:17.438960\n",
      "Minibatch:  234   2018-12-13 16:33:17.747986\n",
      "Minibatch:  235   2018-12-13 16:33:18.018957\n",
      "Minibatch:  236   2018-12-13 16:33:18.303004\n",
      "Minibatch:  237   2018-12-13 16:33:21.441392\n",
      "Minibatch:  238   2018-12-13 16:33:21.798769\n",
      "Minibatch:  239   2018-12-13 16:33:22.112886\n",
      "Minibatch:  240   2018-12-13 16:33:22.409095\n",
      "Minibatch:  241   2018-12-13 16:33:24.583974\n",
      "Minibatch:  242   2018-12-13 16:33:26.310766\n",
      "Minibatch:  243   2018-12-13 16:33:26.519376\n",
      "Minibatch:  244   2018-12-13 16:33:28.448790\n",
      "Minibatch:  245   2018-12-13 16:33:31.664077\n",
      "Minibatch:  246   2018-12-13 16:33:31.859110\n",
      "Minibatch:  247   2018-12-13 16:33:32.061591\n",
      "Minibatch:  248   2018-12-13 16:33:32.478091\n",
      "Minibatch:  249   2018-12-13 16:33:35.624176\n",
      "Minibatch:  250   2018-12-13 16:33:38.090543\n",
      "Minibatch:  251   2018-12-13 16:33:41.707948\n",
      "Minibatch:  252   2018-12-13 16:33:43.340041\n",
      "Minibatch:  253   2018-12-13 16:33:43.719994\n",
      "Minibatch:  254   2018-12-13 16:33:44.051121\n",
      "Minibatch:  255   2018-12-13 16:33:44.315046\n",
      "Minibatch:  256   2018-12-13 16:33:47.527628\n",
      "Minibatch:  257   2018-12-13 16:33:47.912305\n",
      "Minibatch:  258   2018-12-13 16:33:48.192868\n",
      "Minibatch:  259   2018-12-13 16:33:48.759071\n",
      "Minibatch:  260   2018-12-13 16:33:51.697106\n",
      "Minibatch:  261   2018-12-13 16:33:51.893476\n",
      "Minibatch:  262   2018-12-13 16:33:54.523612\n",
      "Minibatch:  263   2018-12-13 16:33:54.823559\n",
      "Minibatch:  264   2018-12-13 16:33:55.141896\n",
      "Minibatch:  265   2018-12-13 16:33:57.076331\n",
      "Minibatch:  266   2018-12-13 16:34:00.272570\n",
      "Minibatch:  267   2018-12-13 16:34:00.566039\n",
      "Minibatch:  268   2018-12-13 16:34:02.750409\n",
      "Minibatch:  269   2018-12-13 16:34:03.033405\n",
      "Minibatch:  270   2018-12-13 16:34:03.326374\n",
      "Minibatch:  271   2018-12-13 16:34:03.604583\n",
      "Minibatch:  272   2018-12-13 16:34:05.371594\n",
      "Minibatch:  273   2018-12-13 16:34:05.592053\n",
      "Minibatch:  274   2018-12-13 16:34:07.920065\n",
      "Minibatch:  275   2018-12-13 16:34:11.460432\n",
      "Minibatch:  276   2018-12-13 16:34:12.982088\n",
      "Minibatch:  277   2018-12-13 16:34:15.174571\n",
      "Minibatch:  278   2018-12-13 16:34:15.540676\n",
      "Minibatch:  279   2018-12-13 16:34:18.079471\n",
      "Minibatch:  280   2018-12-13 16:34:20.040577\n",
      "Minibatch:  281   2018-12-13 16:34:23.677843\n",
      "Minibatch:  282   2018-12-13 16:34:26.918449\n",
      "Minibatch:  283   2018-12-13 16:34:27.128444\n",
      "Minibatch:  284   2018-12-13 16:34:30.104954\n",
      "Minibatch:  285   2018-12-13 16:34:30.493024\n",
      "Minibatch:  286   2018-12-13 16:34:32.727733\n",
      "Minibatch:  287   2018-12-13 16:34:33.904720\n",
      "Minibatch:  288   2018-12-13 16:34:34.184030\n",
      "Minibatch:  289   2018-12-13 16:34:36.547751\n",
      "Minibatch:  290   2018-12-13 16:34:36.834461\n",
      "Minibatch:  291   2018-12-13 16:34:37.125596\n",
      "Minibatch:  292   2018-12-13 16:34:37.411587\n",
      "Minibatch:  293   2018-12-13 16:34:37.640862\n",
      "Minibatch:  294   2018-12-13 16:34:37.834754\n",
      "Minibatch:  295   2018-12-13 16:34:38.042386\n",
      "Minibatch:  296   2018-12-13 16:34:38.271707\n",
      "Minibatch:  297   2018-12-13 16:34:38.511828\n",
      "Minibatch:  298   2018-12-13 16:34:39.064142\n",
      "Minibatch:  299   2018-12-13 16:34:39.340010\n",
      "Minibatch:  300   2018-12-13 16:34:39.626543\n",
      "Minibatch:  301   2018-12-13 16:34:39.895885\n",
      "Minibatch:  302   2018-12-13 16:34:42.033091\n",
      "Minibatch:  303   2018-12-13 16:34:42.250628\n",
      "Minibatch:  304   2018-12-13 16:34:44.961445\n",
      "Minibatch:  305   2018-12-13 16:34:45.195641\n",
      "Minibatch:  306   2018-12-13 16:34:46.562045\n",
      "Minibatch:  307   2018-12-13 16:34:46.789126\n",
      "Minibatch:  308   2018-12-13 16:34:48.730797\n",
      "Minibatch:  309   2018-12-13 16:34:50.952264\n",
      "Minibatch:  310   2018-12-13 16:34:51.239679\n",
      "Minibatch:  311   2018-12-13 16:34:51.518788\n",
      "Minibatch:  312   2018-12-13 16:34:54.869013\n",
      "Minibatch:  313   2018-12-13 16:34:55.807437\n",
      "Minibatch:  314   2018-12-13 16:34:59.200091\n",
      "Minibatch:  315   2018-12-13 16:34:59.402237\n",
      "Minibatch:  316   2018-12-13 16:34:59.737984\n",
      "Minibatch:  317   2018-12-13 16:35:02.460689\n",
      "Minibatch:  318   2018-12-13 16:35:02.856681\n",
      "Minibatch:  319   2018-12-13 16:35:05.160098\n",
      "Minibatch:  320   2018-12-13 16:35:06.861946\n",
      "Minibatch:  321   2018-12-13 16:35:07.168463\n",
      "Minibatch:  322   2018-12-13 16:35:09.107985\n",
      "Minibatch:  323   2018-12-13 16:35:09.600089\n",
      "Minibatch:  324   2018-12-13 16:35:09.891386\n",
      "Minibatch:  325   2018-12-13 16:35:12.295669\n",
      "Minibatch:  326   2018-12-13 16:35:15.301229\n",
      "Minibatch:  327   2018-12-13 16:35:15.500229\n",
      "Minibatch:  328   2018-12-13 16:35:17.527422\n",
      "Minibatch:  329   2018-12-13 16:35:17.815580\n",
      "Minibatch:  330   2018-12-13 16:35:20.093872\n",
      "Minibatch:  331   2018-12-13 16:35:20.346621\n",
      "Minibatch:  332   2018-12-13 16:35:20.758997\n",
      "Minibatch:  333   2018-12-13 16:35:21.662956\n",
      "Minibatch:  334   2018-12-13 16:35:22.025443\n",
      "Minibatch:  335   2018-12-13 16:35:25.151478\n"
     ]
    }
   ],
   "source": [
    "filepath = \"/home/muhammadayub/Desktop/CS230/models_saved/model5/model3_12_13__82_40.ckpt\"#model3_12_123__288_120.ckpt\"#model3_12_9__829_100.ckpt\" #4/model3_12_9__134_100.ckpt\" #model4/model3_12_9__9_80.ckpt\"\n",
    "# dev_test_or_train = 1\n",
    "# if(dev_test_or_train ==1): #train\n",
    "\n",
    "# filepathEvalMetrics = \"/home/muhammadayub/Desktop/CS230/models_saved/model3/model_12_9__\"+str(49)+\"_\"+str(60)+\"_evalMetrics_train.npy\"\n",
    "accuracies= []\n",
    "\n",
    "print(\" Running \", len(outputData) , \" minibatches to get the evaluation metrics\")\n",
    "output_classes = []\n",
    "predicted_classes = []\n",
    "\n",
    "preds_one_hot_ev_value = None\n",
    "output_one_hot_ev_value =None \n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        \n",
    "        #define the graph\n",
    "        X, Y = create_placeholders(64, 64, 28, 34)\n",
    "        parameters = initialize_parameters()\n",
    "        Z3 = forward_prop(X, parameters)\n",
    "        #optimization \n",
    "#         cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))\n",
    "#         optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)#1e-4).minimize(cross_entropy)\n",
    "\n",
    "        #calculating the accuracy of the model \n",
    "        softmaxZ3 = tf.nn.softmax(Z3) # size will be 2200, 34\n",
    "        \n",
    "        top4 =  tf.nn.top_k(softmaxZ3, 3)     \n",
    "        top4Indices = top4.indices\n",
    "        oneHotIndices = tf.one_hot(top4Indices, depth = 34)\n",
    "        summationOfOneHotFor_NHotEncoding = tf.reduce_sum(oneHotIndices, 1)  # this is the N Hot Encoding \n",
    "\n",
    "        \n",
    "        #equals = tf.equal(summationOfOneHotFor_NHotEncoding, Y) #boolean tensor\n",
    "        #equalsFloat32 = tf.cast(equals, tf.float32)  # 1 or 0 tensor\n",
    "        \n",
    "        summationOfOneHotFor_NHotEncoding = tf.cast(summationOfOneHotFor_NHotEncoding, tf.int32)\n",
    "        Y = tf.cast(Y, tf.int32)\n",
    "        \n",
    "        andOp = tf.bitwise.bitwise_and(summationOfOneHotFor_NHotEncoding, Y)\n",
    "        andOp = tf.cast(andOp, tf.float32)\n",
    "        sumVal = tf.reduce_sum(andOp, axis = 1)\n",
    "        sumValDivided = sumVal/3.0\n",
    "        \n",
    "        \n",
    "        accuracy = tf.reduce_mean(sumValDivided)\n",
    "        \n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        saver.restore(sess , filepath)\n",
    "        print(\"Model restored.\")\n",
    "        \n",
    "        #devMiniBatches \n",
    "        #testMiniBatches\n",
    "        for i_  in range(len(outputData)):\n",
    "\n",
    "            output_image64 = outputData[i_]\n",
    "            inputImage64 = inputData[i_]\n",
    "            \n",
    "            \n",
    "            \n",
    "            summationOfOneHotFor_NHotEncoding_val, Y_val, andOpVal, accuracy_val  = sess.run([summationOfOneHotFor_NHotEncoding,Y,andOp, accuracy], feed_dict={X: inputImage64, Y: output_image64})\n",
    "#             equalsFloat32_val = sess.run([equalsFloat32], feed_dict={X: inputImage64, Y: output_image64})\n",
    "            \n",
    "            \n",
    "#             sumValDivided, accuracy_val, nHotEncoding = sess.run([sumValDivided, accuracy, summationOfOneHotFor_NHotEncoding], feed_dict={X: inputImage64, Y: output_image64})\n",
    "#             sumValDivided_val,accuracy_val,nHotEncoding = sess.run([sumValDivided, accuracy, summationOfOneHotFor_NHotEncoding], feed_dict={X: inputImage64, Y: output_image64})\n",
    "            \n",
    "            accuracies.append(accuracy_val)\n",
    "            output_classes.append(Y_val)\n",
    "            predicted_classes.append(summationOfOneHotFor_NHotEncoding_val)\n",
    "            \n",
    "            print('Minibatch: ', str(i_), \" \", datetime.datetime.now())\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(summationOfOneHotFor_NHotEncoding_val[0])\n",
    "print(Y_val[0])\n",
    "# print(andOpVal[0])\n",
    "# type(andOpVal)\n",
    "\n",
    "# andOpVal.shape\n",
    "# summationOfOneHotFor_NHotEncoding_val.shape\n",
    "len(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputData[0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "#new graph for concatenating things and getting into the right format for the F1 score \n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    with tf.Session(graph=graph) as sess:            \n",
    "    \n",
    "        val1 = tf.concat(output_classes, axis = 0)\n",
    "        \n",
    "    \n",
    "        #after running the minibatches-> we can concatenate the values \n",
    "        preds = tf.concat(predicted_classes,axis = 0)\n",
    "        data_outputs = tf.concat(output_classes,axis = 0)\n",
    "        \n",
    "        preds_one_hot_ev, output_one_hot_ev = sess.run([preds, data_outputs])\n",
    "        preds_one_hot_ev_value=preds_one_hot_ev\n",
    "        output_one_hot_ev_value =output_one_hot_ev\n",
    "        \n",
    "#         d = 35 # d is the total number of classes you have \n",
    "        #classLabels =  np.array([2,3,5,6,5,4,2,0,0])# the indexes \n",
    "        # tf.one_hot(classLabels, d)\n",
    "\n",
    "#         preds_one_hot = tf.one_hot(preds, d)\n",
    "#         data_outputs_one_hot =tf.one_hot(data_outputs, d)#evaluated \n",
    "#         preds_one_hot_ev, output_one_hot_ev = sess.run([preds_one_hot, data_outputs_one_hot])\n",
    "        \n",
    "#         preds_one_hot_ev_value =preds_one_hot_ev\n",
    "#         output_one_hot_ev_value =output_one_hot_ev\n",
    "        \n",
    "#For Debugging below:\n",
    "#         zVal = sess.run([Z3], feed_dict={X: inputImage64, Y: output_image64})\n",
    "#         softMaxVal = sess.run([softmaxZ3], feed_dict={X: inputImage64, Y: output_image64})\n",
    "#         output_class_val = sess.run([output_class], feed_dict={X: inputImage64, Y: output_image64})\n",
    "#         num_correct_val = sess.run([num_correct], feed_dict={X: inputImage64, Y: output_image64})\n",
    "#         num_correct_to_int_val = sess.run([num_correct_to_int], feed_dict={X: inputImage64, Y: output_image64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y_true_ = None\n",
    "_y_pred_ = None \n",
    "_f1Score_ = None \n",
    "_precision_ = None \n",
    "_recall_ =None \n",
    "TPVal = None\n",
    "FPVal = None\n",
    "FNVal = None \n",
    "_f1_ = None \n",
    "#using the F1 function found online -> get the values for the predictions  \n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    y_true = tf.Variable(output_one_hot_ev_value)\n",
    "    y_pred = tf.Variable(preds_one_hot_ev_value)\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(y_pred, tf.float64)\n",
    "\n",
    "    #moded from -> https://stackoverflow.com/questions/35365007/tensorflow-precision-recall-f1-score-and-confusion-matrix\n",
    "    TP = tf.count_nonzero(y_pred * y_true, axis=0) \n",
    "    FP = tf.count_nonzero(y_pred * (y_true - 1), axis=0) \n",
    "    FN = tf.count_nonzero((y_pred - 1) * y_true, axis=0)  # .001 for numerical stability \n",
    "\n",
    "    #they are integers right now and should be floats for numerical stability \n",
    "    TP = tf.cast(TP, tf.float32)\n",
    "    FP = tf.cast(FP, tf.float32)\n",
    "    FN = tf.cast(FN, tf.float32)\n",
    "    \n",
    "    \n",
    "    precision = TP / (TP + FP+tf.constant(.001))\n",
    "    recall = TP / (TP + FN+tf.constant(.001))\n",
    "    f1 = 2 * precision * recall / (precision + recall+tf.constant(.001))\n",
    "\n",
    "    f1Score = tf.reduce_mean(f1)\n",
    "\n",
    "    \n",
    "    with tf.Session(graph=graph) as sess:    \n",
    "        tf.global_variables_initializer().run(session=sess)\n",
    "        _f1_, _y_true_, _y_pred_, _f1Score_ , _precision_ ,_recall_ , TPVal,FPVal, FNVal = sess.run([f1, y_true, y_pred, f1Score, precision ,recall, TP,FP, FN ])          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.96230445128111"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(accuracies).flatten())/len(accuracies)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031209901"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_f1_\n",
    "# _precision_ ## 75.1056541156556, f1score -> 0.07420441  -- for training set \n",
    "# _recall_   ## 77.49999894036186, f1score -> .075378135  -- Dev set \n",
    "_f1Score_   ## 74.99286914094884 , f1score -> .07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 34)\n",
      "(2, 3)\n",
      "[array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0.]],\n",
      "\n",
      "       [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]]], dtype=float32)]\n",
      "[array([[ 1,  2, 32],\n",
      "       [ 4,  3,  0]], dtype=int32)]\n",
      "----------------\n",
      "(2, 34)\n",
      "[array([[0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0.],\n",
      "       [1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[0. ,  0.34, 0.33 ,0. ,  0.  , 0. ,  0. ,  0.   ,0.  , 0. , \n",
    "                     0.   ,0. ,  0.,   0., 0.  , 0. ,  0. ,  0.,  0.   ,0.  , 0.  ,\n",
    "                     0. ,  0. ,  0. ,  0.  , 0. ,  0.  , 0., 0. ,  0. ,  0. ,\n",
    "                     0.  , 0.33, 0.  ], \n",
    "                [0.27 ,  0., 0. ,0.35 ,  0.38  , 0. ,  0. ,  0.   ,0.  , 0. , \n",
    "                     0.   ,0. ,  0.,   0., 0.  , 0. ,  0. ,  0.,  0.   ,0.  , 0.  ,\n",
    "                     0. ,  0. ,  0. ,  0.  , 0. ,  0.  , 0., 0. ,  0. ,  0. ,\n",
    "                     0.  , 0., 0.  ]])\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        x = tf.placeholder(shape = (2,34), dtype=np.float32)\n",
    "        top4 =  tf.nn.top_k(x, 3)     \n",
    "\n",
    "        top4Indices = top4.indices\n",
    "        oneHotIndices = tf.one_hot(top4Indices, depth = 34)\n",
    "        summationOfOneHotFor_NHotEncoding = tf.reduce_sum(oneHotIndices, 1)\n",
    "        \n",
    "#         top4Vals = sess.run([top4], feed_dict={x:data})\n",
    "# top4Vals[0].indices\n",
    "        top4IndicesVals = sess.run([top4Indices], feed_dict={x:data})\n",
    "        \n",
    "        oneHotIndicesValues = sess.run([oneHotIndices], feed_dict={x:data})\n",
    "        \n",
    "        NHotEncodingValues = sess.run([summationOfOneHotFor_NHotEncoding], feed_dict={x:data})\n",
    "        \n",
    "        print(oneHotIndicesValues[0].shape)\n",
    "        print(top4IndicesVals[0].shape)\n",
    "        \n",
    "        print(oneHotIndicesValues)\n",
    "        print(top4IndicesVals)\n",
    "        \n",
    "        print('----------------')\n",
    "        \n",
    "        print(NHotEncodingValues[0].shape)\n",
    "        \n",
    "        print(NHotEncodingValues)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "# tf.math.top_k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[array([2., 3.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        ab = tf.constant(np.array([[1,2,4],[42,1,5]]), dtype = np.float32)\n",
    "        bc = tf.constant(np.array([[5,3,4],[42,1,6]]), dtype = np.float32)\n",
    "        \n",
    "        equals = tf.equal(ab, bc)\n",
    "        equalsFloat32 = tf.cast(equals, tf.float32)\n",
    "        \n",
    "        sumVal = tf.reduce_sum(equalsFloat32, axis = 1)\n",
    "        sumValDivided = sumVal/3.0\n",
    "        \n",
    "        meanReduced = tf.reduce_mean (sumValDivided )\n",
    "        \n",
    "        result = sess.run([meanReduced])\n",
    "        print(result[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = tf.constant([[1,2],[3.,4]])\n",
    "        \n",
    "        mean = tf.reduce_mean(x, axis = 0)\n",
    "        print(sess.run([mean]))\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
