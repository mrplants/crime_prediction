{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muhammadayub/anaconda3/envs/py35-2/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/muhammadayub/Desktop/CS230/Notebooks/re'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleanURL(url):\n",
    "    p = pathlib.Path(url)\n",
    "    path = str(p.as_posix()) \n",
    "    return path \n",
    "\n",
    "\n",
    "def getDF(loc, sheetname):\n",
    "    dataframe = pd.read_excel(loc, sheetname)\n",
    "    #https://stackoverflow.com/questions/40950310/strip-trim-all-strings-of-a-dataframe\n",
    "    dataframe = dataframe.applymap(lambda x: x.strip() if type(x) is str else x)\n",
    "    return dataframe\n",
    "\n",
    "def printNulls(df):\n",
    "    null_columns = df.columns[df.isnull().any()]\n",
    "    return df[null_columns].isnull().sum() \n",
    "\n",
    "\n",
    "def writeDFToFile(dfs, path_): #dfs is an array of dataframes and their sheet names , path needs to have\n",
    "    time_ = str(datetime.datetime.now())\n",
    "    current_date_time = time_[0:time_.index(\".\")]\n",
    "    current_date_time = current_date_time.replace(\":\", \"-\")\n",
    "    task4_fileoutput = path_+current_date_time+\".xlsx\"\n",
    "\n",
    "    writer = pd.ExcelWriter(task4_fileoutput)\n",
    "    \n",
    "    for df_tuple in dfs:  \n",
    "        df = df_tuple[0]\n",
    "        sheetName = df_tuple[1]\n",
    "        df.to_excel(writer, sheetName)\n",
    "    print(\"file written to :       \" + task4_fileoutput)\n",
    "    writer.save()\n",
    "    \n",
    "def plotImg(img):\n",
    "    if(type(img) ==type(None)):\n",
    "        img =outputsDataReal[10000]\n",
    "    arr = []\n",
    "    for a in img:\n",
    "        arr = [a] + arr\n",
    "    plt.pcolor( arr, cmap = 'gist_ncar' )\n",
    "\n",
    "    plt.show()    \n",
    "\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the Raw Data for the Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_AMT = 34  # model5/model3_12_9__541_160.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "devMiniBatches = np.load( '/home/muhammadayub/Desktop/CS230/models_saved/model6/devMiniBatches.npy')\n",
    "testMiniBatches = np.load( '/home/muhammadayub/Desktop/CS230/models_saved/model6/testMiniBatches.npy' )\n",
    "minibatches = np.load( '/home/muhammadayub/Desktop/CS230/models_saved/model6/minibatches.npy' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([289, 271,  79, 293,  71, 377, 180, 265, 115, 415,  43,  55, 294,\n",
       "       320, 253, 432, 145,  56, 146, 434, 279,  44, 327, 134, 394, 163])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "devMiniBatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs= np.load('/home/muhammadayub/Desktop/CS230/training_data/minibatches_12_16_3hot_normalized/outputsTotal2.npy', 'r' )\n",
    "inputs =np.load('/home/muhammadayub/Desktop/CS230/training_data/minibatches_12_16_3hot_normalized/inputsTotal2.npy', 'r'  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104800\n",
      "104800\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs))\n",
    "print(len(outputs))\n",
    "#split the data to minibatches\n",
    "MINIBATCHES_AMT = 524 # number of minibatches \n",
    "inputs = np.split(inputs, MINIBATCHES_AMT )\n",
    "outputs = np.split(outputs, MINIBATCHES_AMT )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0.   0.   0.   0.   0.   0.33 0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.34 0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   1.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(cp[:2])\n",
    "# print(outputs[0][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the model and learn from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_H0 -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    X = tf.placeholder(shape =[None, n_H0, n_W0, n_C0], dtype = np.float32, name=\"X\")\n",
    "    Y = tf.placeholder(shape  =[None, n_y], dtype = np.float32 , name=\"Y\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():    \n",
    "\n",
    "    W1 = tf.get_variable(\"W1\", [10, 10, 28, 10], initializer =tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W2 = tf.get_variable(\"W2\", [6, 6, 10, 2], initializer =tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W3 = tf.get_variable(\"W3\", [35,200] , initializer =tf.contrib.layers.xavier_initializer(seed = 0) )\n",
    "\n",
    "    parameters = {\"W1\": W1, \"W2\": W2 , 'W3': 'W3'}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, params):\n",
    "    W1 = params['W1']\n",
    "    W2 = params['W2']    \n",
    "    W3 = params['W3']\n",
    "    \n",
    "    #convolution \n",
    "    Z1 = tf.nn.conv2d(X,W1, strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    #bias added automatically # RELU\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    \n",
    "    #average pooling -> at this point all features/weights are important to us\n",
    "    P1 = tf.nn.avg_pool(A1, ksize = [1,3,3,1], strides = [1,1,1,1], padding = 'SAME')\n",
    "\n",
    "    # convolution \n",
    "    Z2 = tf.nn.conv2d(P1,W2, strides = [1,2,2,1], padding = 'VALID')\n",
    "    \n",
    "    #RELU\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    \n",
    "    #max pooling\n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1,3,3,1], strides = [1,1,1,1], padding = 'VALID')\n",
    "    \n",
    "    #flatten\n",
    "    P2 = tf.contrib.layers.flatten(P2)\n",
    "\n",
    "    #fully connected\n",
    "    Z3 = tf.contrib.layers.fully_connected(P2, CATEGORY_AMT, activation_fn = None) #1 for yes/no\n",
    "    #going to add the softmax directly\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22:04 - 22:08 Tuesday\n",
    "# 22:10 - 22:20\n",
    "#Restoring the model\n",
    "outputData = outputs\n",
    "inputData = inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running  470  minibatches to get the evaluation metrics\n",
      "INFO:tensorflow:Restoring parameters from /home/muhammadayub/Desktop/CS230/models_saved/model6/model6_12_15__194_60.ckpt\n",
      "Model restored.\n",
      "Minibatch:  0   2018-12-16 10:25:17.667160\n",
      "Minibatch:  1   2018-12-16 10:25:19.287598\n",
      "Minibatch:  2   2018-12-16 10:25:20.907697\n",
      "Minibatch:  3   2018-12-16 10:25:22.704461\n",
      "Minibatch:  4   2018-12-16 10:25:24.233625\n",
      "Minibatch:  5   2018-12-16 10:25:25.946632\n",
      "Minibatch:  6   2018-12-16 10:25:27.669638\n",
      "Minibatch:  7   2018-12-16 10:25:29.323396\n",
      "Minibatch:  8   2018-12-16 10:25:31.223139\n",
      "Minibatch:  9   2018-12-16 10:25:32.978946\n",
      "Minibatch:  10   2018-12-16 10:25:35.065317\n",
      "Minibatch:  11   2018-12-16 10:25:36.809393\n",
      "Minibatch:  12   2018-12-16 10:25:38.702144\n",
      "Minibatch:  13   2018-12-16 10:25:40.542982\n",
      "Minibatch:  14   2018-12-16 10:25:42.511926\n",
      "Minibatch:  15   2018-12-16 10:25:44.209970\n",
      "Minibatch:  16   2018-12-16 10:25:46.048136\n",
      "Minibatch:  17   2018-12-16 10:25:47.922777\n",
      "Minibatch:  18   2018-12-16 10:25:49.829973\n",
      "Minibatch:  19   2018-12-16 10:25:51.433081\n",
      "Minibatch:  20   2018-12-16 10:25:53.187820\n",
      "Minibatch:  21   2018-12-16 10:25:54.881219\n",
      "Minibatch:  22   2018-12-16 10:25:56.721633\n",
      "Minibatch:  23   2018-12-16 10:25:58.233920\n",
      "Minibatch:  24   2018-12-16 10:26:00.073478\n",
      "Minibatch:  25   2018-12-16 10:26:01.760556\n",
      "Minibatch:  26   2018-12-16 10:26:03.588509\n",
      "Minibatch:  27   2018-12-16 10:26:05.348447\n",
      "Minibatch:  28   2018-12-16 10:26:06.929183\n",
      "Minibatch:  29   2018-12-16 10:26:08.693141\n",
      "Minibatch:  30   2018-12-16 10:26:10.692639\n",
      "Minibatch:  31   2018-12-16 10:26:12.355544\n",
      "Minibatch:  32   2018-12-16 10:26:14.272731\n",
      "Minibatch:  33   2018-12-16 10:26:16.121058\n",
      "Minibatch:  34   2018-12-16 10:26:17.974819\n",
      "Minibatch:  35   2018-12-16 10:26:19.618601\n",
      "Minibatch:  36   2018-12-16 10:26:21.498964\n",
      "Minibatch:  37   2018-12-16 10:26:23.435005\n",
      "Minibatch:  38   2018-12-16 10:26:25.185430\n",
      "Minibatch:  39   2018-12-16 10:26:27.098266\n",
      "Minibatch:  40   2018-12-16 10:26:28.884755\n",
      "Minibatch:  41   2018-12-16 10:26:30.799051\n",
      "Minibatch:  42   2018-12-16 10:26:32.848129\n",
      "Minibatch:  43   2018-12-16 10:26:34.577417\n",
      "Minibatch:  44   2018-12-16 10:26:36.472860\n",
      "Minibatch:  45   2018-12-16 10:26:38.373589\n",
      "Minibatch:  46   2018-12-16 10:26:39.983710\n",
      "Minibatch:  47   2018-12-16 10:26:41.657345\n",
      "Minibatch:  48   2018-12-16 10:26:43.200766\n",
      "Minibatch:  49   2018-12-16 10:26:44.861568\n",
      "Minibatch:  50   2018-12-16 10:26:46.532972\n",
      "Minibatch:  51   2018-12-16 10:26:48.490729\n",
      "Minibatch:  52   2018-12-16 10:26:50.132659\n",
      "Minibatch:  53   2018-12-16 10:26:51.857117\n",
      "Minibatch:  54   2018-12-16 10:26:53.867107\n",
      "Minibatch:  55   2018-12-16 10:26:55.664249\n",
      "Minibatch:  56   2018-12-16 10:26:57.174436\n",
      "Minibatch:  57   2018-12-16 10:26:58.812326\n",
      "Minibatch:  58   2018-12-16 10:27:00.818028\n",
      "Minibatch:  59   2018-12-16 10:27:02.478598\n",
      "Minibatch:  60   2018-12-16 10:27:04.625109\n",
      "Minibatch:  61   2018-12-16 10:27:06.346228\n",
      "Minibatch:  62   2018-12-16 10:27:07.855441\n",
      "Minibatch:  63   2018-12-16 10:27:09.557050\n",
      "Minibatch:  64   2018-12-16 10:27:11.379065\n",
      "Minibatch:  65   2018-12-16 10:27:13.001155\n",
      "Minibatch:  66   2018-12-16 10:27:13.224948\n",
      "Minibatch:  67   2018-12-16 10:27:14.812711\n",
      "Minibatch:  68   2018-12-16 10:27:16.796467\n",
      "Minibatch:  69   2018-12-16 10:27:18.635272\n",
      "Minibatch:  70   2018-12-16 10:27:20.514476\n",
      "Minibatch:  71   2018-12-16 10:27:22.471014\n",
      "Minibatch:  72   2018-12-16 10:27:24.287890\n",
      "Minibatch:  73   2018-12-16 10:27:26.101078\n",
      "Minibatch:  74   2018-12-16 10:27:28.042522\n",
      "Minibatch:  75   2018-12-16 10:27:29.860224\n",
      "Minibatch:  76   2018-12-16 10:27:31.644222\n",
      "Minibatch:  77   2018-12-16 10:27:33.505064\n",
      "Minibatch:  78   2018-12-16 10:27:35.548334\n",
      "Minibatch:  79   2018-12-16 10:27:37.112926\n",
      "Minibatch:  80   2018-12-16 10:27:38.813187\n",
      "Minibatch:  81   2018-12-16 10:27:40.820990\n",
      "Minibatch:  82   2018-12-16 10:27:42.403090\n",
      "Minibatch:  83   2018-12-16 10:27:44.211647\n",
      "Minibatch:  84   2018-12-16 10:27:45.810303\n",
      "Minibatch:  85   2018-12-16 10:27:47.604533\n",
      "Minibatch:  86   2018-12-16 10:27:49.448128\n",
      "Minibatch:  87   2018-12-16 10:27:51.059205\n",
      "Minibatch:  88   2018-12-16 10:27:52.797187\n",
      "Minibatch:  89   2018-12-16 10:27:54.436787\n",
      "Minibatch:  90   2018-12-16 10:27:56.235473\n",
      "Minibatch:  91   2018-12-16 10:27:57.962808\n",
      "Minibatch:  92   2018-12-16 10:27:59.619764\n",
      "Minibatch:  93   2018-12-16 10:28:01.203888\n",
      "Minibatch:  94   2018-12-16 10:28:02.789026\n",
      "Minibatch:  95   2018-12-16 10:28:04.657817\n",
      "Minibatch:  96   2018-12-16 10:28:06.600141\n",
      "Minibatch:  97   2018-12-16 10:28:08.220896\n",
      "Minibatch:  98   2018-12-16 10:28:10.044504\n",
      "Minibatch:  99   2018-12-16 10:28:11.951528\n",
      "Minibatch:  100   2018-12-16 10:28:13.694923\n",
      "Minibatch:  101   2018-12-16 10:28:15.177170\n",
      "Minibatch:  102   2018-12-16 10:28:16.816987\n",
      "Minibatch:  103   2018-12-16 10:28:18.337644\n",
      "Minibatch:  104   2018-12-16 10:28:20.061662\n",
      "Minibatch:  105   2018-12-16 10:28:21.576720\n",
      "Minibatch:  106   2018-12-16 10:28:23.416773\n",
      "Minibatch:  107   2018-12-16 10:28:25.273004\n",
      "Minibatch:  108   2018-12-16 10:28:26.857197\n",
      "Minibatch:  109   2018-12-16 10:28:28.654578\n",
      "Minibatch:  110   2018-12-16 10:28:30.616550\n",
      "Minibatch:  111   2018-12-16 10:28:32.324825\n",
      "Minibatch:  112   2018-12-16 10:28:34.196154\n",
      "Minibatch:  113   2018-12-16 10:28:35.920673\n",
      "Minibatch:  114   2018-12-16 10:28:37.650816\n",
      "Minibatch:  115   2018-12-16 10:28:39.440142\n",
      "Minibatch:  116   2018-12-16 10:28:41.107088\n",
      "Minibatch:  117   2018-12-16 10:28:43.118085\n",
      "Minibatch:  118   2018-12-16 10:28:44.834916\n",
      "Minibatch:  119   2018-12-16 10:28:46.531371\n",
      "Minibatch:  120   2018-12-16 10:28:48.231034\n",
      "Minibatch:  121   2018-12-16 10:28:49.967368\n",
      "Minibatch:  122   2018-12-16 10:28:51.654429\n",
      "Minibatch:  123   2018-12-16 10:28:53.402945\n",
      "Minibatch:  124   2018-12-16 10:28:55.204130\n",
      "Minibatch:  125   2018-12-16 10:28:57.230863\n",
      "Minibatch:  126   2018-12-16 10:28:59.021816\n",
      "Minibatch:  127   2018-12-16 10:29:00.884209\n",
      "Minibatch:  128   2018-12-16 10:29:03.045269\n",
      "Minibatch:  129   2018-12-16 10:29:04.985523\n",
      "Minibatch:  130   2018-12-16 10:29:06.720862\n",
      "Minibatch:  131   2018-12-16 10:29:09.063809\n",
      "Minibatch:  132   2018-12-16 10:29:10.768740\n",
      "Minibatch:  133   2018-12-16 10:29:12.319841\n",
      "Minibatch:  134   2018-12-16 10:29:14.171560\n",
      "Minibatch:  135   2018-12-16 10:29:16.032048\n",
      "Minibatch:  136   2018-12-16 10:29:17.870284\n",
      "Minibatch:  137   2018-12-16 10:29:19.704403\n",
      "Minibatch:  138   2018-12-16 10:29:20.999922\n",
      "Minibatch:  139   2018-12-16 10:29:22.667482\n",
      "Minibatch:  140   2018-12-16 10:29:24.501349\n",
      "Minibatch:  141   2018-12-16 10:29:26.208845\n",
      "Minibatch:  142   2018-12-16 10:29:28.008505\n",
      "Minibatch:  143   2018-12-16 10:29:29.642478\n",
      "Minibatch:  144   2018-12-16 10:29:31.225293\n",
      "Minibatch:  145   2018-12-16 10:29:33.034382\n",
      "Minibatch:  146   2018-12-16 10:29:35.255063\n",
      "Minibatch:  147   2018-12-16 10:29:36.766921\n",
      "Minibatch:  148   2018-12-16 10:29:38.831082\n",
      "Minibatch:  149   2018-12-16 10:29:40.596630\n",
      "Minibatch:  150   2018-12-16 10:29:42.283139\n",
      "Minibatch:  151   2018-12-16 10:29:43.640412\n",
      "Minibatch:  152   2018-12-16 10:29:45.170265\n",
      "Minibatch:  153   2018-12-16 10:29:46.909083\n",
      "Minibatch:  154   2018-12-16 10:29:48.722315\n",
      "Minibatch:  155   2018-12-16 10:29:50.387880\n",
      "Minibatch:  156   2018-12-16 10:29:52.303352\n",
      "Minibatch:  157   2018-12-16 10:29:54.097865\n",
      "Minibatch:  158   2018-12-16 10:29:56.130416\n",
      "Minibatch:  159   2018-12-16 10:29:57.941656\n",
      "Minibatch:  160   2018-12-16 10:29:59.620389\n",
      "Minibatch:  161   2018-12-16 10:30:01.171554\n",
      "Minibatch:  162   2018-12-16 10:30:02.874065\n",
      "Minibatch:  163   2018-12-16 10:30:04.690573\n",
      "Minibatch:  164   2018-12-16 10:30:06.475341\n",
      "Minibatch:  165   2018-12-16 10:30:08.208962\n",
      "Minibatch:  166   2018-12-16 10:30:10.380245\n",
      "Minibatch:  167   2018-12-16 10:30:11.969189\n",
      "Minibatch:  168   2018-12-16 10:30:13.821981\n",
      "Minibatch:  169   2018-12-16 10:30:15.600011\n",
      "Minibatch:  170   2018-12-16 10:30:17.114954\n",
      "Minibatch:  171   2018-12-16 10:30:18.726416\n",
      "Minibatch:  172   2018-12-16 10:30:20.355252\n",
      "Minibatch:  173   2018-12-16 10:30:22.006247\n",
      "Minibatch:  174   2018-12-16 10:30:23.742869\n",
      "Minibatch:  175   2018-12-16 10:30:25.250133\n",
      "Minibatch:  176   2018-12-16 10:30:27.338752\n",
      "Minibatch:  177   2018-12-16 10:30:29.058018\n",
      "Minibatch:  178   2018-12-16 10:30:30.853950\n",
      "Minibatch:  179   2018-12-16 10:30:32.505705\n",
      "Minibatch:  180   2018-12-16 10:30:34.298177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch:  181   2018-12-16 10:30:35.772405\n",
      "Minibatch:  182   2018-12-16 10:30:37.480794\n",
      "Minibatch:  183   2018-12-16 10:30:39.352255\n",
      "Minibatch:  184   2018-12-16 10:30:41.316695\n",
      "Minibatch:  185   2018-12-16 10:30:42.964292\n",
      "Minibatch:  186   2018-12-16 10:30:44.543973\n",
      "Minibatch:  187   2018-12-16 10:30:46.336763\n",
      "Minibatch:  188   2018-12-16 10:30:48.097493\n",
      "Minibatch:  189   2018-12-16 10:30:50.044813\n",
      "Minibatch:  190   2018-12-16 10:30:51.679797\n",
      "Minibatch:  191   2018-12-16 10:30:53.407716\n",
      "Minibatch:  192   2018-12-16 10:30:55.077603\n",
      "Minibatch:  193   2018-12-16 10:30:56.884154\n",
      "Minibatch:  194   2018-12-16 10:30:58.655449\n",
      "Minibatch:  195   2018-12-16 10:31:00.412179\n",
      "Minibatch:  196   2018-12-16 10:31:01.776077\n",
      "Minibatch:  197   2018-12-16 10:31:03.270762\n",
      "Minibatch:  198   2018-12-16 10:31:04.595197\n",
      "Minibatch:  199   2018-12-16 10:31:06.437017\n",
      "Minibatch:  200   2018-12-16 10:31:07.938106\n",
      "Minibatch:  201   2018-12-16 10:31:09.784828\n",
      "Minibatch:  202   2018-12-16 10:31:11.383702\n",
      "Minibatch:  203   2018-12-16 10:31:13.140723\n",
      "Minibatch:  204   2018-12-16 10:31:14.853063\n",
      "Minibatch:  205   2018-12-16 10:31:16.716224\n",
      "Minibatch:  206   2018-12-16 10:31:18.446520\n",
      "Minibatch:  207   2018-12-16 10:31:20.296496\n",
      "Minibatch:  208   2018-12-16 10:31:22.578456\n",
      "Minibatch:  209   2018-12-16 10:31:24.309107\n",
      "Minibatch:  210   2018-12-16 10:31:26.015950\n",
      "Minibatch:  211   2018-12-16 10:31:27.751232\n",
      "Minibatch:  212   2018-12-16 10:31:29.467532\n",
      "Minibatch:  213   2018-12-16 10:31:31.137189\n",
      "Minibatch:  214   2018-12-16 10:31:32.877951\n",
      "Minibatch:  215   2018-12-16 10:31:34.814966\n",
      "Minibatch:  216   2018-12-16 10:31:36.714158\n",
      "Minibatch:  217   2018-12-16 10:31:38.571340\n",
      "Minibatch:  218   2018-12-16 10:31:40.576472\n",
      "Minibatch:  219   2018-12-16 10:31:42.570238\n",
      "Minibatch:  220   2018-12-16 10:31:44.350432\n",
      "Minibatch:  221   2018-12-16 10:31:46.220316\n",
      "Minibatch:  222   2018-12-16 10:31:48.119522\n",
      "Minibatch:  223   2018-12-16 10:31:49.657384\n",
      "Minibatch:  224   2018-12-16 10:31:51.228776\n",
      "Minibatch:  225   2018-12-16 10:31:53.046388\n",
      "Minibatch:  226   2018-12-16 10:31:54.727075\n",
      "Minibatch:  227   2018-12-16 10:31:56.530892\n",
      "Minibatch:  228   2018-12-16 10:31:58.284769\n",
      "Minibatch:  229   2018-12-16 10:31:59.871956\n",
      "Minibatch:  230   2018-12-16 10:32:02.241761\n",
      "Minibatch:  231   2018-12-16 10:32:04.229072\n",
      "Minibatch:  232   2018-12-16 10:32:06.174027\n",
      "Minibatch:  233   2018-12-16 10:32:08.013989\n",
      "Minibatch:  234   2018-12-16 10:32:09.854640\n",
      "Minibatch:  235   2018-12-16 10:32:11.445123\n",
      "Minibatch:  236   2018-12-16 10:32:13.180695\n",
      "Minibatch:  237   2018-12-16 10:32:15.055226\n",
      "Minibatch:  238   2018-12-16 10:32:16.811727\n",
      "Minibatch:  239   2018-12-16 10:32:18.420207\n",
      "Minibatch:  240   2018-12-16 10:32:20.253861\n",
      "Minibatch:  241   2018-12-16 10:32:21.905387\n",
      "Minibatch:  242   2018-12-16 10:32:23.546034\n",
      "Minibatch:  243   2018-12-16 10:32:25.131378\n",
      "Minibatch:  244   2018-12-16 10:32:26.777696\n",
      "Minibatch:  245   2018-12-16 10:32:28.618008\n",
      "Minibatch:  246   2018-12-16 10:32:30.332670\n",
      "Minibatch:  247   2018-12-16 10:32:32.095483\n",
      "Minibatch:  248   2018-12-16 10:32:33.889418\n",
      "Minibatch:  249   2018-12-16 10:32:35.440909\n",
      "Minibatch:  250   2018-12-16 10:32:37.462647\n",
      "Minibatch:  251   2018-12-16 10:32:39.083829\n",
      "Minibatch:  252   2018-12-16 10:32:40.571004\n",
      "Minibatch:  253   2018-12-16 10:32:42.169322\n",
      "Minibatch:  254   2018-12-16 10:32:44.023399\n",
      "Minibatch:  255   2018-12-16 10:32:45.672921\n",
      "Minibatch:  256   2018-12-16 10:32:47.610935\n",
      "Minibatch:  257   2018-12-16 10:32:49.353459\n",
      "Minibatch:  258   2018-12-16 10:32:50.937120\n",
      "Minibatch:  259   2018-12-16 10:32:52.572525\n",
      "Minibatch:  260   2018-12-16 10:32:54.260417\n",
      "Minibatch:  261   2018-12-16 10:32:55.979551\n",
      "Minibatch:  262   2018-12-16 10:32:57.695268\n",
      "Minibatch:  263   2018-12-16 10:32:59.363894\n",
      "Minibatch:  264   2018-12-16 10:33:00.950146\n",
      "Minibatch:  265   2018-12-16 10:33:02.844115\n",
      "Minibatch:  266   2018-12-16 10:33:04.567203\n",
      "Minibatch:  267   2018-12-16 10:33:06.163548\n",
      "Minibatch:  268   2018-12-16 10:33:07.727772\n",
      "Minibatch:  269   2018-12-16 10:33:09.415959\n",
      "Minibatch:  270   2018-12-16 10:33:11.037797\n",
      "Minibatch:  271   2018-12-16 10:33:12.741655\n",
      "Minibatch:  272   2018-12-16 10:33:14.446643\n",
      "Minibatch:  273   2018-12-16 10:33:16.276494\n",
      "Minibatch:  274   2018-12-16 10:33:18.064732\n",
      "Minibatch:  275   2018-12-16 10:33:19.865553\n",
      "Minibatch:  276   2018-12-16 10:33:21.491937\n",
      "Minibatch:  277   2018-12-16 10:33:22.978211\n",
      "Minibatch:  278   2018-12-16 10:33:25.041344\n",
      "Minibatch:  279   2018-12-16 10:33:26.622104\n",
      "Minibatch:  280   2018-12-16 10:33:28.141050\n",
      "Minibatch:  281   2018-12-16 10:33:29.963997\n",
      "Minibatch:  282   2018-12-16 10:33:31.779041\n",
      "Minibatch:  283   2018-12-16 10:33:33.263973\n",
      "Minibatch:  284   2018-12-16 10:33:35.305214\n",
      "Minibatch:  285   2018-12-16 10:33:37.024401\n",
      "Minibatch:  286   2018-12-16 10:33:38.852873\n",
      "Minibatch:  287   2018-12-16 10:33:40.534784\n",
      "Minibatch:  288   2018-12-16 10:33:42.312955\n",
      "Minibatch:  289   2018-12-16 10:33:43.716961\n",
      "Minibatch:  290   2018-12-16 10:33:45.668645\n",
      "Minibatch:  291   2018-12-16 10:33:47.560745\n",
      "Minibatch:  292   2018-12-16 10:33:49.120355\n",
      "Minibatch:  293   2018-12-16 10:33:50.921200\n",
      "Minibatch:  294   2018-12-16 10:33:52.805322\n",
      "Minibatch:  295   2018-12-16 10:33:54.444951\n",
      "Minibatch:  296   2018-12-16 10:33:55.965024\n",
      "Minibatch:  297   2018-12-16 10:33:57.692856\n",
      "Minibatch:  298   2018-12-16 10:33:59.349961\n",
      "Minibatch:  299   2018-12-16 10:34:01.485559\n",
      "Minibatch:  300   2018-12-16 10:34:03.050369\n",
      "Minibatch:  301   2018-12-16 10:34:04.785163\n",
      "Minibatch:  302   2018-12-16 10:34:06.345713\n",
      "Minibatch:  303   2018-12-16 10:34:08.053341\n",
      "Minibatch:  304   2018-12-16 10:34:09.910371\n",
      "Minibatch:  305   2018-12-16 10:34:11.600940\n",
      "Minibatch:  306   2018-12-16 10:34:13.370916\n",
      "Minibatch:  307   2018-12-16 10:34:15.032643\n",
      "Minibatch:  308   2018-12-16 10:34:16.764064\n",
      "Minibatch:  309   2018-12-16 10:34:18.461779\n",
      "Minibatch:  310   2018-12-16 10:34:20.037753\n",
      "Minibatch:  311   2018-12-16 10:34:21.747703\n",
      "Minibatch:  312   2018-12-16 10:34:23.504784\n",
      "Minibatch:  313   2018-12-16 10:34:25.143604\n",
      "Minibatch:  314   2018-12-16 10:34:26.808652\n",
      "Minibatch:  315   2018-12-16 10:34:28.491895\n",
      "Minibatch:  316   2018-12-16 10:34:30.216142\n",
      "Minibatch:  317   2018-12-16 10:34:32.094626\n",
      "Minibatch:  318   2018-12-16 10:34:33.866757\n",
      "Minibatch:  319   2018-12-16 10:34:35.728750\n",
      "Minibatch:  320   2018-12-16 10:34:37.288280\n",
      "Minibatch:  321   2018-12-16 10:34:39.182216\n",
      "Minibatch:  322   2018-12-16 10:34:41.008623\n",
      "Minibatch:  323   2018-12-16 10:34:42.813120\n",
      "Minibatch:  324   2018-12-16 10:34:44.626525\n",
      "Minibatch:  325   2018-12-16 10:34:46.066055\n",
      "Minibatch:  326   2018-12-16 10:34:47.994648\n",
      "Minibatch:  327   2018-12-16 10:34:49.513048\n",
      "Minibatch:  328   2018-12-16 10:34:51.186265\n",
      "Minibatch:  329   2018-12-16 10:34:52.785345\n",
      "Minibatch:  330   2018-12-16 10:34:54.438664\n",
      "Minibatch:  331   2018-12-16 10:34:56.029375\n",
      "Minibatch:  332   2018-12-16 10:34:57.636748\n",
      "Minibatch:  333   2018-12-16 10:34:59.172241\n",
      "Minibatch:  334   2018-12-16 10:35:00.916518\n",
      "Minibatch:  335   2018-12-16 10:35:02.701573\n",
      "Minibatch:  336   2018-12-16 10:35:04.428111\n",
      "Minibatch:  337   2018-12-16 10:35:06.200059\n",
      "Minibatch:  338   2018-12-16 10:35:07.713101\n",
      "Minibatch:  339   2018-12-16 10:35:09.446824\n",
      "Minibatch:  340   2018-12-16 10:35:11.154961\n",
      "Minibatch:  341   2018-12-16 10:35:13.008079\n",
      "Minibatch:  342   2018-12-16 10:35:14.783822\n",
      "Minibatch:  343   2018-12-16 10:35:16.445915\n",
      "Minibatch:  344   2018-12-16 10:35:18.351954\n",
      "Minibatch:  345   2018-12-16 10:35:20.347838\n",
      "Minibatch:  346   2018-12-16 10:35:22.137973\n",
      "Minibatch:  347   2018-12-16 10:35:23.955703\n",
      "Minibatch:  348   2018-12-16 10:35:26.027631\n",
      "Minibatch:  349   2018-12-16 10:35:27.606902\n",
      "Minibatch:  350   2018-12-16 10:35:29.209634\n",
      "Minibatch:  351   2018-12-16 10:35:30.671758\n",
      "Minibatch:  352   2018-12-16 10:35:32.080827\n",
      "Minibatch:  353   2018-12-16 10:35:34.162737\n",
      "Minibatch:  354   2018-12-16 10:35:35.925933\n",
      "Minibatch:  355   2018-12-16 10:35:37.605101\n",
      "Minibatch:  356   2018-12-16 10:35:39.355139\n",
      "Minibatch:  357   2018-12-16 10:35:41.182953\n",
      "Minibatch:  358   2018-12-16 10:35:43.228326\n",
      "Minibatch:  359   2018-12-16 10:35:44.718018\n",
      "Minibatch:  360   2018-12-16 10:35:46.560562\n",
      "Minibatch:  361   2018-12-16 10:35:48.413687\n",
      "Minibatch:  362   2018-12-16 10:35:50.011209\n",
      "Minibatch:  363   2018-12-16 10:35:51.615835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch:  364   2018-12-16 10:35:53.400843\n",
      "Minibatch:  365   2018-12-16 10:35:55.062404\n",
      "Minibatch:  366   2018-12-16 10:35:56.901270\n",
      "Minibatch:  367   2018-12-16 10:35:58.501657\n",
      "Minibatch:  368   2018-12-16 10:36:00.148350\n",
      "Minibatch:  369   2018-12-16 10:36:02.024594\n",
      "Minibatch:  370   2018-12-16 10:36:03.865368\n",
      "Minibatch:  371   2018-12-16 10:36:05.732353\n",
      "Minibatch:  372   2018-12-16 10:36:07.247004\n",
      "Minibatch:  373   2018-12-16 10:36:08.831782\n",
      "Minibatch:  374   2018-12-16 10:36:10.686022\n",
      "Minibatch:  375   2018-12-16 10:36:12.364538\n",
      "Minibatch:  376   2018-12-16 10:36:13.843341\n",
      "Minibatch:  377   2018-12-16 10:36:15.841509\n",
      "Minibatch:  378   2018-12-16 10:36:17.559151\n",
      "Minibatch:  379   2018-12-16 10:36:19.418088\n",
      "Minibatch:  380   2018-12-16 10:36:21.170189\n",
      "Minibatch:  381   2018-12-16 10:36:23.229630\n",
      "Minibatch:  382   2018-12-16 10:36:24.835824\n",
      "Minibatch:  383   2018-12-16 10:36:26.351942\n",
      "Minibatch:  384   2018-12-16 10:36:28.182814\n",
      "Minibatch:  385   2018-12-16 10:36:29.987033\n",
      "Minibatch:  386   2018-12-16 10:36:31.811073\n",
      "Minibatch:  387   2018-12-16 10:36:33.422870\n",
      "Minibatch:  388   2018-12-16 10:36:35.133166\n",
      "Minibatch:  389   2018-12-16 10:36:36.909983\n",
      "Minibatch:  390   2018-12-16 10:36:38.517568\n",
      "Minibatch:  391   2018-12-16 10:36:40.366058\n",
      "Minibatch:  392   2018-12-16 10:36:42.113182\n",
      "Minibatch:  393   2018-12-16 10:36:43.646032\n",
      "Minibatch:  394   2018-12-16 10:36:45.094820\n",
      "Minibatch:  395   2018-12-16 10:36:46.821642\n",
      "Minibatch:  396   2018-12-16 10:36:48.320491\n",
      "Minibatch:  397   2018-12-16 10:36:50.112915\n",
      "Minibatch:  398   2018-12-16 10:36:51.709163\n",
      "Minibatch:  399   2018-12-16 10:36:53.311642\n",
      "Minibatch:  400   2018-12-16 10:36:54.979454\n",
      "Minibatch:  401   2018-12-16 10:36:56.812636\n",
      "Minibatch:  402   2018-12-16 10:36:58.496557\n",
      "Minibatch:  403   2018-12-16 10:37:00.170296\n",
      "Minibatch:  404   2018-12-16 10:37:01.939599\n",
      "Minibatch:  405   2018-12-16 10:37:03.644785\n",
      "Minibatch:  406   2018-12-16 10:37:05.401880\n",
      "Minibatch:  407   2018-12-16 10:37:07.668829\n",
      "Minibatch:  408   2018-12-16 10:37:09.531529\n",
      "Minibatch:  409   2018-12-16 10:37:11.271847\n",
      "Minibatch:  410   2018-12-16 10:37:13.217884\n",
      "Minibatch:  411   2018-12-16 10:37:14.884684\n",
      "Minibatch:  412   2018-12-16 10:37:16.787272\n",
      "Minibatch:  413   2018-12-16 10:37:18.580451\n",
      "Minibatch:  414   2018-12-16 10:37:20.140029\n",
      "Minibatch:  415   2018-12-16 10:37:21.625837\n",
      "Minibatch:  416   2018-12-16 10:37:23.428771\n",
      "Minibatch:  417   2018-12-16 10:37:25.437703\n",
      "Minibatch:  418   2018-12-16 10:37:26.940724\n",
      "Minibatch:  419   2018-12-16 10:37:28.493079\n",
      "Minibatch:  420   2018-12-16 10:37:30.068842\n",
      "Minibatch:  421   2018-12-16 10:37:32.062813\n",
      "Minibatch:  422   2018-12-16 10:37:34.286470\n",
      "Minibatch:  423   2018-12-16 10:37:36.069243\n",
      "Minibatch:  424   2018-12-16 10:37:37.906215\n",
      "Minibatch:  425   2018-12-16 10:37:39.585589\n",
      "Minibatch:  426   2018-12-16 10:37:41.316143\n",
      "Minibatch:  427   2018-12-16 10:37:43.166545\n",
      "Minibatch:  428   2018-12-16 10:37:44.845935\n",
      "Minibatch:  429   2018-12-16 10:37:46.482724\n",
      "Minibatch:  430   2018-12-16 10:37:47.892149\n",
      "Minibatch:  431   2018-12-16 10:37:49.589244\n",
      "Minibatch:  432   2018-12-16 10:37:51.172121\n",
      "Minibatch:  433   2018-12-16 10:37:52.954316\n",
      "Minibatch:  434   2018-12-16 10:37:54.581765\n",
      "Minibatch:  435   2018-12-16 10:37:56.153419\n",
      "Minibatch:  436   2018-12-16 10:37:58.048072\n",
      "Minibatch:  437   2018-12-16 10:38:00.012051\n",
      "Minibatch:  438   2018-12-16 10:38:01.789817\n",
      "Minibatch:  439   2018-12-16 10:38:03.571102\n",
      "Minibatch:  440   2018-12-16 10:38:05.285264\n",
      "Minibatch:  441   2018-12-16 10:38:07.005643\n",
      "Minibatch:  442   2018-12-16 10:38:08.656957\n",
      "Minibatch:  443   2018-12-16 10:38:10.349040\n",
      "Minibatch:  444   2018-12-16 10:38:11.944929\n",
      "Minibatch:  445   2018-12-16 10:38:13.529933\n",
      "Minibatch:  446   2018-12-16 10:38:15.055986\n",
      "Minibatch:  447   2018-12-16 10:38:16.736397\n",
      "Minibatch:  448   2018-12-16 10:38:18.248423\n",
      "Minibatch:  449   2018-12-16 10:38:20.060958\n",
      "Minibatch:  450   2018-12-16 10:38:21.965484\n",
      "Minibatch:  451   2018-12-16 10:38:23.665981\n",
      "Minibatch:  452   2018-12-16 10:38:25.397093\n",
      "Minibatch:  453   2018-12-16 10:38:26.858260\n",
      "Minibatch:  454   2018-12-16 10:38:28.531445\n",
      "Minibatch:  455   2018-12-16 10:38:30.619889\n",
      "Minibatch:  456   2018-12-16 10:38:32.451625\n",
      "Minibatch:  457   2018-12-16 10:38:34.355780\n",
      "Minibatch:  458   2018-12-16 10:38:36.274715\n",
      "Minibatch:  459   2018-12-16 10:38:37.965469\n",
      "Minibatch:  460   2018-12-16 10:38:39.853972\n",
      "Minibatch:  461   2018-12-16 10:38:41.737350\n",
      "Minibatch:  462   2018-12-16 10:38:43.224104\n",
      "Minibatch:  463   2018-12-16 10:38:45.172040\n",
      "Minibatch:  464   2018-12-16 10:38:46.933027\n",
      "Minibatch:  465   2018-12-16 10:38:48.454554\n",
      "Minibatch:  466   2018-12-16 10:38:49.969835\n",
      "Minibatch:  467   2018-12-16 10:38:51.603875\n",
      "Minibatch:  468   2018-12-16 10:38:53.842433\n",
      "Minibatch:  469   2018-12-16 10:38:55.634253\n"
     ]
    }
   ],
   "source": [
    "filepath = \"/home/muhammadayub/Desktop/CS230/models_saved/model6/model6_12_15__194_60.ckpt\" #4/model3_12_9__134_100.ckpt\" #model4/model3_12_9__9_80.ckpt\"\n",
    "# dev_test_or_train = 1\n",
    "# if(dev_test_or_train ==1): #train\n",
    "\n",
    "# filepathEvalMetrics = \"/home/muhammadayub/Desktop/CS230/models_saved/model3/model_12_9__\"+str(49)+\"_\"+str(60)+\"_evalMetrics_train.npy\"\n",
    "accuracies= []\n",
    "\n",
    "print(\" Running \", len(minibatches) , \" minibatches to get the evaluation metrics\") # testMiniBatches  devMiniBatches\n",
    "output_classes = []\n",
    "predicted_classes = []\n",
    "\n",
    "preds_one_hot_ev_value = None\n",
    "output_one_hot_ev_value =None \n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        \n",
    "        #define the graph\n",
    "        X, Y = create_placeholders(64, 64, 28, 34)\n",
    "        parameters = initialize_parameters()\n",
    "        Z3 = forward_prop(X, parameters)\n",
    "        #optimization \n",
    "#         cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))\n",
    "#         optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)#1e-4).minimize(cross_entropy)\n",
    "\n",
    "        #calculating the accuracy of the model \n",
    "        softmaxZ3 = tf.nn.softmax(Z3) # size will be 2200, 34\n",
    "        \n",
    "        top4 =  tf.nn.top_k(softmaxZ3, 3)     \n",
    "        top4Indices = top4.indices\n",
    "        oneHotIndices = tf.one_hot(top4Indices, depth = 34)\n",
    "        summationOfOneHotFor_NHotEncoding = tf.reduce_sum(oneHotIndices, 1)  # this is the N Hot Encoding \n",
    "\n",
    "        \n",
    "        #equals = tf.equal(summationOfOneHotFor_NHotEncoding, Y) #boolean tensor\n",
    "        #equalsFloat32 = tf.cast(equals, tf.float32)  # 1 or 0 tensor\n",
    "        \n",
    "        summationOfOneHotFor_NHotEncoding = tf.cast(summationOfOneHotFor_NHotEncoding, tf.int32)\n",
    "        Y = tf.cast(Y, tf.int32)\n",
    "        \n",
    "        andOp = tf.bitwise.bitwise_and(summationOfOneHotFor_NHotEncoding, Y)\n",
    "        andOp = tf.cast(andOp, tf.float32)\n",
    "        sumVal = tf.reduce_sum(andOp, axis = 1)\n",
    "        sumValDivided = sumVal/3.0\n",
    "        \n",
    "        \n",
    "        accuracy = tf.reduce_mean(sumValDivided)\n",
    "        \n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        saver.restore(sess , filepath)\n",
    "        print(\"Model restored.\")\n",
    "        \n",
    "        #devMiniBatches \n",
    "        #testMiniBatches\n",
    "        countMinibatch = 0\n",
    "        for i_  in minibatches:\n",
    "\n",
    "            output_image64 = outputData[i_]\n",
    "            inputImage64 = inputData[i_]\n",
    "            \n",
    "            abc = np.where(output_image64!= 0)\n",
    "            cp = np.array(output_image64)\n",
    "            cp[abc[0], abc[1]] = 1\n",
    "            \n",
    "            \n",
    "            summationOfOneHotFor_NHotEncoding_val, Y_val, andOpVal, accuracy_val  = sess.run([summationOfOneHotFor_NHotEncoding,Y,andOp, accuracy], feed_dict={X: inputImage64, Y: cp})\n",
    "#             equalsFloat32_val = sess.run([equalsFloat32], feed_dict={X: inputImage64, Y: output_image64})\n",
    "            \n",
    "            \n",
    "#             sumValDivided, accuracy_val, nHotEncoding = sess.run([sumValDivided, accuracy, summationOfOneHotFor_NHotEncoding], feed_dict={X: inputImage64, Y: output_image64})\n",
    "#             sumValDivided_val,accuracy_val,nHotEncoding = sess.run([sumValDivided, accuracy, summationOfOneHotFor_NHotEncoding], feed_dict={X: inputImage64, Y: output_image64})\n",
    "            \n",
    "            accuracies.append(accuracy_val)\n",
    "            output_classes.append(Y_val)\n",
    "            predicted_classes.append(summationOfOneHotFor_NHotEncoding_val)\n",
    "            \n",
    "            print('Minibatch: ', str(countMinibatch), \" \", datetime.datetime.now())\n",
    "            countMinibatch +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "470"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(summationOfOneHotFor_NHotEncoding_val[0])\n",
    "print(Y_val[0])\n",
    "# print(andOpVal[0])\n",
    "# type(andOpVal)\n",
    "\n",
    "# andOpVal.shape\n",
    "# summationOfOneHotFor_NHotEncoding_val.shape\n",
    "len(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "#new graph for concatenating things and getting into the right format for the F1 score \n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    with tf.Session(graph=graph) as sess:            \n",
    "    \n",
    "        val1 = tf.concat(output_classes, axis = 0)\n",
    "        \n",
    "    \n",
    "        #after running the minibatches-> we can concatenate the values \n",
    "        preds = tf.concat(predicted_classes,axis = 0)\n",
    "        data_outputs = tf.concat(output_classes,axis = 0)\n",
    "        \n",
    "        preds_one_hot_ev, output_one_hot_ev = sess.run([preds, data_outputs])\n",
    "        preds_one_hot_ev_value=preds_one_hot_ev\n",
    "        output_one_hot_ev_value =output_one_hot_ev\n",
    "        \n",
    "#         d = 35 # d is the total number of classes you have \n",
    "        #classLabels =  np.array([2,3,5,6,5,4,2,0,0])# the indexes \n",
    "        # tf.one_hot(classLabels, d)\n",
    "\n",
    "#         preds_one_hot = tf.one_hot(preds, d)\n",
    "#         data_outputs_one_hot =tf.one_hot(data_outputs, d)#evaluated \n",
    "#         preds_one_hot_ev, output_one_hot_ev = sess.run([preds_one_hot, data_outputs_one_hot])\n",
    "        \n",
    "#         preds_one_hot_ev_value =preds_one_hot_ev\n",
    "#         output_one_hot_ev_value =output_one_hot_ev\n",
    "        \n",
    "#For Debugging below:\n",
    "#         zVal = sess.run([Z3], feed_dict={X: inputImage64, Y: output_image64})\n",
    "#         softMaxVal = sess.run([softmaxZ3], feed_dict={X: inputImage64, Y: output_image64})\n",
    "#         output_class_val = sess.run([output_class], feed_dict={X: inputImage64, Y: output_image64})\n",
    "#         num_correct_val = sess.run([num_correct], feed_dict={X: inputImage64, Y: output_image64})\n",
    "#         num_correct_to_int_val = sess.run([num_correct_to_int], feed_dict={X: inputImage64, Y: output_image64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y_true_ = None\n",
    "_y_pred_ = None \n",
    "_f1Score_ = None \n",
    "_precision_ = None \n",
    "_recall_ =None \n",
    "TPVal = None\n",
    "FPVal = None\n",
    "FNVal = None \n",
    "_f1_ = None \n",
    "#using the F1 function found online -> get the values for the predictions  \n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    y_true = tf.Variable(output_one_hot_ev_value)\n",
    "    y_pred = tf.Variable(preds_one_hot_ev_value)\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(y_pred, tf.float64)\n",
    "\n",
    "    #moded from -> https://stackoverflow.com/questions/35365007/tensorflow-precision-recall-f1-score-and-confusion-matrix\n",
    "    TP = tf.count_nonzero(y_pred * y_true, axis=0) \n",
    "    FP = tf.count_nonzero(y_pred * (y_true - 1), axis=0) \n",
    "    FN = tf.count_nonzero((y_pred - 1) * y_true, axis=0)  # .001 for numerical stability \n",
    "\n",
    "    #they are integers right now and should be floats for numerical stability \n",
    "    TP = tf.cast(TP, tf.float32)\n",
    "    FP = tf.cast(FP, tf.float32)\n",
    "    FN = tf.cast(FN, tf.float32)\n",
    "    \n",
    "    \n",
    "    precision = TP / (TP + FP+tf.constant(.001))\n",
    "    recall = TP / (TP + FN+tf.constant(.001))\n",
    "    f1 = 2 * precision * recall / (precision + recall+tf.constant(.001))\n",
    "\n",
    "    f1Score = tf.reduce_mean(f1)\n",
    "\n",
    "    \n",
    "    with tf.Session(graph=graph) as sess:    \n",
    "        tf.global_variables_initializer().run(session=sess)\n",
    "        _f1_, _y_true_, _y_pred_, _f1Score_ , _precision_ ,_recall_ , TPVal,FPVal, FNVal = sess.run([f1, y_true, y_pred, f1Score, precision ,recall, TP,FP, FN ])          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.542553281688946"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(accuracies).flatten())/len(accuracies)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.48776957, 0.        , 0.        ,\n",
       "       0.        , 0.51086456, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.62970567, 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_f1_\n",
    "# _precision_\n",
    "# _recall_ \n",
    "# _f1Score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 34)\n",
      "(2, 3)\n",
      "[array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0.]],\n",
      "\n",
      "       [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]]], dtype=float32)]\n",
      "[array([[ 1,  2, 32],\n",
      "       [ 4,  3,  0]], dtype=int32)]\n",
      "----------------\n",
      "(2, 34)\n",
      "[array([[0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0.],\n",
      "       [1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[0. ,  0.34, 0.33 ,0. ,  0.  , 0. ,  0. ,  0.   ,0.  , 0. , \n",
    "                     0.   ,0. ,  0.,   0., 0.  , 0. ,  0. ,  0.,  0.   ,0.  , 0.  ,\n",
    "                     0. ,  0. ,  0. ,  0.  , 0. ,  0.  , 0., 0. ,  0. ,  0. ,\n",
    "                     0.  , 0.33, 0.  ], \n",
    "                [0.27 ,  0., 0. ,0.35 ,  0.38  , 0. ,  0. ,  0.   ,0.  , 0. , \n",
    "                     0.   ,0. ,  0.,   0., 0.  , 0. ,  0. ,  0.,  0.   ,0.  , 0.  ,\n",
    "                     0. ,  0. ,  0. ,  0.  , 0. ,  0.  , 0., 0. ,  0. ,  0. ,\n",
    "                     0.  , 0., 0.  ]])\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default(): \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        x = tf.placeholder(shape = (2,34), dtype=np.float32)\n",
    "        top4 =  tf.nn.top_k(x, 3)     \n",
    "\n",
    "        top4Indices = top4.indices\n",
    "        oneHotIndices = tf.one_hot(top4Indices, depth = 34)\n",
    "        summationOfOneHotFor_NHotEncoding = tf.reduce_sum(oneHotIndices, 1)\n",
    "        \n",
    "#         top4Vals = sess.run([top4], feed_dict={x:data})\n",
    "# top4Vals[0].indices\n",
    "        top4IndicesVals = sess.run([top4Indices], feed_dict={x:data})\n",
    "        \n",
    "        oneHotIndicesValues = sess.run([oneHotIndices], feed_dict={x:data})\n",
    "        \n",
    "        NHotEncodingValues = sess.run([summationOfOneHotFor_NHotEncoding], feed_dict={x:data})\n",
    "        \n",
    "        print(oneHotIndicesValues[0].shape)\n",
    "        print(top4IndicesVals[0].shape)\n",
    "        \n",
    "        print(oneHotIndicesValues)\n",
    "        print(top4IndicesVals)\n",
    "        \n",
    "        print('----------------')\n",
    "        \n",
    "        print(NHotEncodingValues[0].shape)\n",
    "        \n",
    "        print(NHotEncodingValues)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "# tf.math.top_k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[array([2., 3.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        ab = tf.constant(np.array([[1,2,4],[42,1,5]]), dtype = np.float32)\n",
    "        bc = tf.constant(np.array([[5,3,4],[42,1,6]]), dtype = np.float32)\n",
    "        \n",
    "        equals = tf.equal(ab, bc)\n",
    "        equalsFloat32 = tf.cast(equals, tf.float32)\n",
    "        \n",
    "        sumVal = tf.reduce_sum(equalsFloat32, axis = 1)\n",
    "        sumValDivided = sumVal/3.0\n",
    "        \n",
    "        meanReduced = tf.reduce_mean (sumValDivided )\n",
    "        \n",
    "        result = sess.run([meanReduced])\n",
    "        print(result[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = tf.constant([[1,2],[3.,4]])\n",
    "        \n",
    "        mean = tf.reduce_mean(x, axis = 0)\n",
    "        print(sess.run([mean]))\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
