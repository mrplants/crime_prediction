{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Statements \n",
    "####################\n",
    "import os \n",
    "import sys\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pathlib as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import pathlib\n",
    "import datetime\n",
    "import itertools as ite \n",
    "import math\n",
    "import calendar\n",
    "\n",
    "NUM_TIME_BINS_PER_DAY = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_=\"\"\"\n",
    "\n",
    "Define any useful functions \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def cleanURL(url):\n",
    "    p = pathlib.Path(url)\n",
    "    path = str(p.as_posix()) \n",
    "    return path \n",
    "\n",
    "\n",
    "def getDF(loc, sheetname):\n",
    "    dataframe = pd.read_excel(loc, sheetname)\n",
    "    #https://stackoverflow.com/questions/40950310/strip-trim-all-strings-of-a-dataframe\n",
    "    dataframe = dataframe.applymap(lambda x: x.strip() if type(x) is str else x)\n",
    "    return dataframe\n",
    "\n",
    "def printNulls(df):\n",
    "    null_columns = df.columns[df.isnull().any()]\n",
    "    return df[null_columns].isnull().sum() \n",
    "\n",
    "\n",
    "def writeDFToFile(dfs, path_): #dfs is an array of dataframes and their sheet names , path needs to have\n",
    "    time_ = str(datetime.datetime.now())\n",
    "    current_date_time = time_[0:time_.index(\".\")]\n",
    "    current_date_time = current_date_time.replace(\":\", \"-\")\n",
    "    task4_fileoutput = path_+current_date_time+\".xlsx\"\n",
    "\n",
    "    writer = pd.ExcelWriter(task4_fileoutput)\n",
    "    \n",
    "    for df_tuple in dfs:  \n",
    "        df = df_tuple[0]\n",
    "        sheetName = df_tuple[1]\n",
    "        df.to_excel(writer, sheetName)\n",
    "    print(\"file written to :       \" + task4_fileoutput)\n",
    "    writer.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############\n",
    "##############\n",
    "# Seans Stuff Before 11/27/2018 dataset\n",
    "##############\n",
    "##############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "###################\n",
    "# LOAD CRIME DATA #\n",
    "###################\n",
    "crime_dataframe = pd.read_csv(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch/Crimes (Chicago).csv'))\n",
    "crbk = crime_dataframe.copy()\n",
    "crime_dataframe = crime_dataframe.head(100000).copy()\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "########################\n",
    "# CONDITION CRIME DATA #\n",
    "########################\n",
    "# Delete columns that are redundant or unhelpful\n",
    "columns_to_delete = ['Case Number', 'Location Description', 'Block', 'Arrest', 'Domestic', 'FBI Code', 'IUCR', 'Description','X Coordinate', 'Y Coordinate', 'Year', 'Updated On', 'Location']\n",
    "final_dataframe = crime_dataframe.drop(columns_to_delete, axis = 1).copy().dropna() ## changed\n",
    "print('Length: %d' % len(final_dataframe))\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "##################################\n",
    "# CONVERT CRIME STATS TO ONE-HOT #\n",
    "##################################\n",
    "# Convert 'Beat', 'District', 'Ward', and 'Community Area' to one-hot vectors\n",
    "final_dataframe['Beat'] = final_dataframe['Beat'].astype(int)#.astype(str).apply(lambda x: 'BEAT_'+x)\n",
    "final_dataframe['District'] = final_dataframe['District'].astype(int)#.astype(str).apply(lambda x: 'DISTRICT_'+x)\n",
    "final_dataframe['Ward'] = final_dataframe['Ward'].astype(int)#.astype(str).apply(lambda x: 'WARD_'+x)\n",
    "final_dataframe['Community Area'] = final_dataframe['Community Area'].astype(int)#.astype(str).apply(lambda x: 'COMMUNITY_'+x)\n",
    "print('Length: %d' % len(final_dataframe))\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "##################\n",
    "# CONDITION DATE #\n",
    "##################\n",
    "# Convert crime dates to YEAR, MONTH, DAY, HOUR, MINUTE, and weekday columns\n",
    "# Convert those to one-hot and concat with final dataframe\n",
    "final_dataframe['Date'] = pd.to_datetime(final_dataframe['Date'])    ### modified \n",
    "final_dataframe['YEAR'] = final_dataframe['Date'].dt.year#.astype(str).apply(lambda x: 'YEAR_'+x)\n",
    "final_dataframe['MONTH'] = final_dataframe['Date'].dt.month#.apply(lambda x: calendar.month_abbr[x])\n",
    "final_dataframe['DAY'] = final_dataframe['Date'].dt.day#.astype(str).apply(lambda x: 'DAY_'+x)\n",
    "final_dataframe['WEEKDAY'] = final_dataframe['Date'].dt.weekday.apply(lambda x: calendar.day_name[x])\n",
    "hours = final_dataframe['Date'].dt.hour\n",
    "minutes = final_dataframe['Date'].dt.minute\n",
    "final_dataframe['TIME_OF_DAY'] = ((hours + minutes / 60.) / 24. * NUM_TIME_BINS_PER_DAY).astype(int)#.astype(str).apply(lambda x: 'TIME_SLOT_'+x)\n",
    "print('Length: %d' % len(final_dataframe))\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "####################\n",
    "# JOIN TEMPERATURE #\n",
    "####################\n",
    "temperature_dataframe = pd.read_csv(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch/Temperatures (Chicago).csv'))\n",
    "# Drop the TAVG column because it has too many NaNs\n",
    "temperature_dataframe = temperature_dataframe.drop(['TAVG'], axis = 1)  ## modified \n",
    "# Convert the Precipitation, max T, and min T columns to float\n",
    "temperature_dataframe['PRCP'] = pd.to_numeric(temperature_dataframe['PRCP'])\n",
    "temperature_dataframe['TMAX'] = pd.to_numeric(temperature_dataframe['TMAX'])\n",
    "temperature_dataframe['TMIN'] = pd.to_numeric(temperature_dataframe['TMIN'])\n",
    "temperature_dataframe.rename(columns={'PRCP':'PRECIPITATION', 'TMAX':'MAX TEMP', 'TMIN':'MIN TEMP'}, inplace = True)\n",
    "# Join with the final dataframe\n",
    "temperature_dataframe['date_join'] = pd.to_datetime(temperature_dataframe['DATE']).dt.date\n",
    "final_dataframe['date_join'] = final_dataframe['Date'].dt.date\n",
    "final_dataframe = final_dataframe.merge(temperature_dataframe, on=['date_join'], how='left').drop(['DATE', 'STATION', 'NAME', 'date_join'], axis = 1).dropna()## modified \n",
    "print('Length: %d' % len(final_dataframe))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save final_dataframe for reference\n",
    "final_dataframe.to_csv(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch/buildOff3.csv') , sep = ',' )\n",
    "\n",
    "\n",
    "# Really, after we finalize input dataset\n",
    "# retrieve final_dataframe and start from here\n",
    "#final_dataframe  = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "# Step 1.Assume Latitude and Longitude are normally distributed, filter out anything +/- 2 sigmas for 95% of data - empirically  not many datapoints are removed\n",
    "############  \n",
    "#get the old values \n",
    "latMax1 = final_dataframe.Latitude.max()\n",
    "latMin1 = final_dataframe.Latitude.min()\n",
    "longMax1  = final_dataframe.Longitude.max()\n",
    "longMin1  = final_dataframe.Longitude.min()\n",
    "print(latMax1 - latMin1)\n",
    "print(longMax1 - longMin1)\n",
    "\n",
    "#get mean and standard deviations to use for filteration\n",
    "latMean = final_dataframe.Latitude.mean()\n",
    "longMean = final_dataframe.Longitude.mean()\n",
    "latStd = final_dataframe.Latitude.std()  #we could toggle degrees of freedom but for now I don't think it matters \n",
    "longStd = final_dataframe.Longitude.std()\n",
    "\n",
    "# perform filteration \n",
    "final_dataframe = final_dataframe.loc[(final_dataframe['Latitude'] < latMean+2*latStd) & (final_dataframe['Latitude'] > latMean-2*latStd)].copy()\n",
    "final_dataframe = final_dataframe.loc[(final_dataframe['Longitude'] < longMean+2*longStd) & (final_dataframe['Longitude'] > longMean-2*longStd)].copy()\n",
    "\n",
    "latMax2 = final_dataframe.Latitude.max()\n",
    "latMin2 = final_dataframe.Latitude.min()\n",
    "longMax2  = final_dataframe.Longitude.max()\n",
    "longMin2  = final_dataframe.Longitude.min()\n",
    "print(latMax2 - latMin2)\n",
    "print(longMax2 - longMin2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "# Step 2. Generate Image Size and Scaling Multipliers to convert to image format. Have a 256 by 256 datasize. \n",
    "############\n",
    "\n",
    "iHeight = 256.\n",
    "iWidth = 256.\n",
    "\n",
    "# pixel/degree ratios\n",
    "latMultiplier = (iHeight- 0.)/(latMax2 - latMin2)  \n",
    "longMultiplier = (iWidth- 0.)/(longMax2 - longMin2)\n",
    "\n",
    "#convert the latitude and longitude values to the pixel values\n",
    "final_dataframe['latPixel'] = (final_dataframe.Latitude - latMin2)*latMultiplier\n",
    "final_dataframe['longPixel'] =(final_dataframe.Longitude - longMin2)*longMultiplier\n",
    "\n",
    "#pixel values are floats and not integers. type cast\n",
    "final_dataframe['latPixel'] = final_dataframe['latPixel'].astype(np.int64)\n",
    "final_dataframe['longPixel'] = final_dataframe['longPixel'].astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "# Step 3. Provide masks for precincts, beats, district, and wards\n",
    "############\n",
    "#Kludge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# Step 4. Get ready for the Data Generation Loop \n",
    "###########\n",
    "\n",
    "\n",
    "#initialize values for the for loop \n",
    "mindate = final_dataframe.Date.min().date()\n",
    "maxdate = final_dataframe.Date.max().date()\n",
    "\n",
    "delta = maxdate - mindate\n",
    "\n",
    "#https://stackoverflow.com/questions/7274267/print-all-day-dates-between-two-dates\n",
    "datesInDays = [ mindate + datetime.timedelta(index) for index in range(delta.days+1)] \n",
    "timeOfDays = [i for i in range(NUM_TIME_BINS_PER_DAY)]\n",
    "\n",
    "\n",
    "#we want the cartesian product of days and the time of days -> this is how many images we are making\n",
    "dates_and_timeOfDays_iterator = ite.product(datesInDays, timeOfDays)\n",
    "dates_and_timeOfDays = [[z[0], z[1]] for z in dates_and_timeOfDays_iterator]  # has the day and the time of day\n",
    "\n",
    "#check we got all possible combinations \n",
    "assert(len(dates_and_timeOfDays) == NUM_TIME_BINS_PER_DAY * len(datesInDays))\n",
    "\n",
    "#https://stackoverflow.com/questions/13635032/what-is-the-inverse-function-of-zip-in-python\n",
    "#allDates, allTimesOfDay  = zip(*dates_and_timeOfDays) # we unzip the cartesian product so that we can now loop through everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# Step 5. The For loop for generating the images \n",
    "###########\n",
    "\n",
    "\n",
    "#The General Algorithm:\n",
    "# For each day:\n",
    "#   For each time of day:\n",
    "#       dataframe = get the data points for this time period\n",
    "#       generate an image\n",
    "\n",
    "\n",
    "# For faster filtering, remove some columns\n",
    "all_cols = final_dataframe.columns.tolist()\n",
    "all_cols.remove('ID')\n",
    "all_cols.remove('Latitude') # have the pixelated values already\n",
    "all_cols.remove('Longitude')\n",
    "fDF = final_dataframe[all_cols].copy()\n",
    "\n",
    "#assert datatypes\n",
    "assert(final_dataframe['TIME_OF_DAY'].dtype == np.dtype('int32'))\n",
    "assert(final_dataframe['DAY'].dtype == np.dtype('int64'))\n",
    "assert(final_dataframe['MONTH'].dtype == np.dtype('int64'))\n",
    "assert(final_dataframe['YEAR'].dtype == np.dtype('int64'))\n",
    "\n",
    "\n",
    "LAYERS_IN_A_IMAGE = 5\n",
    "\n",
    "images_x = np.zeros(shape = (len(dates_and_timeOfDays), LAYERS_IN_A_IMAGE , int(iHeight),int(iWidth)))\n",
    "outputs_y = np.zeros(10) \n",
    "\n",
    "outputs = []\n",
    "\n",
    "dt = None\n",
    "tofD = None \n",
    "zero_img = np.zeros(shape = (int(iHeight),int(iWidth)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Step 5 continued... the actual for loop for adding data\n",
    "\n",
    "for i, _ in enumerate(dates_and_timeOfDays):\n",
    "    dt = _[0]\n",
    "    tofD = _[1]\n",
    "    df = fDF.loc[(tofD == fDF['TIME_OF_DAY']) & (dt.day == fDF['DAY']) & (dt.month == fDF['MONTH']) & (dt.year == fDF['YEAR'])]\n",
    "\n",
    "    img = np.zeros((LAYERS_IN_A_IMAGE,zero_img.shape[0], zero_img.shape[1]))\n",
    "    #the lat/long base layer\n",
    "    #img[0] = np.array(zero_img)\n",
    "    if(len(df) != 0):\n",
    "        row = df.latPixel.values\n",
    "        col = df.longPixel.values\n",
    "        img[0][row,col] = 1.\n",
    "        #to check:  a = np.where(img ==1), a[0] , a[1]\n",
    "    \n",
    "    #temperature layers \n",
    "    # Max temperature \n",
    "    img[1] = df['MAX TEMP'].iat[0]\n",
    "    # Min temperature \n",
    "    img[2] =  df['MIN TEMP'].iat[0]\n",
    "    # PRECIPITATION\n",
    "    img[3] =  df['PRECIPITATION'].iat[0]\n",
    "    \n",
    "    # TODO: add more layers here\n",
    "\n",
    "    images_x[i] = img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#######\n",
    "# Save data\n",
    "\n",
    "np.save(r'/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/27_November_cnn.npy', images_x)\n",
    "np.save(r'/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/27_November_yOutput.npy', outputs_y)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
