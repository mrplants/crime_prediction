{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements \n",
    "####################\n",
    "import os \n",
    "import sys\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pathlib as pl\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image, ImageDraw\n",
    "import pathlib\n",
    "import datetime\n",
    "import itertools as ite \n",
    "import math\n",
    "import calendar\n",
    "\n",
    "NUM_TIME_BINS_PER_DAY = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_=\"\"\"\n",
    "\n",
    "Define any useful functions \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def cleanURL(url):\n",
    "    p = pathlib.Path(url)\n",
    "    path = str(p.as_posix()) \n",
    "    return path \n",
    "\n",
    "\n",
    "def getDF(loc, sheetname):\n",
    "    dataframe = pd.read_excel(loc, sheetname)\n",
    "    #https://stackoverflow.com/questions/40950310/strip-trim-all-strings-of-a-dataframe\n",
    "    dataframe = dataframe.applymap(lambda x: x.strip() if type(x) is str else x)\n",
    "    return dataframe\n",
    "\n",
    "def printNulls(df):\n",
    "    null_columns = df.columns[df.isnull().any()]\n",
    "    return df[null_columns].isnull().sum() \n",
    "\n",
    "\n",
    "def writeDFToFile(dfs, path_): #dfs is an array of dataframes and their sheet names , path needs to have\n",
    "    time_ = str(datetime.datetime.now())\n",
    "    current_date_time = time_[0:time_.index(\".\")]\n",
    "    current_date_time = current_date_time.replace(\":\", \"-\")\n",
    "    task4_fileoutput = path_+current_date_time+\".xlsx\"\n",
    "\n",
    "    writer = pd.ExcelWriter(task4_fileoutput)\n",
    "    \n",
    "    for df_tuple in dfs:  \n",
    "        df = df_tuple[0]\n",
    "        sheetName = df_tuple[1]\n",
    "        df.to_excel(writer, sheetName)\n",
    "    print(\"file written to :       \" + task4_fileoutput)\n",
    "    writer.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############\n",
    "##############\n",
    "# Seans Stuff Before 11/27/2018 dataset\n",
    "##############\n",
    "##############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "###################\n",
    "# LOAD CRIME DATA #\n",
    "###################\n",
    "crime_dataframe = pd.read_csv(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch/Crimes (Chicago).csv'))\n",
    "crbk = crime_dataframe.copy()\n",
    "crime_dataframe = crime_dataframe.head(100000).copy()\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "########################\n",
    "# CONDITION CRIME DATA #\n",
    "########################\n",
    "# Delete columns that are redundant or unhelpful\n",
    "columns_to_delete = ['Case Number', 'Location Description', 'Block', 'Arrest', 'Domestic', 'FBI Code', 'IUCR', 'Description','X Coordinate', 'Y Coordinate', 'Year', 'Updated On', 'Location']\n",
    "final_dataframe = crime_dataframe.drop(columns_to_delete, axis = 1).copy().dropna() ## changed\n",
    "print('Length: %d' % len(final_dataframe))\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "##################################\n",
    "# CONVERT CRIME STATS TO ONE-HOT #\n",
    "##################################\n",
    "# Convert 'Beat', 'District', 'Ward', and 'Community Area' to one-hot vectors\n",
    "final_dataframe['Beat'] = final_dataframe['Beat'].astype(int)#.astype(str).apply(lambda x: 'BEAT_'+x)\n",
    "final_dataframe['District'] = final_dataframe['District'].astype(int)#.astype(str).apply(lambda x: 'DISTRICT_'+x)\n",
    "final_dataframe['Ward'] = final_dataframe['Ward'].astype(int)#.astype(str).apply(lambda x: 'WARD_'+x)\n",
    "final_dataframe['Community Area'] = final_dataframe['Community Area'].astype(int)#.astype(str).apply(lambda x: 'COMMUNITY_'+x)\n",
    "print('Length: %d' % len(final_dataframe))\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "##################\n",
    "# CONDITION DATE #\n",
    "##################\n",
    "# Convert crime dates to YEAR, MONTH, DAY, HOUR, MINUTE, and weekday columns\n",
    "# Convert those to one-hot and concat with final dataframe\n",
    "final_dataframe['Date'] = pd.to_datetime(final_dataframe['Date'])    ### modified \n",
    "final_dataframe['YEAR'] = final_dataframe['Date'].dt.year#.astype(str).apply(lambda x: 'YEAR_'+x)\n",
    "final_dataframe['MONTH'] = final_dataframe['Date'].dt.month#.apply(lambda x: calendar.month_abbr[x])\n",
    "final_dataframe['DAY'] = final_dataframe['Date'].dt.day#.astype(str).apply(lambda x: 'DAY_'+x)\n",
    "final_dataframe['WEEKDAY'] = final_dataframe['Date'].dt.weekday.apply(lambda x: calendar.day_name[x])\n",
    "hours = final_dataframe['Date'].dt.hour\n",
    "minutes = final_dataframe['Date'].dt.minute\n",
    "final_dataframe['TIME_OF_DAY'] = ((hours + minutes / 60.) / 24. * NUM_TIME_BINS_PER_DAY).astype(int)#.astype(str).apply(lambda x: 'TIME_SLOT_'+x)\n",
    "print('Length: %d' % len(final_dataframe))\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "####################\n",
    "# JOIN TEMPERATURE #\n",
    "####################\n",
    "temperature_dataframe = pd.read_csv(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch/Temperatures (Chicago).csv'))\n",
    "# Drop the TAVG column because it has too many NaNs\n",
    "temperature_dataframe = temperature_dataframe.drop(['TAVG'], axis = 1)  ## modified \n",
    "# Convert the Precipitation, max T, and min T columns to float\n",
    "temperature_dataframe['PRCP'] = pd.to_numeric(temperature_dataframe['PRCP'])\n",
    "temperature_dataframe['TMAX'] = pd.to_numeric(temperature_dataframe['TMAX'])\n",
    "temperature_dataframe['TMIN'] = pd.to_numeric(temperature_dataframe['TMIN'])\n",
    "temperature_dataframe.rename(columns={'PRCP':'PRECIPITATION', 'TMAX':'MAX TEMP', 'TMIN':'MIN TEMP'}, inplace = True)\n",
    "# Join with the final dataframe\n",
    "temperature_dataframe['date_join'] = pd.to_datetime(temperature_dataframe['DATE']).dt.date\n",
    "final_dataframe['date_join'] = final_dataframe['Date'].dt.date\n",
    "final_dataframe = final_dataframe.merge(temperature_dataframe, on=['date_join'], how='left').drop(['DATE', 'STATION', 'NAME', 'date_join'], axis = 1).dropna()## modified \n",
    "print('Length: %d' % len(final_dataframe))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save final_dataframe for reference\n",
    "#final_dataframe.to_csv(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch/buildOff3.csv') , sep = ',' )\n",
    "\n",
    "\n",
    "# Really, after we finalize input dataset\n",
    "# retrieve final_dataframe and start from here\n",
    "final_dataframe  =pd.read_csv(cleanURL(r'C:\\Users\\User\\Documents\\CS230 Project\\new_github\\crime_prediction\\Ali\\buildOff3.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Primary Type</th>\n",
       "      <th>Beat</th>\n",
       "      <th>District</th>\n",
       "      <th>Ward</th>\n",
       "      <th>Community Area</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>WEEKDAY</th>\n",
       "      <th>TIME_OF_DAY</th>\n",
       "      <th>PRECIPITATION</th>\n",
       "      <th>MAX TEMP</th>\n",
       "      <th>MIN TEMP</th>\n",
       "      <th>latPixel</th>\n",
       "      <th>longPixel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10000092</td>\n",
       "      <td>2015-03-18 19:44:00</td>\n",
       "      <td>BATTERY</td>\n",
       "      <td>1111</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>41.891399</td>\n",
       "      <td>-87.744385</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>166</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10000094</td>\n",
       "      <td>2015-03-18 23:00:00</td>\n",
       "      <td>OTHER OFFENSE</td>\n",
       "      <td>725</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>67</td>\n",
       "      <td>41.773372</td>\n",
       "      <td>-87.665319</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>78</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10000095</td>\n",
       "      <td>2015-03-18 22:45:00</td>\n",
       "      <td>BATTERY</td>\n",
       "      <td>222</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>41.813861</td>\n",
       "      <td>-87.596643</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>108</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10000096</td>\n",
       "      <td>2015-03-18 22:30:00</td>\n",
       "      <td>BATTERY</td>\n",
       "      <td>225</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>41.800802</td>\n",
       "      <td>-87.622619</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>98</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10000097</td>\n",
       "      <td>2015-03-18 21:00:00</td>\n",
       "      <td>ROBBERY</td>\n",
       "      <td>1113</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>41.878065</td>\n",
       "      <td>-87.743354</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>156</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        ID                 Date   Primary Type  Beat  District  \\\n",
       "0           0  10000092  2015-03-18 19:44:00        BATTERY  1111        11   \n",
       "1           1  10000094  2015-03-18 23:00:00  OTHER OFFENSE   725         7   \n",
       "2           2  10000095  2015-03-18 22:45:00        BATTERY   222         2   \n",
       "3           3  10000096  2015-03-18 22:30:00        BATTERY   225         2   \n",
       "4           4  10000097  2015-03-18 21:00:00        ROBBERY  1113        11   \n",
       "\n",
       "   Ward  Community Area   Latitude  Longitude  YEAR  MONTH  DAY    WEEKDAY  \\\n",
       "0    28              25  41.891399 -87.744385  2015      3   18  Wednesday   \n",
       "1    15              67  41.773372 -87.665319  2015      3   18  Wednesday   \n",
       "2     4              39  41.813861 -87.596643  2015      3   18  Wednesday   \n",
       "3     3              40  41.800802 -87.622619  2015      3   18  Wednesday   \n",
       "4    28              25  41.878065 -87.743354  2015      3   18  Wednesday   \n",
       "\n",
       "   TIME_OF_DAY  PRECIPITATION  MAX TEMP  MIN TEMP  latPixel  longPixel  \n",
       "0           19            0.0      49.0      24.0       166         50  \n",
       "1           23            0.0      49.0      24.0        78        134  \n",
       "2           22            0.0      49.0      24.0       108        208  \n",
       "3           22            0.0      49.0      24.0        98        180  \n",
       "4           21            0.0      49.0      24.0       156         51  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-87.74438456700001, 41.891398861)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_ = (final_dataframe.iat[0,9], final_dataframe.iat[0,8])\n",
    "point_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with GIS Shape Files to convert Map data to CNN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIS_comm_area = pd.read_csv(cleanURL(r'C:\\Users\\User\\Documents\\CS230 Project\\new_github\\crime_prediction\\Ali\\GIS datasets\\CommAreas.csv'))\n",
    "# import shapefile as shp\n",
    "import shapefile\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import shape\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sf = shapefile.Reader(cleanURL(r'C:\\Users\\User\\Documents\\CS230 Project\\new_github\\crime_prediction\\Ali\\GIS datasets\\geo_export_eb09b63d-cc4a-4268-8df4-e40c4f11a38b.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/GeospatialPython/pyshp\n",
    "#https://gis.stackexchange.com/questions/113799/how-to-read-a-shapefile-in-python\n",
    "feature = sf.shapeRecords()[0]\n",
    "first = feature.shape.__geo_interface__\n",
    "\n",
    "print(len(sf.shapeRecords()))\n",
    "print(len(sf.shapes()))\n",
    "# print(sf.shapes()[0].shapeType)\n",
    "print(feature)\n",
    "print(type(first))\n",
    "print(feature.shape)\n",
    "print(sf.fields)\n",
    "print(feature.record[2])\n",
    "print(first)\n",
    "print(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "<shapefile.Shape object at 0x000000000C91BD30>\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'15'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sf.shapes()[0].__geo_interface__\n",
    "feature = sf.shapeRecords()[0]\n",
    "feature.record\n",
    "feature.shape.shapeTypeName\n",
    "\n",
    "##check if all the shapes are valid polygons in a shapefile \n",
    "def checkIfShapeFileFilledWithPolygons(sfile):\n",
    "    for shape_ in sfile.shapes():\n",
    "        assert(shape_.shapeType == 5) # the shape.shapeTypeName is 'POLYGON'\n",
    "\n",
    "\n",
    "#https://gis.stackexchange.com/questions/250172/finding-out-if-coordinate-is-within-shapefile-shp-using-pyshp\n",
    "def return_Category_A_Point_Belongs_In(colIndex, sfile, point):  # (longitude,latitude) wise \n",
    "    features = sf.shapeRecords()\n",
    "    record_ = None\n",
    "    shape_boundary = None\n",
    "    for i, feature in enumerate(features):\n",
    "        record_ = feature.record\n",
    "        shape_boundary = feature.shape\n",
    "        if(Point(point).within(shape(shape_boundary))):\n",
    "            return record_[colIndex], shape_boundary\n",
    "    #if none found, it equals none \n",
    "    return None, None\n",
    "\n",
    "def getShapeThatCorrespondsToCategory(colIndex, category, sfile):\n",
    "    records = sfile.records()\n",
    "    for i, record in enumerate(records):\n",
    "#         print(i)\n",
    "        if(record[colIndex] == category):\n",
    "            return sfile.shapes()[i]\n",
    "        \n",
    "    #if none found, it equals none \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# Test 1\n",
    "r,s = return_Category_A_Point_Belongs_In(2, sf, point_) # for community areas -> the 2nd column gives the comm area\n",
    "print(r)\n",
    "print(s)\n",
    "############works!!!  returns category 25 which is the right value for community area for that point!!!!! \n",
    "\n",
    "\n",
    "# Test 2\n",
    "sampleRecord = sf.records()[14]\n",
    "realShape = sf.shapes()[14]\n",
    "colWanted = sampleRecord[2] # for Community area -> look at column 2 \n",
    "shape_ = getShapeThatCorrespondsToCategory(2, colWanted, sf)\n",
    "\n",
    "# shape_.equals(realShape)\n",
    "# print(realShape.__geo_interface__['coordinates'])\n",
    "# print(shape_.__geo_interface__['coordinates'])\n",
    "a =shape_.__geo_interface__['coordinates']\n",
    "b =realShape.__geo_interface__['coordinates']\n",
    "print(a ==b ) #should be true\n",
    "\n",
    "colWanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convertImageToCategoryMask(colIndex, categoryVal, sfile ,iHeight , iWidth ,latMultiplier, longMultiplier, latMin2,longMin2 ):\n",
    "    mask = np.zeros(shape=(iHeight, iWidth))\n",
    "    shape_that_points_should_be_in = getShapeThatCorrespondsToCategory(colIndex, categoryVal, sfile)   \n",
    "    \n",
    "    #if None is returned, the shapefile didn't have that category and therefore this category has no mask, no point belongs\n",
    "    if(type(shape_that_points_should_be_in) == type(None)):\n",
    "        return mask # KLUDGE: should we have a mask of -1's since this category is not available?  or -1 when point belongs to no category\n",
    "    \n",
    "    boundary = shape(shape_that_points_should_be_in)\n",
    "    \n",
    "    for row in range(iHeight):\n",
    "        for col in range(iWidth):\n",
    "            colMid = col+.5 # make the latitude and longitude be in the middle of the pixel\n",
    "            rowMid = iHeight - row - 1 +.5 # double check for this to work \n",
    "            \n",
    "            longVal = (colMid-0.)*(1./longMultiplier)+longMin2\n",
    "            latVal = (rowMid -0.)*(1./latMultiplier)+latMin2\n",
    "            point_ = (longVal,latVal)\n",
    "            \n",
    "            if(Point(point_).within(boundary)):\n",
    "                mask[row][col]= 1 \n",
    "            \n",
    "    return mask \n",
    "\n",
    "def convertImageToCategoryMask2(colIndex,  sfile ,iHeight  , iWidth ,latMultiplier, longMultiplier, latMin2,longMin2 ):\n",
    "    iHeight = int(iHeight)\n",
    "    iWidth = int(iWidth)\n",
    "    mask = np.zeros(shape=(iHeight, iWidth))\n",
    "#     shape_that_points_should_be_in = getShapeThatCorrespondsToCategory(colIndex, categoryVal, sfile)   \n",
    "    \n",
    "#     #if None is returned, the shapefile didn't have that category and therefore this category has no mask, no point belongs\n",
    "#     if(type(shape_that_points_should_be_in) == type(None)):\n",
    "#         return mask # KLUDGE: should we have a mask of -1's since this category is not available?  or -1 when point belongs to no category\n",
    "    \n",
    "#     boundary = shape(shape_that_points_should_be_in)\n",
    "    \n",
    "    for row in range(iHeight):\n",
    "        for col in range(iWidth):\n",
    "            colMid = col+.5 # make the latitude and longitude be in the middle of the pixel\n",
    "            rowMid = iHeight - row - 1 +.5 # double check for this to work \n",
    "            \n",
    "            longVal = (colMid-0.)*(1./longMultiplier)+longMin2\n",
    "            latVal = (rowMid -0.)*(1./latMultiplier)+latMin2\n",
    "            point_ = (longVal,latVal)\n",
    "            \n",
    "            r,s = return_Category_A_Point_Belongs_In(colIndex, sfile, point_)\n",
    "            \n",
    "            if(type(r) ==type(None)):\n",
    "                r = -1\n",
    "                \n",
    "            r = int(r)\n",
    "            mask[row][col]= r                \n",
    "            \n",
    "    return mask \n",
    "\n",
    "#general algorithm for any spatial feature is to have the masks ready for each of the categories. \n",
    "a = convertImageToCategoryMask2(2, sf, 256, 256, latMultiplier, longMultiplier, latMin2,longMin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.array(a)\n",
    "set(b.flatten().tolist())\n",
    "len(np.where(b.flatten()==-1)[0])\n",
    "\n",
    "np.save(cleanURL(r'C:\\Users\\User\\Documents\\CS230 Project\\new_github\\crime_prediction\\Ali\\GIS datasets/comunityArea_maps256by256.npy'), a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where start the CNN image dataset generation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3779438509999977\n",
      "0.4032663549999995\n",
      "0.34226877799999755\n",
      "0.23889396600000623\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# Step 1.Assume Latitude and Longitude are normally distributed, filter out anything +/- 2 sigmas for 95% of data - empirically  not many datapoints are removed\n",
    "############  \n",
    "#get the old values \n",
    "latMax1 = final_dataframe.Latitude.max()\n",
    "latMin1 = final_dataframe.Latitude.min()\n",
    "longMax1  = final_dataframe.Longitude.max()\n",
    "longMin1  = final_dataframe.Longitude.min()\n",
    "print(latMax1 - latMin1)\n",
    "print(longMax1 - longMin1)\n",
    "\n",
    "#get mean and standard deviations to use for filteration\n",
    "latMean = final_dataframe.Latitude.mean()\n",
    "longMean = final_dataframe.Longitude.mean()\n",
    "latStd = final_dataframe.Latitude.std()  #we could toggle degrees of freedom but for now I don't think it matters \n",
    "longStd = final_dataframe.Longitude.std()\n",
    "\n",
    "# perform filteration \n",
    "final_dataframe = final_dataframe.loc[(final_dataframe['Latitude'] < latMean+2*latStd) & (final_dataframe['Latitude'] > latMean-2*latStd)].copy()\n",
    "final_dataframe = final_dataframe.loc[(final_dataframe['Longitude'] < longMean+2*longStd) & (final_dataframe['Longitude'] > longMean-2*longStd)].copy()\n",
    "\n",
    "latMax2 = final_dataframe.Latitude.max()\n",
    "latMin2 = final_dataframe.Latitude.min()\n",
    "longMax2  = final_dataframe.Longitude.max()\n",
    "longMin2  = final_dataframe.Longitude.min()\n",
    "print(latMax2 - latMin2)\n",
    "print(longMax2 - longMin2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# Step 2. Generate Image Size and Scaling Multipliers to convert to image format. Have a 256 by 256 datasize. \n",
    "############\n",
    "\n",
    "iHeight = 256.\n",
    "iWidth = 256.\n",
    "\n",
    "# pixel/degree ratios\n",
    "latMultiplier = (iHeight- 0.)/(latMax2 - latMin2)  \n",
    "longMultiplier = (iWidth- 0.)/(longMax2 - longMin2)\n",
    "\n",
    "#convert the latitude and longitude values to the pixel values\n",
    "final_dataframe['latPixel'] = (final_dataframe.Latitude - latMin2)*latMultiplier\n",
    "final_dataframe['longPixel'] =(final_dataframe.Longitude - longMin2)*longMultiplier\n",
    "\n",
    "#pixel values are floats and not integers. type cast\n",
    "final_dataframe['latPixel'] = final_dataframe['latPixel'].astype(np.int64)\n",
    "final_dataframe['longPixel'] = final_dataframe['longPixel'].astype(np.int64)\n",
    "\n",
    "final_dataframe['latPixel'] = iHeight - final_dataframe['latPixel'] - 1  # this is important because matrix row numbers\n",
    "#don't go from bottom to top but latitudes do. So we simply invert it. \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "# Step 3. Provide masks for precincts, beats, district, and wards\n",
    "############\n",
    "#Kludge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# Step 4. Get ready for the Data Generation Loop \n",
    "###########\n",
    "\n",
    "\n",
    "#initialize values for the for loop \n",
    "mindate = final_dataframe.Date.min().date()\n",
    "maxdate = final_dataframe.Date.max().date()\n",
    "\n",
    "delta = maxdate - mindate\n",
    "\n",
    "#https://stackoverflow.com/questions/7274267/print-all-day-dates-between-two-dates\n",
    "datesInDays = [ mindate + datetime.timedelta(index) for index in range(delta.days+1)] \n",
    "timeOfDays = [i for i in range(NUM_TIME_BINS_PER_DAY)]\n",
    "\n",
    "\n",
    "#we want the cartesian product of days and the time of days -> this is how many images we are making\n",
    "dates_and_timeOfDays_iterator = ite.product(datesInDays, timeOfDays)\n",
    "dates_and_timeOfDays = [[z[0], z[1]] for z in dates_and_timeOfDays_iterator]  # has the day and the time of day\n",
    "\n",
    "#check we got all possible combinations \n",
    "assert(len(dates_and_timeOfDays) == NUM_TIME_BINS_PER_DAY * len(datesInDays))\n",
    "\n",
    "#https://stackoverflow.com/questions/13635032/what-is-the-inverse-function-of-zip-in-python\n",
    "#allDates, allTimesOfDay  = zip(*dates_and_timeOfDays) # we unzip the cartesian product so that we can now loop through everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# Step 4.5  Get the data masks ready for any location vectors . We have 4 -> presinct, beat, community area\n",
    "###########\n",
    "COMM_AREAS = [] #77\n",
    "commAreaSF = shapefile.Reader(cleanURL(r'C:\\Users\\User\\Documents\\CS230 Project\\new_github\\crime_prediction\\Ali\\GIS datasets\\geo_export_eb09b63d-cc4a-4268-8df4-e40c4f11a38b.shp'))\n",
    "BEATS_AREAS = []\n",
    "BEATS_DICT = {} # if the values are not sequential\n",
    "\n",
    "\n",
    "community_area_mask = np.zeros(NUM_COMMUNITY_AREAS, iHeight, iWidth)\n",
    "for j in range(NUM_COMMUNITY_AREAS):\n",
    "    community_area_mask[j] = convertImageToCategoryMask(2, str(j) , commAreaSF ,iHeight , iWidth ,latMultiplier, longMultiplier, latMin2,longMin2 )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# Step 5. The For loop for generating the images \n",
    "###########\n",
    "\n",
    "\n",
    "#The General Algorithm:\n",
    "# For each day:\n",
    "#   For each time of day:\n",
    "#       dataframe = get the data points for this time period\n",
    "#       generate an image\n",
    "\n",
    "\n",
    "# For faster filtering, remove some columns\n",
    "all_cols = final_dataframe.columns.tolist()\n",
    "all_cols.remove('ID')\n",
    "all_cols.remove('Latitude') # have the pixelated values already\n",
    "all_cols.remove('Longitude')\n",
    "fDF = final_dataframe[all_cols].copy()\n",
    "\n",
    "#assert datatypes\n",
    "assert(final_dataframe['TIME_OF_DAY'].dtype == np.dtype('int32'))\n",
    "assert(final_dataframe['DAY'].dtype == np.dtype('int64'))\n",
    "assert(final_dataframe['MONTH'].dtype == np.dtype('int64'))\n",
    "assert(final_dataframe['YEAR'].dtype == np.dtype('int64'))\n",
    "\n",
    "\n",
    "LAYERS_IN_A_IMAGE = 5\n",
    "\n",
    "images_x = np.zeros(shape = (len(dates_and_timeOfDays), LAYERS_IN_A_IMAGE , int(iHeight),int(iWidth)))\n",
    "outputs_y = np.zeros(10) \n",
    "\n",
    "outputs = []\n",
    "\n",
    "dt = None\n",
    "tofD = None \n",
    "zero_img = np.zeros(shape = (int(iHeight),int(iWidth)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Step 5 continued... the actual for loop for adding data\n",
    "\n",
    "for i, _ in enumerate(dates_and_timeOfDays):\n",
    "    dt = _[0]\n",
    "    tofD = _[1]\n",
    "    df = fDF.loc[(tofD == fDF['TIME_OF_DAY']) & (dt.day == fDF['DAY']) & (dt.month == fDF['MONTH']) & (dt.year == fDF['YEAR'])]\n",
    "\n",
    "    img = np.zeros((LAYERS_IN_A_IMAGE,zero_img.shape[0], zero_img.shape[1]))\n",
    "    #the lat/long base layer\n",
    "    #img[0] = np.array(zero_img)\n",
    "    if(len(df) != 0):\n",
    "        row = df.latPixel.values\n",
    "        col = df.longPixel.values\n",
    "        img[0][row,col] = 1.\n",
    "        #to check:  a = np.where(img ==1), a[0] , a[1]\n",
    "    \n",
    "    #temperature layers \n",
    "    # Max temperature \n",
    "    img[1] = df['MAX TEMP'].iat[0]\n",
    "    # Min temperature \n",
    "    img[2] =  df['MIN TEMP'].iat[0]\n",
    "    # PRECIPITATION\n",
    "    img[3] =  df['PRECIPITATION'].iat[0]\n",
    "    \n",
    "    # TODO: add more layers here\n",
    "\n",
    "    images_x[i] = img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#######\n",
    "# Save data\n",
    "\n",
    "np.save(r'/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/27_November_cnn.npy', images_x)\n",
    "np.save(r'/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/27_November_yOutput.npy', outputs_y)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
