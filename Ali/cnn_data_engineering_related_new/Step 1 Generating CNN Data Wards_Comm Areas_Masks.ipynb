{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Statements \n",
    "####################\n",
    "import os \n",
    "import sys\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pathlib as pl\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image, ImageDraw\n",
    "import pathlib\n",
    "import datetime\n",
    "import itertools as ite \n",
    "import math\n",
    "import calendar\n",
    "\n",
    "import shapefile  #conda install -c conda-forge pyshp    # (version should be 2.0)\n",
    "from shapely.geometry import Point   #conda install -c conda-forge shapely\n",
    "from shapely.geometry import shape\n",
    "\n",
    "\n",
    "NUM_TIME_BINS_PER_DAY = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_=\"\"\"\n",
    "\n",
    "Define any useful functions \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def cleanURL(url):\n",
    "    p = pathlib.Path(url)\n",
    "    path = str(p.as_posix()) \n",
    "    return path \n",
    "\n",
    "\n",
    "def getDF(loc, sheetname):\n",
    "    dataframe = pd.read_excel(loc, sheetname)\n",
    "    #https://stackoverflow.com/questions/40950310/strip-trim-all-strings-of-a-dataframe\n",
    "    dataframe = dataframe.applymap(lambda x: x.strip() if type(x) is str else x)\n",
    "    return dataframe\n",
    "\n",
    "def printNulls(df):\n",
    "    null_columns = df.columns[df.isnull().any()]\n",
    "    return df[null_columns].isnull().sum() \n",
    "\n",
    "\n",
    "def writeDFToFile(dfs, path_): #dfs is an array of dataframes and their sheet names , path needs to have\n",
    "    time_ = str(datetime.datetime.now())\n",
    "    current_date_time = time_[0:time_.index(\".\")]\n",
    "    current_date_time = current_date_time.replace(\":\", \"-\")\n",
    "    task4_fileoutput = path_+current_date_time+\".xlsx\"\n",
    "\n",
    "    writer = pd.ExcelWriter(task4_fileoutput)\n",
    "    \n",
    "    for df_tuple in dfs:  \n",
    "        df = df_tuple[0]\n",
    "        sheetName = df_tuple[1]\n",
    "        df.to_excel(writer, sheetName)\n",
    "    print(\"file written to :       \" + task4_fileoutput)\n",
    "    writer.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#Save final_dataframe for reference\n",
    "#final_dataframe.to_csv(cleanURL(r'C:\\Users\\j70514\\Documents\\Data Science Stuff\\DeepLearning_cs230\\CNN_data_crunch/buildOff3.csv') , sep = ',' )\n",
    "\n",
    "\n",
    "# Really, after we finalize input dataset\n",
    "# retrieve final_dataframe and start from here\n",
    "final_dataframe =pd.read_csv(cleanURL(r'C:\\Users\\User\\Documents\\CS230 Project\\new_github\\final_dataframe_after_removing_extreme_vals.csv'))\n",
    "final_dataframe['Date'] = pd.to_datetime(final_dataframe['Date'], format ='%Y-%m-%d %H:%M:%S')\n",
    "print('done')\n",
    "# assert(final_dataframe.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Primary Type</th>\n",
       "      <th>Beat</th>\n",
       "      <th>District</th>\n",
       "      <th>Ward</th>\n",
       "      <th>Community Area</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>WEEKDAY</th>\n",
       "      <th>TIME_OF_DAY</th>\n",
       "      <th>PRECIPITATION</th>\n",
       "      <th>MAX TEMP</th>\n",
       "      <th>MIN TEMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10000092</td>\n",
       "      <td>2015-03-18 07:44:00</td>\n",
       "      <td>BATTERY</td>\n",
       "      <td>1111</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>41.891399</td>\n",
       "      <td>-87.744385</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000094</td>\n",
       "      <td>2015-03-18 11:00:00</td>\n",
       "      <td>OTHER OFFENSE</td>\n",
       "      <td>725</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>67</td>\n",
       "      <td>41.773372</td>\n",
       "      <td>-87.665319</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10000095</td>\n",
       "      <td>2015-03-18 10:45:00</td>\n",
       "      <td>BATTERY</td>\n",
       "      <td>222</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>41.813861</td>\n",
       "      <td>-87.596643</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10000096</td>\n",
       "      <td>2015-03-18 10:30:00</td>\n",
       "      <td>BATTERY</td>\n",
       "      <td>225</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>41.800802</td>\n",
       "      <td>-87.622619</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10000097</td>\n",
       "      <td>2015-03-18 09:00:00</td>\n",
       "      <td>ROBBERY</td>\n",
       "      <td>1113</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>41.878065</td>\n",
       "      <td>-87.743354</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1        ID                Date   Primary Type  \\\n",
       "0           0             0  10000092 2015-03-18 07:44:00        BATTERY   \n",
       "1           1             1  10000094 2015-03-18 11:00:00  OTHER OFFENSE   \n",
       "2           2             2  10000095 2015-03-18 10:45:00        BATTERY   \n",
       "3           3             3  10000096 2015-03-18 10:30:00        BATTERY   \n",
       "4           4             4  10000097 2015-03-18 09:00:00        ROBBERY   \n",
       "\n",
       "   Beat  District  Ward  Community Area   Latitude  Longitude  YEAR  MONTH  \\\n",
       "0  1111        11    28              25  41.891399 -87.744385  2015      3   \n",
       "1   725         7    15              67  41.773372 -87.665319  2015      3   \n",
       "2   222         2     4              39  41.813861 -87.596643  2015      3   \n",
       "3   225         2     3              40  41.800802 -87.622619  2015      3   \n",
       "4  1113        11    28              25  41.878065 -87.743354  2015      3   \n",
       "\n",
       "   DAY    WEEKDAY  TIME_OF_DAY  PRECIPITATION  MAX TEMP  MIN TEMP  \n",
       "0   18  Wednesday            7            0.0      49.0      24.0  \n",
       "1   18  Wednesday           11            0.0      49.0      24.0  \n",
       "2   18  Wednesday           10            0.0      49.0      24.0  \n",
       "3   18  Wednesday           10            0.0      49.0      24.0  \n",
       "4   18  Wednesday            9            0.0      49.0      24.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-87.74438456700001, 41.891398861)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_ = (final_dataframe.iat[0,9], final_dataframe.iat[0,8])\n",
    "point_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIS Shape Files Functions to convert Map data to CNN data (library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sf.shapes()[0].__geo_interface__\n",
    "# feature = sf.shapeRecords()[0]\n",
    "# feature.record\n",
    "# feature.shape.shapeTypeName\n",
    "\n",
    "##check if all the shapes are valid polygons in a shapefile \n",
    "def checkIfShapeFileFilledWithPolygons(sfile):\n",
    "    for shape_ in sfile.shapes():\n",
    "        assert(shape_.shapeType == 5) # the shape.shapeTypeName is 'POLYGON'\n",
    "\n",
    "\n",
    "#https://gis.stackexchange.com/questions/250172/finding-out-if-coordinate-is-within-shapefile-shp-using-pyshp\n",
    "def return_Category_A_Point_Belongs_In(colIndex, sfile, point):  # (longitude,latitude) wise \n",
    "    features = sf.shapeRecords()\n",
    "    record_ = None\n",
    "    shape_boundary = None\n",
    "    for i, feature in enumerate(features):\n",
    "        record_ = feature.record\n",
    "        shape_boundary = feature.shape\n",
    "        if(Point(point).within(shape(shape_boundary))):\n",
    "            return record_[colIndex], shape_boundary\n",
    "    #if none found, it equals none \n",
    "    return None, None\n",
    "\n",
    "def getShapeThatCorrespondsToCategory(colIndex, category, sfile):\n",
    "    records = sfile.records()\n",
    "    for i, record in enumerate(records):\n",
    "#         print(i)\n",
    "        if(record[colIndex] == category):\n",
    "            return sfile.shapes()[i]\n",
    "        \n",
    "    #if none found, it equals none \n",
    "    return None\n",
    "\n",
    "\n",
    "#optimized for 30x speed up over other functions \n",
    "#list_of_categories must be list of ints\n",
    "#colIndex must be an integer\n",
    "def maskGISPolygonsData(colIndex, list_of_categories, iHeight, iWidth,sf ): # sf is shapes file \n",
    "    iHeight = int(iHeight)\n",
    "    iWidth = int(iWidth)\n",
    "\n",
    "    records = sf.records()\n",
    "    actualCategoriesFoundInShapeFile = []\n",
    "    for record in records:\n",
    "        category = int(record[colIndex])\n",
    "        actualCategoriesFoundInShapeFile.append(category)\n",
    "\n",
    "    shapesOfShapeFile = sf.shapes()\n",
    "    # now that we have the categories the data provides, we loop through the categories the data has\n",
    "    # if there are categories that the data has that shape file does not => leave None in that list \n",
    "    shapes= []\n",
    "    #shapes = sf.shapes()\n",
    "    list_ = list_of_categories\n",
    "    lookup_ = {}\n",
    "\n",
    "    list_ = list_of_categories\n",
    "    locationInShapeFile = None \n",
    "    #look up dictionary \n",
    "    for i, ele in enumerate(list_):\n",
    "        if(ele in actualCategoriesFoundInShapeFile):\n",
    "            locationInShapeFile = actualCategoriesFoundInShapeFile.index(ele)\n",
    "            shapes.append(shapesOfShapeFile[locationInShapeFile])\n",
    "        else: \n",
    "            shapes.append(None) \n",
    "        lookup_[i+1] = ele #we are saying that this category resides in i+1 in the Masks table  \n",
    "\n",
    "    assert(len(shapes) == len(lookup_))\n",
    "    #set the masks\n",
    "    MASKS= np.zeros((len(lookup_)+1, iHeight, iWidth)) # we add a category layer and hence we added the +1 -> this allows us to have a -1 category \n",
    "\n",
    "    ## look up dictionary for points \n",
    "    pointsLU = {} # points look up \n",
    "\n",
    "    for row in range(iHeight):\n",
    "        for col in range(iWidth):\n",
    "\n",
    "            colMid = col+.5 # make the latitude and longitude be in the middle of the pixel\n",
    "            rowMid = iHeight - row - 1 +.5 # double check for this to work \n",
    "            #rowMid = row +.5\n",
    "            \n",
    "            longVal = (colMid-0.)*(1./longMultiplier)+longMin2\n",
    "            latVal = (rowMid -0.)*(1./latMultiplier)+latMin2\n",
    "            #point_ = (longVal,latVal)\n",
    "\n",
    "            #plug in latVal and longVal\n",
    "            pointsLU[(row, col)] = Point((longVal, latVal))\n",
    "\n",
    "\n",
    "    #now for each category, apply the row col to get the mask \n",
    "\n",
    "    for ishape, shape_ in enumerate(shapes):\n",
    "        if(type(shape_) == type(None)): #we didn't find a shape for the element at this index\n",
    "            print(\"found no shape\")\n",
    "            continue # no shape -> this category cannot compete in the code \n",
    "        boundary = shape(shape_)\n",
    "        for row in range(iHeight):\n",
    "            for col in range(iWidth):\n",
    "                if(len(np.where(MASKS[:,row,col] ==1)[0]) == 1):\n",
    "                    continue\n",
    "            \n",
    "                #now check if point exists within boundary \n",
    "                if(pointsLU[(row, col)].within(boundary)):  \n",
    "                    MASKS[ishape+1, row, col] = 1.   # shift by +1 because at 0 we will have -1 layer\n",
    "    \n",
    "    for row in range(iHeight):\n",
    "        for col in range(iWidth):\n",
    "            if(len(np.where(MASKS[:,row,col] ==1)[0]) == 0): # if nothing found for this row, col we put it in the null category \n",
    "                MASKS[0, row, col] = 1.\n",
    "        \n",
    "    \n",
    "    return lookup_ , MASKS \n",
    "\n",
    "def convertImageToCategoryMask(colIndex, categoryVal, sfile ,iHeight , iWidth ,latMultiplier, longMultiplier, latMin2,longMin2 ):\n",
    "    mask = np.zeros(shape=(iHeight, iWidth))\n",
    "    shape_that_points_should_be_in = getShapeThatCorrespondsToCategory(colIndex, categoryVal, sfile)   \n",
    "    \n",
    "    #if None is returned, the shapefile didn't have that category and therefore this category has no mask, no point belongs\n",
    "    if(type(shape_that_points_should_be_in) == type(None)):\n",
    "        return mask # KLUDGE: should we have a mask of -1's since this category is not available?  or -1 when point belongs to no category\n",
    "    \n",
    "    boundary = shape(shape_that_points_should_be_in)\n",
    "    \n",
    "    for row in range(iHeight):\n",
    "        for col in range(iWidth):\n",
    "            colMid = col+.5 # make the latitude and longitude be in the middle of the pixel\n",
    "            #rowMid = iHeight - row - 1 +.5 # double check for this to work \n",
    "            rowMid = row +.5\n",
    "            \n",
    "            longVal = (colMid-0.)*(1./longMultiplier)+longMin2\n",
    "            latVal = (rowMid -0.)*(1./latMultiplier)+latMin2\n",
    "            point_ = (longVal,latVal)\n",
    "            \n",
    "            if(Point(point_).within(boundary)):\n",
    "                mask[row][col]= 1 \n",
    "            \n",
    "    return mask \n",
    "\n",
    "def convertImageToCategoryMask2(colIndex,  sfile ,iHeight  , iWidth ,latMultiplier, longMultiplier, latMin2,longMin2 ):\n",
    "    iHeight = int(iHeight)\n",
    "    iWidth = int(iWidth)\n",
    "    mask = np.zeros(shape=(iHeight, iWidth))\n",
    "#     shape_that_points_should_be_in = getShapeThatCorrespondsToCategory(colIndex, categoryVal, sfile)   \n",
    "    \n",
    "#     #if None is returned, the shapefile didn't have that category and therefore this category has no mask, no point belongs\n",
    "#     if(type(shape_that_points_should_be_in) == type(None)):\n",
    "#         return mask # KLUDGE: should we have a mask of -1's since this category is not available?  or -1 when point belongs to no category\n",
    "    \n",
    "#     boundary = shape(shape_that_points_should_be_in)\n",
    "    \n",
    "    for row in range(iHeight):\n",
    "        for col in range(iWidth):\n",
    "            colMid = col+.5 # make the latitude and longitude be in the middle of the pixel\n",
    "            #rowMid = iHeight - row - 1 +.5 # double check for this to work \n",
    "            rowMid = row +.5\n",
    "            \n",
    "            longVal = (colMid-0.)*(1./longMultiplier)+longMin2\n",
    "            latVal = (rowMid -0.)*(1./latMultiplier)+latMin2\n",
    "            point_ = (longVal,latVal)\n",
    "            \n",
    "            r,s = return_Category_A_Point_Belongs_In(colIndex, sfile, point_)\n",
    "            \n",
    "            if(type(r) ==type(None)):\n",
    "                r = -1\n",
    "                \n",
    "            r = int(r)\n",
    "            mask[row][col]= r                \n",
    "            \n",
    "    return mask \n",
    "\n",
    "# # Test 1\n",
    "# r,s = return_Category_A_Point_Belongs_In(2, sf, point_) # for community areas -> the 2nd column gives the comm area\n",
    "# print(r)\n",
    "# print(s)\n",
    "# ############works!!!  returns category 25 which is the right value for community area for that point!!!!! \n",
    "\n",
    "\n",
    "# # Test 2\n",
    "# sampleRecord = sf.records()[14]\n",
    "# realShape = sf.shapes()[14]\n",
    "# colWanted = sampleRecord[2] # for Community area -> look at column 2 \n",
    "# shape_ = getShapeThatCorrespondsToCategory(2, colWanted, sf)\n",
    "\n",
    "# # shape_.equals(realShape)\n",
    "# # print(realShape.__geo_interface__['coordinates'])\n",
    "# # print(shape_.__geo_interface__['coordinates'])\n",
    "# a =shape_.__geo_interface__['coordinates']\n",
    "# b =realShape.__geo_interface__['coordinates']\n",
    "# print(a ==b ) #should be true\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sample Usage\n",
    "\n",
    "bc = maskGISPolygonsData(2, [j for j in range(1,78)], iHeight, iWidth,commAreaSF)\n",
    "lookup , mask = bc\n",
    "mask.shape  # remember there is an extra layer on the first element because we add the -1 class \n",
    "lookup\n",
    "img = np.array(mask)\n",
    "#to transform the image\n",
    "for ind in range(1, len(img)):\n",
    "    img[ind] = img[ind]*lookup[ind]\n",
    "img[0] = img[0]*-1\n",
    "resImage = np.sum(img, axis = 0)\n",
    "np.save(cleanURL(r'C:\\Users\\User\\Documents\\CS230 Project\\new_github\\crime_prediction\\Ali\\GIS datasets/PDistrict_SHORT_WAY_maps256by256.npy'),resImage)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where we start the CNN image dataset generation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3779438509999977\n",
      "0.4032663549999995\n",
      "0.34226877799999755\n",
      "0.23889396600000623\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# Step 1.Assume Latitude and Longitude are normally distributed, filter out anything +/- 2 sigmas for 95% of data - empirically  not many datapoints are removed\n",
    "############  \n",
    "#get the old values \n",
    "latMax1 = final_dataframe.Latitude.max()\n",
    "latMin1 = final_dataframe.Latitude.min()\n",
    "longMax1  = final_dataframe.Longitude.max()\n",
    "longMin1  = final_dataframe.Longitude.min()\n",
    "print(latMax1 - latMin1)\n",
    "print(longMax1 - longMin1)\n",
    "\n",
    "#get mean and standard deviations to use for filteration\n",
    "latMean = final_dataframe.Latitude.mean()\n",
    "longMean = final_dataframe.Longitude.mean()\n",
    "latStd = final_dataframe.Latitude.std()  #we could toggle degrees of freedom but for now I don't think it matters \n",
    "longStd = final_dataframe.Longitude.std()\n",
    "\n",
    "# perform filteration \n",
    "final_dataframe = final_dataframe.loc[(final_dataframe['Latitude'] < latMean+2*latStd) & (final_dataframe['Latitude'] > latMean-2*latStd)].copy()\n",
    "final_dataframe = final_dataframe.loc[(final_dataframe['Longitude'] < longMean+2*longStd) & (final_dataframe['Longitude'] > longMean-2*longStd)].copy()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3565254240000044\n",
      "0.24557258000000104\n"
     ]
    }
   ],
   "source": [
    "latMax2 = final_dataframe.Latitude.max()\n",
    "latMin2 = final_dataframe.Latitude.min()\n",
    "longMax2  = final_dataframe.Longitude.max()\n",
    "longMin2  = final_dataframe.Longitude.min()\n",
    "print(latMax2 - latMin2)\n",
    "print(longMax2 - longMin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# Step 2. Generate Image Size and Scaling Multipliers to convert to image format. Have a 256 by 256 datasize. \n",
    "############\n",
    "\n",
    "iHeight = 256.\n",
    "iWidth = 256.\n",
    "\n",
    "# pixel/degree ratios\n",
    "latMultiplier = (iHeight- 0.)/(latMax2 - latMin2)  \n",
    "longMultiplier = (iWidth- 0.)/(longMax2 - longMin2)\n",
    "\n",
    "#convert the latitude and longitude values to the pixel values\n",
    "final_dataframe['latPixel'] = (final_dataframe.Latitude - latMin2)*latMultiplier\n",
    "final_dataframe['longPixel'] =(final_dataframe.Longitude - longMin2)*longMultiplier\n",
    "\n",
    "#pixel values are floats and not integers. type cast\n",
    "final_dataframe['latPixel'] = final_dataframe['latPixel'].astype(np.int64)\n",
    "final_dataframe['longPixel'] = final_dataframe['longPixel'].astype(np.int64)\n",
    "\n",
    "final_dataframe['latPixel'] = iHeight - final_dataframe['latPixel'] - 1  # this is important because matrix row numbers\n",
    "#don't go from bottom to top but latitudes do. So we simply invert it. \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2001-01-01 01:00:00')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataframe.columns.tolist()\n",
    "# final_dataframe['Date']\n",
    "final_dataframe.Date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########  Provide masks for precincts, beats, district, and wards\n",
    "# Step 3.  Get the data masks ready for any location vectors . We have 4 -> presinct, beat, community area\n",
    "###########\n",
    "COMM_AREAS = final_dataframe['Community Area'].unique().tolist() #77#[] #77\n",
    "commAreaSF = shapefile.Reader(cleanURL(r'C:\\Users\\User\\Documents\\CS230 Project\\new_github\\crime_prediction\\Ali\\GIS datasets\\geo_export_eb09b63d-cc4a-4268-8df4-e40c4f11a38b.shp'))\n",
    "\n",
    "\n",
    "BEATS_AREAS = final_dataframe['Beat'].unique().tolist() \n",
    "beatsAreaSF = shapefile.Reader(cleanURL(r'C:\\Users\\User\\Documents\\CS230 Project\\new_github\\crime_prediction\\Ali\\GIS datasets\\police beats\\geo_export_d3113d24-90eb-442c-83d9-4208a84c7275.shp'))\n",
    "\n",
    "DISTRICT_AREAS = final_dataframe['District'].unique().tolist()\n",
    "districtsAreaSF = shapefile.Reader(cleanURL(r'C:\\Users\\User\\Documents\\CS230 Project\\new_github\\crime_prediction\\Ali\\GIS datasets\\police districts\\geo_export_2f172cc2-d492-418f-a4e6-a19358ac79bc.shp'))\n",
    "\n",
    "WARD_AREAS = final_dataframe['Ward'].unique().tolist()\n",
    "wardsAreaSF = shapefile.Reader(cleanURL(r'C:\\Users\\User\\Documents\\CS230 Project\\new_github\\crime_prediction\\Ali\\GIS datasets\\wards\\geo_export_ddb6bf98-2328-402f-b3f0-faf9cded16a8.shp'))\n",
    "\n",
    "# PRECINCT_AREAS = final_dataframe['Ward'].unique().tolist()\n",
    "# precinctsAreaSF = shapefile.Reader(cleanURL(r'C:\\Users\\User\\Documents\\CS230 Project\\new_github\\crime_prediction\\Ali\\GIS datasets\\precincts\\geo_export_16916c2a-daf5-4f30-97ee-18f919748e95.shp'))\n",
    "\n",
    "\n",
    "# community_area_mask = np.zeros(COMM_AREAS, iHeight, iWidth)\n",
    "# for j in range(NUM_COMMUNITY_AREAS):\n",
    "#     community_area_mask[j] = convertImageToCategoryMask(2, str(j) , commAreaSF ,iHeight , iWidth ,latMultiplier, longMultiplier, latMin2,longMin2 )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recordsPrint(sf):\n",
    "    print(sf.fields)\n",
    "    for record in sf.records():\n",
    "        print(record[:])\n",
    "        \n",
    "def saveMaskImg(mask, lookup, path):\n",
    "    img = np.array(mask)\n",
    "    #to transform the image\n",
    "    for ind in range(1, len(img)):\n",
    "        img[ind] = img[ind]*lookup[ind]\n",
    "    img[0] = img[0]*-1\n",
    "    resImage = np.sum(img, axis = 0)\n",
    "    np.save(cleanURL(path),resImage)\n",
    "\n",
    "def saveMaskImg(mask, lookup, path, path2):\n",
    "    keys =[]\n",
    "    values=[]\n",
    "    for key in lookup:\n",
    "        keys.append(key)\n",
    "        values.append(lookup[key])\n",
    "\n",
    "    lookupDF = pd.DataFrame({'key':keys, 'value':values})\n",
    "    lookupDF.to_csv(cleanURL(path2), sep = ',')\n",
    "    np.save(cleanURL(path),mask)\n",
    "    \n",
    "        \n",
    "# recordsPrint(commAreaSF)\n",
    "# print(COMM_AREAS)\n",
    "        \n",
    "# recordsPrint(beatsAreaSF)\n",
    "# print(BEATS_AREAS)\n",
    "\n",
    "# recordsPrint(districtsAreaSF)\n",
    "# print(DISTRICT_AREAS)\n",
    "\n",
    "# recordsPrint(wardsAreaSF)\n",
    "# print(WARD_AREAS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Documents\\\\CS230 Project\\\\new_github\\\\crime_prediction\\\\Ali\\\\GIS datasets\\\\resultMasks\\\\'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\User\\Documents\\CS230 Project\\new_github\\crime_prediction\\Ali\\GIS datasets\\resultMasks\\ \"\n",
    "base_path = base_path.strip()\n",
    "base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found no shape\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#comm area\n",
    "COMM_AREA_MASK_LOOKUP , COMM_AREA_MASK = maskGISPolygonsData(2, COMM_AREAS, iHeight, iWidth,commAreaSF)\n",
    "saveMaskImg(COMM_AREA_MASK, COMM_AREA_MASK_LOOKUP, path=base_path+'commArea.npy', path2=base_path+'commAreaL.csv')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "found no shape\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#BEATS \n",
    "BEATS_AREA_MASK_LOOKUP , BEATS_AREA_MASK = maskGISPolygonsData(1, BEATS_AREAS, iHeight, iWidth,beatsAreaSF)\n",
    "saveMaskImg(BEATS_AREA_MASK, BEATS_AREA_MASK_LOOKUP, path=base_path+'beatArea.npy', path2=base_path+'beatAreaL.csv')\n",
    "print('done')#5 beats not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found no shape\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#Districts\n",
    "DISTRICT_AREA_MASK_LOOKUP , DISTRICT_AREA_MASK = maskGISPolygonsData(1, DISTRICT_AREAS, iHeight, iWidth,districtsAreaSF)\n",
    "saveMaskImg(DISTRICT_AREA_MASK, DISTRICT_AREA_MASK_LOOKUP, path=base_path+'districtArea.npy', path2=base_path+'districtAreaL.csv')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# WARDS \n",
    "WARD_AREA_MASK_LOOKUP , WARD_AREA_MASK = maskGISPolygonsData(2, WARD_AREAS, iHeight, iWidth,wardsAreaSF)\n",
    "saveMaskImg(WARD_AREA_MASK, WARD_AREA_MASK_LOOKUP, path=base_path+'wardArea.npy', path2=base_path+'wardAreaL.csv')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Documents\\\\CS230 Project\\\\new_github\\\\crime_prediction\\\\Ali\\\\GIS datasets\\\\resultMasks\\\\'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PRECINCTS \n",
    "# not in dataset\n",
    "base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148560\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# Step 4. Get ready for the Data Generation Loop \n",
    "###########\n",
    "\n",
    "\n",
    "#initialize values for the for loop \n",
    "mindate = final_dataframe.Date.min().date()\n",
    "maxdate = final_dataframe.Date.max().date()\n",
    "\n",
    "delta = maxdate - mindate\n",
    "\n",
    "#https://stackoverflow.com/questions/7274267/print-all-day-dates-between-two-dates\n",
    "datesInDays = [ mindate + datetime.timedelta(index) for index in range(delta.days+1)] \n",
    "timeOfDays = [i for i in range(NUM_TIME_BINS_PER_DAY)]\n",
    "\n",
    "\n",
    "#we want the cartesian product of days and the time of days -> this is how many images we are making\n",
    "dates_and_timeOfDays_iterator = ite.product(datesInDays, timeOfDays)\n",
    "dates_and_timeOfDays = [[z[0], z[1]] for z in dates_and_timeOfDays_iterator]  # has the day and the time of day\n",
    "\n",
    "#check we got all possible combinations \n",
    "assert(len(dates_and_timeOfDays) == NUM_TIME_BINS_PER_DAY * len(datesInDays))\n",
    "\n",
    "print(len(dates_and_timeOfDays))\n",
    "#https://stackoverflow.com/questions/13635032/what-is-the-inverse-function-of-zip-in-python\n",
    "#allDates, allTimesOfDay  = zip(*dates_and_timeOfDays) # we unzip the cartesian product so that we can now loop through everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# Step 5. The For loop for generating the images \n",
    "###########\n",
    "\n",
    "\n",
    "#The General Algorithm:\n",
    "# For each day:\n",
    "#   For each time of day:\n",
    "#       dataframe = get the data points for this time period\n",
    "#       generate an image\n",
    "\n",
    "\n",
    "# For faster filtering, remove some columns\n",
    "all_cols = final_dataframe.columns.tolist()\n",
    "all_cols.remove('ID')\n",
    "all_cols.remove('Latitude') # have the pixelated values already\n",
    "all_cols.remove('Longitude')\n",
    "fDF = final_dataframe[all_cols].copy()\n",
    "\n",
    "#assert datatypes\n",
    "assert(final_dataframe['TIME_OF_DAY'].dtype == np.dtype('int32'))\n",
    "assert(final_dataframe['DAY'].dtype == np.dtype('int64'))\n",
    "assert(final_dataframe['MONTH'].dtype == np.dtype('int64'))\n",
    "assert(final_dataframe['YEAR'].dtype == np.dtype('int64'))\n",
    "\n",
    "\n",
    "LAYERS_IN_A_IMAGE = 5\n",
    "\n",
    "images_x = np.zeros(shape = (len(dates_and_timeOfDays), LAYERS_IN_A_IMAGE , int(iHeight),int(iWidth)))\n",
    "outputs_y = np.zeros(10) \n",
    "\n",
    "outputs = []\n",
    "\n",
    "dt = None\n",
    "tofD = None \n",
    "zero_img = np.zeros(shape = (int(iHeight),int(iWidth)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Step 5 continued... the actual for loop for adding data\n",
    "\n",
    "for i, _ in enumerate(dates_and_timeOfDays):\n",
    "    dt = _[0]\n",
    "    tofD = _[1]\n",
    "    df = fDF.loc[(tofD == fDF['TIME_OF_DAY']) & (dt.day == fDF['DAY']) & (dt.month == fDF['MONTH']) & (dt.year == fDF['YEAR'])]\n",
    "\n",
    "    img = np.zeros((LAYERS_IN_A_IMAGE,zero_img.shape[0], zero_img.shape[1]))\n",
    "    #the lat/long base layer\n",
    "    #img[0] = np.array(zero_img)\n",
    "    if(len(df) != 0):\n",
    "        row = df.latPixel.values\n",
    "        col = df.longPixel.values\n",
    "        img[0][row,col] = 1.\n",
    "        #to check:  a = np.where(img ==1), a[0] , a[1]\n",
    "    \n",
    "    #temperature layers \n",
    "    # Max temperature \n",
    "    img[1] = df['MAX TEMP'].iat[0]\n",
    "    # Min temperature \n",
    "    img[2] =  df['MIN TEMP'].iat[0]\n",
    "    # PRECIPITATION\n",
    "    img[3] =  df['PRECIPITATION'].iat[0]\n",
    "    \n",
    "    # TODO: add more layers here\n",
    "\n",
    "    images_x[i] = img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#######\n",
    "# Save data\n",
    "\n",
    "np.save(r'/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/27_November_cnn.npy', images_x)\n",
    "np.save(r'/Volumes/GoogleDrive/My Drive/Crime Data/Composite Data/Sean Workspace/27_November_yOutput.npy', outputs_y)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
